# The Anscsombe Quartet (R)

# demonstration data from
# Anscombe, F. J. 1973, February. Graphs in statistical analysis. 
#  The American Statistician 27: 17â€“21.

# define the anscombe data frame
anscombe <- data.frame(
    x1 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),
    x2 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),
    x3 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),
    x4 = c(8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8),
    y1 = c(8.04, 6.95,  7.58, 8.81, 8.33, 9.96, 7.24, 4.26,10.84, 4.82, 5.68),
    y2 = c(9.14, 8.14,  8.74, 8.77, 9.26, 8.1, 6.13, 3.1,  9.13, 7.26, 4.74),
    y3 = c(7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73),
    y4 = c(6.58, 5.76,  7.71, 8.84, 8.47, 7.04, 5.25, 12.5, 5.56, 7.91, 6.89))

# show results from four regression analyses
with(anscombe, print(summary(lm(y1 ~ x1, data = anscombe))))
with(anscombe, print(summary(lm(y2 ~ x2, data = anscombe))))
with(anscombe, print(summary(lm(y3 ~ x3, data = anscombe))))
with(anscombe, print(summary(lm(y4 ~ x4, data = anscombe))))

# place four plots on one page using standard R graphics
# ensuring that all have the same scales
# for horizontal and vertical axes
pdf(file = "fig_anscombe_R.pdf", width = 8.5, height = 8.5)
par(mfrow=c(2,2), mar=c(5.1, 4.1, 4.1, 2.1))
with(anscombe, plot(x1, y1, xlim=c(2,20), ylim=c(2,14), pch = 19, 
    col = "darkblue", cex = 1.5, las = 1, xlab = "x1", ylab = "y1"))  
title("Set I")
with(anscombe,plot(x2, y2, xlim=c(2,20), ylim=c(2,14), pch = 19, 
    col = "darkblue", cex = 1.5, las = 1, xlab = "x2", ylab = "y2"))
title("Set II")
with(anscombe,plot(x3, y3, xlim=c(2,20), ylim=c(2,14), pch = 19, 
    col = "darkblue", cex = 1.5, las = 1, xlab = "x3", ylab = "y3"))
title("Set III")
with(anscombe,plot(x4, y4, xlim=c(2,20), ylim=c(2,14), pch = 19, 
    col = "darkblue", cex = 1.5, las = 1, xlab = "x4", ylab = "y4"))
title("Set IV")
dev.off()

# par(mfrow=c(1,1),mar=c(5.1, 4.1, 4.1, 2.1))  # return to plotting defaults

# Predictive Model for Los Angeles Dodgers Promotion and Attendance (R)

library(car)  # special functions for linear regression
library(lattice)  # graphics package

# read in data and create a data frame called dodgers
dodgers <- read.csv("dodgers.csv")
print(str(dodgers))  # check the structure of the data frame

# define an ordered day-of-week variable 
# for plots and data summaries
dodgers$ordered_day_of_week <- with(data=dodgers,
  ifelse ((day_of_week == "Monday"),1,
  ifelse ((day_of_week == "Tuesday"),2,
  ifelse ((day_of_week == "Wednesday"),3,
  ifelse ((day_of_week == "Thursday"),4,
  ifelse ((day_of_week == "Friday"),5,
  ifelse ((day_of_week == "Saturday"),6,7)))))))
dodgers$ordered_day_of_week <- factor(dodgers$ordered_day_of_week, levels=1:7,
labels=c("Mon", "Tue", "Wed", "Thur", "Fri", "Sat", "Sun"))

# exploratory data analysis with standard graphics: attendance by day of week
with(data=dodgers,plot(ordered_day_of_week, attend/1000, 
xlab = "Day of Week", ylab = "Attendance (thousands)", 
col = "violet", las = 1))

# when do the Dodgers use bobblehead promotions
with(dodgers, table(bobblehead,ordered_day_of_week)) # bobbleheads on Tuesday

# define an ordered month variable 
# for plots and data summaries
dodgers$ordered_month <- with(data=dodgers,
  ifelse ((month == "APR"),4,
  ifelse ((month == "MAY"),5,
  ifelse ((month == "JUN"),6,
  ifelse ((month == "JUL"),7,
  ifelse ((month == "AUG"),8,
  ifelse ((month == "SEP"),9,10)))))))
dodgers$ordered_month <- factor(dodgers$ordered_month, levels=4:10,
labels = c("April", "May", "June", "July", "Aug", "Sept", "Oct"))

# exploratory data analysis with standard R graphics: attendance by month 
with(data=dodgers,plot(ordered_month,attend/1000, xlab = "Month", 
ylab = "Attendance (thousands)", col = "light blue", las = 1))

# exploratory data analysis displaying many variables
# looking at attendance and conditioning on day/night
# the skies and whether or not fireworks are displayed
library(lattice) # used for plotting 
# let us prepare a graphical summary of the dodgers data
group.labels <- c("No Fireworks","Fireworks")
group.symbols <- c(21,24)
group.colors <- c("black","black") 
group.fill <- c("black","red")
xyplot(attend/1000 ~ temp | skies + day_night, 
    data = dodgers, groups = fireworks, pch = group.symbols, 
    aspect = 1, cex = 1.5, col = group.colors, fill = group.fill,
    layout = c(2, 2), type = c("p","g"),
    strip=strip.custom(strip.levels=TRUE,strip.names=FALSE, style=1),
    xlab = "Temperature (Degrees Fahrenheit)", 
    ylab = "Attendance (thousands)",
    key = list(space = "top", 
        text = list(rev(group.labels),col = rev(group.colors)),
        points = list(pch = rev(group.symbols), col = rev(group.colors),
        fill = rev(group.fill))))  
                
# attendance by opponent and day/night game
group.labels <- c("Day","Night")
group.symbols <- c(1,20)
group.symbols.size <- c(2,2.75)
bwplot(opponent ~ attend/1000, data = dodgers, groups = day_night, 
    xlab = "Attendance (thousands)",
    panel = function(x, y, groups, subscripts, ...) 
       {panel.grid(h = (length(levels(dodgers$opponent)) - 1), v = -1)
        panel.stripplot(x, y, groups = groups, subscripts = subscripts, 
        cex = group.symbols.size, pch = group.symbols, col = "darkblue")
       },
    key = list(space = "top", 
    text = list(group.labels,col = "black"),
    points = list(pch = group.symbols, cex = group.symbols.size, 
    col = "darkblue")))
     
# employ training-and-test regimen for model validation
set.seed(1234) # set seed for repeatability of training-and-test split
training_test <- c(rep(1,length=trunc((2/3)*nrow(dodgers))),
rep(2,length=(nrow(dodgers) - trunc((2/3)*nrow(dodgers)))))
dodgers$training_test <- sample(training_test) # random permutation 
dodgers$training_test <- factor(dodgers$training_test, 
  levels=c(1,2), labels=c("TRAIN","TEST"))
dodgers.train <- subset(dodgers, training_test == "TRAIN")
print(str(dodgers.train)) # check training data frame
dodgers.test <- subset(dodgers, training_test == "TEST")
print(str(dodgers.test)) # check test data frame

# specify a simple model with bobblehead entered last
my.model <- {attend ~ ordered_month + ordered_day_of_week + bobblehead}

# fit the model to the training set
train.model.fit <- lm(my.model, data = dodgers.train)
# summary of model fit to the training set
print(summary(train.model.fit))
# training set predictions from the model fit to the training set
dodgers.train$predict_attend <- predict(train.model.fit) 

# test set predictions from the model fit to the training set
dodgers.test$predict_attend <- predict(train.model.fit, 
  newdata = dodgers.test)

# compute the proportion of response variance
# accounted for when predicting out-of-sample
cat("\n","Proportion of Test Set Variance Accounted for: ",
round((with(dodgers.test,cor(attend,predict_attend)^2)),
  digits=3),"\n",sep="")

# merge the training and test sets for plotting
dodgers.plotting.frame <- rbind(dodgers.train,dodgers.test)

# generate predictive modeling visual for management
group.labels <- c("No Bobbleheads","Bobbleheads")
group.symbols <- c(21,24)
group.colors <- c("black","black") 
group.fill <- c("black","red")  
xyplot(predict_attend/1000 ~ attend/1000 | training_test, 
       data = dodgers.plotting.frame, groups = bobblehead, cex = 2,
       pch = group.symbols, col = group.colors, fill = group.fill, 
       layout = c(2, 1), xlim = c(20,65), ylim = c(20,65), 
       aspect=1, type = c("p","g"),
       panel=function(x,y, ...)
            {panel.xyplot(x,y,...)
             panel.segments(25,25,60,60,col="black",cex=2)
            },
       strip=function(...) strip.default(..., style=1),
       xlab = "Actual Attendance (thousands)", 
       ylab = "Predicted Attendance (thousands)",
       key = list(space = "top", 
              text = list(rev(group.labels),col = rev(group.colors)),
              points = list(pch = rev(group.symbols), 
              col = rev(group.colors),
              fill = rev(group.fill))))            
        
# use the full data set to obtain an estimate of the increase in
# attendance due to bobbleheads, controlling for other factors 
my.model.fit <- lm(my.model, data = dodgers)  # use all available data
print(summary(my.model.fit))
# tests statistical significance of the bobblehead promotion
# type I anova computes sums of squares for sequential tests
print(anova(my.model.fit))  

cat("\n","Estimated Effect of Bobblehead Promotion on Attendance: ",
round(my.model.fit$coefficients[length(my.model.fit$coefficients)],
digits = 0),"\n",sep="")

# standard graphics provide diagnostic plots
plot(my.model.fit)

# additional model diagnostics drawn from the car package
library(car)
residualPlots(my.model.fit)
marginalModelPlots(my.model.fit)
print(outlierTest(my.model.fit))

# Traditional Conjoint Analysis (R)

# R preliminaries to get the user-defined function for spine chart: 
# place the spine chart code file <R_utility_program_1.R>
# in your working directory and execute it by
#     source("R_utility_program_1.R")
# Or if you have the R binary file in your working directory, use
#     load(file="mtpa_spine_chart.Rdata")

# spine chart accommodates up to 45 part-worths on one page
# |part-worth| <= 40 can be plotted directly on the spine chart
# |part-worths| > 40 can be accommodated through standardization

print.digits <- 2  # set number of digits on print and spine chart

library(support.CEs)  # package for survey construction 

# generate a balanced set of product profiles for survey
provider.survey <- Lma.design(attribute.names = 
  list(brand = c("AT&T","T-Mobile","US Cellular","Verizon"), 
  startup = c("$100","$200","$300","$400"), 
  monthly = c("$100","$200","$300","$400"),
  service = c("4G NO","4G YES"), 
  retail = c("Retail NO","Retail YES"),
  apple = c("Apple NO","Apple YES"), 
  samsung = c("Samsung NO","Samsung YES"), 
  google = c("Nexus NO","Nexus YES")), nalternatives = 1, nblocks=1, seed=9999)
print(questionnaire(provider.survey))  # print survey design for review

sink("questions_for_survey.txt")  # send survey to external text file
questionnaire(provider.survey)
sink() # send output back to the screen

# user-defined function for plotting descriptive attribute names 
effect.name.map <- function(effect.name) { 
  if(effect.name=="brand") return("Mobile Service Provider")
  if(effect.name=="startup") return("Start-up Cost")
  if(effect.name=="monthly") return("Monthly Cost")
  if(effect.name=="service") return("Offers 4G Service")
  if(effect.name=="retail") return("Has Nearby Retail Store")
  if(effect.name=="apple") return("Sells Apple Products")
  if(effect.name=="samsung") return("Sells Samsung Products")
  if(effect.name=="google") return("Sells Google/Nexus Products")
  } 

# read in conjoint survey profiles with respondent ranks
conjoint.data.frame <- read.csv("mobile_services_ranking.csv")

# set up sum contrasts for effects coding as needed for conjoint analysis
options(contrasts=c("contr.sum","contr.poly"))

# main effects model specification
main.effects.model <- {ranking ~ brand + startup + monthly + service + 
  retail + apple + samsung + google}

# fit linear regression model using main effects only (no interaction terms)
main.effects.model.fit <- lm(main.effects.model, data=conjoint.data.frame)
print(summary(main.effects.model.fit)) 

# save key list elements of the fitted model as needed for conjoint measures
conjoint.results <- 
  main.effects.model.fit[c("contrasts","xlevels","coefficients")]

conjoint.results$attributes <- names(conjoint.results$contrasts)

# compute and store part-worths in the conjoint.results list structure
part.worths <- conjoint.results$xlevels  # list of same structure as xlevels
end.index.for.coefficient <- 1  # intitialize skipping the intercept
part.worth.vector <- NULL # used for accumulation of part worths
for(index.for.attribute in seq(along=conjoint.results$contrasts)) {
  nlevels <- length(unlist(conjoint.results$xlevels[index.for.attribute]))
  begin.index.for.coefficient <- end.index.for.coefficient + 1
  end.index.for.coefficient <- begin.index.for.coefficient + nlevels -2
  last.part.worth <- -sum(conjoint.results$coefficients[
    begin.index.for.coefficient:end.index.for.coefficient])
  part.worths[index.for.attribute] <- 
    list(as.numeric(c(conjoint.results$coefficients[
      begin.index.for.coefficient:end.index.for.coefficient],
      last.part.worth)))
  part.worth.vector <- 
    c(part.worth.vector,unlist(part.worths[index.for.attribute]))    
  } 
conjoint.results$part.worths <- part.worths

# compute standardized part-worths
standardize <- function(x) {(x - mean(x)) / sd(x)}
conjoint.results$standardized.part.worths <- 
  lapply(conjoint.results$part.worths,standardize)
 
# compute and store part-worth ranges for each attribute 
part.worth.ranges <- conjoint.results$contrasts
for(index.for.attribute in seq(along=conjoint.results$contrasts)) 
  part.worth.ranges[index.for.attribute] <- 
  dist(range(conjoint.results$part.worths[index.for.attribute]))
conjoint.results$part.worth.ranges <- part.worth.ranges

sum.part.worth.ranges <- sum(as.numeric(conjoint.results$part.worth.ranges))

# compute and store importance values for each attribute 
attribute.importance <- conjoint.results$contrasts
for(index.for.attribute in seq(along=conjoint.results$contrasts)) 
  attribute.importance[index.for.attribute] <- 
  (dist(range(conjoint.results$part.worths[index.for.attribute]))/
  sum.part.worth.ranges) * 100
conjoint.results$attribute.importance <- attribute.importance
 
# data frame for ordering attribute names
attribute.name <- names(conjoint.results$contrasts)
attribute.importance <- as.numeric(attribute.importance)
temp.frame <- data.frame(attribute.name,attribute.importance)
conjoint.results$ordered.attributes <- 
  as.character(temp.frame[sort.list(
  temp.frame$attribute.importance,decreasing = TRUE),"attribute.name"])

# respondent internal consistency added to list structure
conjoint.results$internal.consistency <- summary(main.effects.model.fit)$r.squared 
 
# user-defined function for printing conjoint measures
if (print.digits == 2) 
  pretty.print <- function(x) {sprintf("%1.2f",round(x,digits = 2))} 
if (print.digits == 3) 
  pretty.print <- function(x) {sprintf("%1.3f",round(x,digits = 3))} 
 
# report conjoint measures to console 
# use pretty.print to provide nicely formated output
for(k in seq(along=conjoint.results$ordered.attributes)) {
  cat("\n","\n")
  cat(conjoint.results$ordered.attributes[k],"Levels: ",
  unlist(conjoint.results$xlevels[conjoint.results$ordered.attributes[k]]))
  
  cat("\n"," Part-Worths:  ")
  cat(pretty.print(unlist(conjoint.results$part.worths
    [conjoint.results$ordered.attributes[k]])))
    
  cat("\n"," Standardized Part-Worths:  ")
  cat(pretty.print(unlist(conjoint.results$standardized.part.worths
    [conjoint.results$ordered.attributes[k]])))  
    
  cat("\n"," Attribute Importance:  ")
  cat(pretty.print(unlist(conjoint.results$attribute.importance
    [conjoint.results$ordered.attributes[k]])))
  }

# plotting of spine chart begins here
# all graphical output is routed to external pdf file
pdf(file = "fig_preference_mobile_services_results.pdf", width=8.5, height=11)
spine.chart(conjoint.results)
dev.off()  # close the graphics output device

# Association Rules for Market Basket Analysis (R)

library(arules)  # association rules
library(arulesViz)  # data visualization of association rules
library(RColorBrewer)  # color palettes for plots

data(Groceries)  # grocery transcations object from arules package

# show the dimensions of the transactions object
print(dim(Groceries))

print(dim(Groceries)[1])  # 9835 market baskets for shopping trips
print(dim(Groceries)[2])  # 169 initial store items  

# examine frequency for each item with support greater than 0.025
pdf(file="fig_market_basket_initial_item_support.pdf", 
  width = 8.5, height = 11)
itemFrequencyPlot(Groceries, support = 0.025, cex.names=0.8, xlim = c(0,0.3),
  type = "relative", horiz = TRUE, col = "dark red", las = 1,
  xlab = paste("Proportion of Market Baskets Containing Item",
    "\n(Item Relative Frequency or Support)"))
dev.off()    

# explore possibilities for combining similar items
print(head(itemInfo(Groceries))) 
print(levels(itemInfo(Groceries)[["level1"]]))  # 10 levels... too few 
print(levels(itemInfo(Groceries)[["level2"]]))  # 55 distinct levels

# aggregate items using the 55 level2 levels for food categories
# to create a more meaningful set of items
groceries <- aggregate(Groceries, itemInfo(Groceries)[["level2"]])  

print(dim(groceries)[1])  # 9835 market baskets for shopping trips
print(dim(groceries)[2])  # 55 final store items (categories)  

pdf(file="fig_market_basket_final_item_support.pdf", width = 8.5, height = 11)
itemFrequencyPlot(groceries, support = 0.025, cex.names=1.0, xlim = c(0,0.5),
  type = "relative", horiz = TRUE, col = "blue", las = 1,
  xlab = paste("Proportion of Market Baskets Containing Item",
    "\n(Item Relative Frequency or Support)"))
dev.off()   

# obtain large set of association rules for items by category and all shoppers
# this is done by setting very low criteria for support and confidence
first.rules <- apriori(groceries, 
  parameter = list(support = 0.001, confidence = 0.05))
print(summary(first.rules))  # yields 69,921 rules... too many

# select association rules using thresholds for support and confidence 
second.rules <- apriori(groceries, 
  parameter = list(support = 0.025, confidence = 0.05))
print(summary(second.rules))  # yields 344 rules
  
# data visualization of association rules in scatter plot
pdf(file="fig_market_basket_rules.pdf", width = 8.5, height = 8.5)
plot(second.rules, 
  control=list(jitter=2, col = rev(brewer.pal(9, "Greens")[4:9])),
  shading = "lift")   
dev.off()    
  
# grouped matrix of rules 
pdf(file="fig_market_basket_rules_matrix.pdf", width = 8.5, height = 8.5)
plot(second.rules, method="grouped",   
  control=list(col = rev(brewer.pal(9, "Greens")[4:9])))
dev.off()    

# select rules with vegetables in consequent (right-hand-side) item subsets
vegie.rules <- subset(second.rules, subset = rhs %pin% "vegetables")
inspect(vegie.rules)  # 41 rules

# sort by lift and identify the top 10 rules
top.vegie.rules <- head(sort(vegie.rules, decreasing = TRUE, by = "lift"), 10)
inspect(top.vegie.rules) 

pdf(file="fig_market_basket_farmer_rules.pdf", width = 11, height = 8.5)
plot(top.vegie.rules, method="graph", 
  control=list(type="items"), 
  shading = "lift")
dev.off()  

# Analysis of Economic Time Series (R)

# economic time series gathered with this program are continually
# updated... so predictive models, forecasts, and data visualizations
# produced by this program may differ from those shown in the book

library(quantmod) # use for gathering and charting economic data
library(lubridate) # date functions
library(latticeExtra) # package used for horizon plot
library(forecast) # functions for time series forecasting 
library(lmtest) # for Granger test of causality

par(mfrow = c(2,2)) # four plots on one window/page

# Economic Data from Federal Reserve Bank of St. Louis (FRED system)
# National Civilian Unemployment Rate (monthly, percentage)
getSymbols("UNRATENSA", src="FRED", return.class = "xts")
ER <- 100 - UNRATENSA # convert to employment rate
dimnames(ER)[2] <- "ER"
chartSeries(ER,theme="white")
ER.data.frame <- as.data.frame(ER)
ER.data.frame$date <- ymd(rownames(ER.data.frame))
ER.time.series <- ts(ER.data.frame$ER, 
  start = c(year(min(ER.data.frame$date)),month(min(ER.data.frame$date))),
  end = c(year(max(ER.data.frame$date)),month(max(ER.data.frame$date))),
  frequency=12)

# Manufacturers' New Orders: Durable Goods (millions of dollars) 
getSymbols("DGORDER", src="FRED", return.class = "xts")
DGO <- DGORDER/1000 # convert to billions of dollars
dimnames(DGO)[2] <- "DGO" # use simple name for index
chartSeries(DGO, theme="white") 
DGO.data.frame <- as.data.frame(DGO)
DGO.data.frame$DGO <- DGO.data.frame$DGO
DGO.data.frame$date <- ymd(rownames(DGO.data.frame))
DGO.time.series <- ts(DGO.data.frame$DGO, 
  start = c(year(min(DGO.data.frame$date)),month(min(DGO.data.frame$date))),
  end = c(year(max(DGO.data.frame$date)),month(max(DGO.data.frame$date))),
  frequency=12)

# University of Michigan Index of Consumer Sentiment (1Q 1966 = 100)
getSymbols("UMCSENT", src="FRED", return.class = "xts")
ICS <- UMCSENT # use simple name for xts object
dimnames(ICS)[2] <- "ICS" # use simple name for index
chartSeries(ICS, theme="white")
ICS.data.frame <- as.data.frame(ICS)
ICS.data.frame$ICS <- ICS.data.frame$ICS
ICS.data.frame$date <- ymd(rownames(ICS.data.frame))
ICS.time.series <- ts(ICS.data.frame$ICS, 
  start = c(year(min(ICS.data.frame$date)), month(min(ICS.data.frame$date))),
  end = c(year(max(ICS.data.frame$date)),month(max(ICS.data.frame$date))),
  frequency=12)

# New Homes Sold in the US, not seasonally adjusted (monthly, millions)
getSymbols("HSN1FNSA",src="FRED",return.class = "xts")
NHS <- HSN1FNSA
dimnames(NHS)[2] <- "NHS" # use simple name for index
chartSeries(NHS, theme="white")
NHS.data.frame <- as.data.frame(NHS)
NHS.data.frame$NHS <- NHS.data.frame$NHS
NHS.data.frame$date <- ymd(rownames(NHS.data.frame))
NHS.time.series <- ts(NHS.data.frame$NHS, 
  start = c(year(min(NHS.data.frame$date)),month(min(NHS.data.frame$date))),
  end = c(year(max(NHS.data.frame$date)),month(max(NHS.data.frame$date))),
  frequency=12)

# define multiple time series object
economic.mts <- cbind(ER.time.series, DGO.time.series, ICS.time.series,
  NHS.time.series) 
  dimnames(economic.mts)[[2]] <- c("ER","DGO","ICS","NHS") # keep simple names 
modeling.mts <- na.omit(economic.mts) # keep overlapping time intervals only

# plot multiple time series 
pdf(file="fig_economic_analysis_mts_R.pdf",width = 8.5,height = 11)    
plot(modeling.mts,main="")
dev.off()

# create new indexed series IER using base date March 1997
ER0 <- mean(as.numeric(window(ER.time.series,start=c(1997,3),end=c(1997,3))))
IER.time.series <- (ER.time.series/ER0) * 100  

# create new indexed series IDGO using base date March 1997
DGO0 <- mean(as.numeric(window(DGO.time.series,start=c(1997,3),end=c(1997,3))))
IDGO.time.series <- (DGO.time.series/DGO0) * 100  

# create new indexed series INHS using base date March 1997
NHS0 <- mean(as.numeric(window(NHS.time.series,start=c(1997,3),end=c(1997,3))))
INHS.time.series <- (NHS.time.series/NHS0) * 100  

# create a multiple time series object from the index series
economic.mts <- cbind(IER.time.series,
IDGO.time.series,
ICS.time.series,
INHS.time.series) 
dimnames(economic.mts)[[2]] <- c("IER","IDGO","ICS","INHS")
working.economic.mts <- na.omit(economic.mts) # months complete for all series
# partial listing to check calculations
print(head(working.economic.mts))

# plot multiple economic time series as horizon plot
# using the index 100 as the reference point (origin = 100)
# with scaling fixed across the index numbers (horizonscale = 25)
# use ylab rather than strip.left, for readability
# also shade any times with missing data values.
# latticeExtra package used for horizon plot
pdf(file="fig_economic_time_series_indexed_R.pdf",width = 8.5,height = 11)
print(horizonplot(working.economic.mts, colorkey = TRUE,
  layout = c(1,4), strip.left = FALSE, origin = 100, horizonscale = 25,
  ylab = list(rev(colnames(working.economic.mts)), rot = 0, cex = 0.7)) +
  layer_(panel.fill(col = "gray90"), panel.xblocks(..., col = "white")))
dev.off()
  
# return to the individual economic time series prior to indexing  
# functions from forecast package for time series forecasting 

# ARIMA model fit to the employment rate data
ER.auto.arima.fit <- auto.arima(ER.time.series, d=NA, D=NA, max.p=3, max.q=3,
  max.P=2, max.Q=2, max.order=3, start.p=2, start.q=2,
  start.P=1, start.Q=1, stationary=FALSE, seasonal=TRUE,
  ic=c("aic"), stepwise=TRUE, trace=FALSE,
  approximation=FALSE, xreg=NULL,
  test=c("kpss","adf","pp"), seasonal.test=c("ocsb","ch"),
  allowdrift=FALSE, lambda=NULL, parallel=FALSE, num.cores=NULL)
print(summary(ER.auto.arima.fit))
# national employment rate two-year forecast (horizon h = 24 months) 
ER.forecast <- forecast.Arima(ER.auto.arima.fit, h=24, level=c(90), 
  fan=FALSE, xreg=NULL, bootstrap=FALSE)
# plot national employment rate time series with two-year forecast 
pdf(file = "fig_economic_analysis_er_forecast_R.pdf", width = 11, height = 8.5)
plot(ER.forecast,main="", ylab="Employment Rate (100 - Unemployment Rate)",
  xlab = "Time", las = 1, lwd = 1.5)
dev.off()

# ARIMA model fit to the manufacturers durable goods orders
DGO.auto.arima.fit <- auto.arima(DGO.time.series, d=NA, D=NA, max.p=3, max.q=3,
  max.P=2, max.Q=2, max.order=3, start.p=2, start.q=2,
  start.P=1, start.Q=1, stationary=FALSE, seasonal=TRUE,
  ic=c("aic"), stepwise=TRUE, trace=FALSE,
  approximation=FALSE, xreg=NULL,
  test=c("kpss","adf","pp"), seasonal.test=c("ocsb","ch"),
  allowdrift=FALSE, lambda=NULL, parallel=FALSE, num.cores=NULL)
print(summary(DGO.auto.arima.fit))
# durable goods orders two-year forecast (horizon h = 24 months) 
DGO.forecast <- forecast.Arima(DGO.auto.arima.fit, h=24, level=c(90), 
  fan=FALSE, xreg=NULL, bootstrap=FALSE)
# plot durable goods time series with two-year forecast 
pdf(file = "fig_economic_analysis_dgo_forecast_R.pdf", width = 11, height = 8.5)
plot(DGO.forecast,main="", ylab="Durable Goods Orders (billions of dollars)",
  xlab = "Time", las = 1, lwd = 1.5)
dev.off()  

# ARIMA model fit to index of consumer sentiment
ICS.auto.arima.fit <- auto.arima(ICS.time.series, d=NA, D=NA, max.p=3, max.q=3,
  max.P=2, max.Q=2, max.order=3, start.p=2, start.q=2,
  start.P=1, start.Q=1, stationary=FALSE, seasonal=TRUE,
  ic=c("aic"), stepwise=TRUE, trace=FALSE,
  approximation=FALSE, xreg=NULL,
  test=c("kpss","adf","pp"), seasonal.test=c("ocsb","ch"),
  allowdrift=FALSE, lambda=NULL, parallel=FALSE, num.cores=NULL)
print(summary(ICS.auto.arima.fit))
# index of consumer sentiment two-year forecast (horizon h = 24 months) 
ICS.forecast <- forecast.Arima(ICS.auto.arima.fit, h=24, level=c(90), 
  fan=FALSE, xreg=NULL, bootstrap=FALSE)
# plot index of consumer sentiment time series with two-year forecast 
pdf(file = "fig_economic_analysis_ics_forecast_R.pdf", width = 11, height = 8.5)
plot(ICS.forecast,main="", ylab="Index of Consumer Sentiment (1Q 1966 = 100)",
  xlab = "Time", las = 1, lwd = 1.5)
dev.off()

# ARIMA model fit to new home sales
NHS.auto.arima.fit <- auto.arima(NHS.time.series, d=NA, D=NA, max.p=3, max.q=3,
  max.P=2, max.Q=2, max.order=3, start.p=2, start.q=2,
  start.P=1, start.Q=1, stationary=FALSE, seasonal=TRUE,
  ic=c("aic"), stepwise=TRUE, trace=FALSE,
  approximation=FALSE, xreg=NULL,
  test=c("kpss","adf","pp"), seasonal.test=c("ocsb","ch"),
  allowdrift=FALSE, lambda=NULL, parallel=FALSE, num.cores=NULL)
print(summary(NHS.auto.arima.fit))
# new home sales two-year forecast (horizon h = 24 months) 
NHS.forecast <- forecast.Arima(NHS.auto.arima.fit, h=24, level=c(90), 
  fan=FALSE, xreg=NULL, bootstrap=FALSE)
# plot new home sales time series with two-year forecast 
pdf(file = "fig_economic_analysis_nhs_forecast_R.pdf", width = 11, height = 8.5)
plot(NHS.forecast,main="", ylab="New Homes Sold (millions)",
  xlab = "Time", las = 1, lwd = 1.5)
dev.off()

# Which regressors have potential as leading indicators?
# look for relationships across three of the time series
# using the period of overlap for those series
# function from lmtest package for Granger test of causality
grangertest(ICS~ER, order = 3, data=modeling.mts)
grangertest(ICS~DGO, order = 3, data=modeling.mts)
grangertest(DGO~ER, order = 3, data=modeling.mts)
grangertest(DGO~ICS, order = 3, data=modeling.mts)
grangertest(ER~DGO, order = 3, data=modeling.mts)
grangertest(ER~ICS, order = 3, data=modeling.mts)

# export data frames for economic measures 
write.csv(ER.data.frame, file = "FRED_ER_data.csv", row.names = FALSE)
write.csv(DGO.data.frame, file = "FRED_DGO_data.csv", row.names = FALSE)
write.csv(ICS.data.frame, file = "FRED_ICS_data.csv", row.names = FALSE)
write.csv(NHS.data.frame, file = "FRED_NHS_data.csv", row.names = FALSE)

# save current workspace
save.image(file = "R_workspace.Rdata")

# Workforce Scheduling for Anonymous Bank Call Center (R)

library(lubridate)  # date functions
library(grid)  # graphics utilities needed for split-plotting
library(ggplot2)  # graphics package with ribbon plot
library(queueing)  # queueing functions, including Erlang C
library(lpSolve)  # linear programming package

# ensure that two binary files are in the working directory
# these come from running R code from R_Utilities_Appendix
# source("R_utility_program_3.R") provides split-plotting utilities
load("mtpa_split_plotting_utilities.Rdata")
# source("R_utility_program_4.R") provides wait-time ribbon plots
load("mtpa_wait_time_ribbon_utility.Rdata")

put.title.on.plots <- TRUE  # put title on wait-time ribbon plots
# The call center data from "Anonymous Bank" in Israel were provided 
# by Avi Mandelbaum, with the help of Ilan Guedj.
# data source: http://ie.technion.ac.il/serveng/callcenterdata/index.html
# variable names and definitions from documentation 
# VRU  Voice Response Unit automated service
# vru.line  6 digits Each entering phone-call is first routed through a VRU: 
#           There are 6 VRUs labeled AA01 to AA06. Each VRU has several lines
#           labeled 1-16. There are a total of 65 lines. Each call is assigned 
#           a VRU number and a line number.
# call.id  unique call identifier
# customer.id  unique identifier for existing customer, zero for non-customer  
# priority  0 or 1 for inidentified or regular customers
#           2 for priority customers who receive advanced position in queue
# type  type of service
#       PS  regular activity (coded 'PS' for 'Peilut Shotefet')
#       PE  regular activity in English (coded 'PE' for 'Peilut English')
#       IN  internet consulting (coded 'IN' for 'Internet')
#       NE  stock exchange activity (coded 'NE' for 'Niarot Erech') 
#       NW  potential customer getting information
#       TT  customers who left a message asking the bank to return their call 
#           but, while the system returned their call, the calling-agent became 
#           busy hence the customers were put on hold in the queue.
# date  year-month-day
# vru_entry  time that the phone-call enters the call-center or VRU
# vru_exit  time of exit from VRU directly to service or to queue
# vru_time  time in seconds spent in the VRU 
#           (calculated by exit_time â€“ entry_time)
# q_start  time of joining the queue (00:00:00 for customers who abandon VRU
#          or do not enter the queue) 
# q_exit  time in seconds of exiting queue to receive service or abandonment
# q_time  time spent in queue (calculated by q_exit â€“ q_start)
# outcome  AGENT = service
#          HANG = hang up
#          PHANTOM = a virtual call to be ignored
# ser_start  time of beginning of service by agent
# ser_exit  time of end of service by agent
# ser_time  service duration in seconds (calculated by ser_exit â€“ ser_start)
# server  name of agent, NO_SERVER if no service provided

# focus upon February 1999
call.center.input.data <- read.table("data_anonymous_bank_february.txt", 
  header = TRUE, colClasses = c("character","integer","numeric",
  "integer","character","character","character","character","integer",
  "character","character","integer","factor","character","character",
  "integer","character"))
  
# check data frame object and variable values
print(summary(call.center.input.data))

# delete PHANTOM calls
call.center.data <- subset(call.center.input.data, subset = (outcome != "PHANTOM"))

# negative VRU times make no sense... drop these rows from data frame
call.center.data <- subset(call.center.data, subset = (vru_time >= 0))

# calculate wait time as sum of vru_time and q_time
call.center.data$wait_time <- 
  call.center.data$vru_time + call.center.data$q_time

# define four-digit year so year is not read as 2099
# convert date string to date variable 
call.center.data$date <- paste("19", call.center.data$date, sep ="")
call.center.data$date <- ymd(call.center.data$date)

# identify day of the week 1 = Sunday ... 7 = Saturday
call.center.data$day_of_week <- wday(call.center.data$date)
call.center.data$day_of_week <- factor(call.center.data$day_of_week,
  levels = c(1:7), labels = c("Sunday","Monday","Tuesday",
  "Wednesday","Thursday","Friday","Saturday"))

# examine frequency of calls by day of week
print(table(call.center.data$day_of_week))

# identify the hour of entry into the system
time.list <- strsplit(call.center.data$vru_entry,":")
call.hour <- numeric(nrow(call.center.data))
for (index.for.call in 1:nrow(call.center.data)) 
  call.hour[index.for.call] <- as.numeric(time.list[[index.for.call]][1])
call.center.data$call_hour <- call.hour

# check frequency of calls in february by hour and day of week
print(with(call.center.data, table(day_of_week, call_hour)))

# select first week of February 1999 for data visualization and analysis
# that week began on Monday, February 1 and ended on Sunday, February 7
selected.week <- subset(call.center.data, subset = (date < ymd("19990208")))

# check frequency of calls in week by hour and day of week
print(with(selected.week, table(day_of_week, call_hour)))



# loop for day of week ignoring Saturdays in Isreal
day.of.week.list <- c("Monday","Tuesday",
  "Wednesday","Thursday","Friday","Sunday")
  
# wait-time ribbon plots for the six selected days
# call upon utility function wait.time.ribbon
# the utility makes use of grid split-plotting 
# place ribbon plot and text table/plot on each file
# each plot goes to its own external pdf file
for(index.day in seq(along=day.of.week.list)) {
  this.day.of.week <- day.of.week.list[index.day]
  pdf(file = paste("fig_operations_management_ribbon_",
  tolower(this.day.of.week),".pdf",sep=""), width = 11, height = 8.5)  
  if(put.title.on.plots) {
    ribbon.plot.title <- paste(this.day.of.week,"Call Center Operations")
    }
    else {
    ribbon.plot.title <- "" 
    }
  selected.day <- subset(selected.week, 
    subset = (day_of_week == this.day.of.week),
    select = c("call_hour","wait_time","ser_time","server"))
  colnames(selected.day) <- c("hour","wait","service","server")
  wait.time.ribbon(wait.service.data = selected.day, 
    title = ribbon.plot.title,
    use.text.tagging = TRUE, wait.time.goal = 30, wait.time.max = 90,
    plotting.min = 0, plotting.max = 250)    
  dev.off()  
  }

# select Wednesdays in February for the queueing model
wednesdays <- subset(call.center.data, subset = (day_of_week == "Wednesday"))

# compute arrival rate of calls as calls for hour  
# we do not use table() here because some hours could have zero calls
calls.for.hour <- numeric(24)
for(index.for.hour in 1:24) { 
# 24-hour clock has first hour coded as zero in input data file
  coded.index.for.hour <- index.for.hour - 1  
  this.hour.calls <- 
    subset(wednesdays, subset = (call_hour == coded.index.for.hour))  
  if(nrow(this.hour.calls) > 0) 
    calls.for.hour[index.for.hour] <- nrow(this.hour.calls)  
  }

# compute arrival rate as average number of calls into VRU per hour
hourly.arrival.rate <- calls.for.hour/4  # four Wednesdays in February

# service times can vary hour-by-hour due to differences 
# in service requests and individuals calling hour-by-hour
# begin by selecting calls that receive service
wednesdays.served <- subset(wednesdays, subset = (server != "NO_SERVER"))

hourly.mean.service.time <- numeric(24)
served.for.hour <- numeric(24)
for(index.for.hour in 1:24) { 
# 24-hour clock has first hour coded as zero in input data file
  coded.index.for.hour <- index.for.hour - 1  
  this.hour.calls <- 
    subset(wednesdays.served, subset = (call_hour == coded.index.for.hour))
  if(nrow(this.hour.calls) > 0) {
    served.for.hour[index.for.hour] <- nrow(this.hour.calls)
    hourly.mean.service.time[index.for.hour] <- mean(this.hour.calls$ser_time)
    }
  } 
  
# hourly service rate given the current numbers of service operators
hourly.served.rate <- served.for.hour/4  # four Wednesdays in February

# build data frame for plotting arrival and service rates
hour <- 1:24  # hour for horizontal axix of line chart
type <- rep("Arrived", length = 24)
value <- hourly.arrival.rate
arrival.data.frame <- data.frame(hour, value, type) 
type <- rep("Served", length = 24)
value <- hourly.served.rate
service.data.frame <- data.frame(hour, value, type) 
arrival.service.data.frame <- rbind(arrival.data.frame, service.data.frame)

pdf(file = "fig_operations_management_wednesdays_arrived_served.pdf", 
  width = 11, height = 8.5)
plotting.object <- ggplot(data = arrival.service.data.frame, 
  aes(x = hour, y = value, fill = type)) + 
  geom_line() +
  geom_point(size = 4, shape = 21) +
  scale_x_continuous(breaks = c(1,3,5,7,9,11,13,15,17,19,21,23,25),
    labels = 
      c("00","02","04","06","08","10","12","14","16","18","20","22","24")) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)) +
  labs(x = "Hour of Day (24-Hour Clock)", y = "Average Calls per Hour") +
  scale_fill_manual(values = c("yellow","dark green"), 
    guide = guide_legend(title = NULL))  +
  theme(legend.position = c(1,1), legend.justification = c(1,1)) +
  theme(legend.text = element_text(size=15)) +
  coord_fixed(ratio = 1/10)    
print(plotting.object)
dev.off()

# examine service times per service operator
# for hours with no service time information use the mean as value
hourly.mean.service.time <- 
  ifelse((hourly.mean.service.time == 0),
    mean(wednesdays.served$ser_time),
    hourly.mean.service.time) 
# compute service rate noting that there are 3600 seconds in an hour
# adding 60 seconds to each mean service time for time between calls
# this 60 seconds is the wrap up time or time an service agent remains 
# unavailable to answer a new call after a call has been completed
hourly.service.rate <- 3600/(hourly.mean.service.time + 60)

# we observe that mean service times do not vary that much hour-by-hour
# so we use the mean hourly service rate in queueing calculations
# mean(hourly.service.rate) is 14.86443
# so we use 15 calls per hour as the rate for one service operator
SERVICE.RATE <- 15

# C_erlang function from the queueing package
# inputs c = number of servers
#        r = ratio of rate of arrivals and rate of service
# returns the propability of waiting in queue because all servers are busy
# let us set a target for the probability of waiting in queue to be 0.50
# using while-loop iteration we determine the number of servers needed 
# we do this for each hour of the day knowing the hourly arrival rate

PROBABILITY.GOAL <- 0.50
servers.needed <- integer(24)  # initialize to zero
for(index.for.hour in 1:24) {
  if (hourly.arrival.rate[index.for.hour] > 0) {
    erlang.probability <- 1.00  # intialization prior to entering while-loop
    while (erlang.probability > PROBABILITY.GOAL) {
      servers.needed[index.for.hour] <- servers.needed[index.for.hour] + 1
      erlang.probability <- C_erlang(c = servers.needed[index.for.hour], 
          r = hourly.arrival.rate[index.for.hour]/SERVICE.RATE)
      }  # end while-loop for defining servers needed given probability goal 
    }  # end if-block for hours with calls
  }  # end for-loop for the hour

# the result for servers.needed is obtained as
# 1  1  1  0  1  1  1  4  8  9 10  9  8 16 10 10  6  7  8  8  6  6  5  4
# we will assume the bank call center will be closed hours 00 through 05
# but use the other values as the bank's needed numbers of servers
servers.needed[1:6] <- 0

cat("\n","----- Hourly Operator Requirements -----","\n")
print(servers.needed)

# read in case data for the structure of call center worker shifts
bank.shifts.data.frame <- read.csv("data_anonymous_bank_shifts.csv")

# examine the structure of the case data frame
print(str(bank.shifts.data.frame))

constraint.matrix <- as.matrix(bank.shifts.data.frame[,3:10])
cat("\n","----- Call Center Shift Constraint Matrix -----","\n")
print(constraint.matrix)

# six-hour shift salaries in Israeli sheqels 
# 1 ILS = 3.61 USD in June 2013
# these go into the objective function for integer programing
# with the objective of minimizing total costs
cost.vector <- c(252,288,180,180,180,288,288,288) 

call.center.schedule <- lp(const.mat=constraint.matrix,
const.rhs = servers.needed,
const.dir = rep(">=",times=8),
int.vec = 1:8,
objective = cost.vector,
direction = "min")

# prepare summary of the results for the call center problem
ShiftID <- 1:8
StartTime <- c(0,6,8,10,12,2,4,6)
# c("Midnight","6 AM","8 AM","10 AM","Noon","2 PM","4 PM","6 PM")
ShiftDuration <- rep(6,times=8)
HourlyShiftSalary <- c(42,48,30,30,30,48,48,48)
HourlyShiftCost <- call.center.schedule$objective # six x hourly shift salary
Solution <- call.center.schedule$solution  
ShiftCost <- call.center.schedule$solution * call.center.schedule$objective

call.center.summary <- 
  data.frame(ShiftID,StartTime,ShiftDuration,HourlyShiftSalary,
  HourlyShiftCost,Solution,ShiftCost)
  
cat("\n\n","Call Center Summary","\n\n")
print(call.center.summary)

# the solution is obtained by print(call.center.schedule) 
# or by summing across the hourly solution times the cost objective
 print(call.center.schedule) 
cat("\n\n","Call Center Summary Minimum Cost Solution:",sum(ShiftCost),"\n\n")
# build data frame for plotting the solution compared with need
hour <- 1:24  # hour for horizontal axix of line chart
type <- rep("Hourly Need", length = 24)
value <- servers.needed
needs.data.frame <- data.frame(hour, value, type) 
type <- rep("Optimal Solution", length = 24)
value <- schedule.fit.to.need <- 
  constraint.matrix %*% call.center.schedule$solution
solution.data.frame <- data.frame(hour, value, type) 
plotting.data.frame <- rbind(needs.data.frame, solution.data.frame)

# plot the solution... solution match to the workforce need
pdf(file = "fig_operations_management_solution.pdf", width = 11, height = 8.5)
plotting.object <- ggplot(data = plotting.data.frame, 
  aes(x = hour, y = value, fill = type)) + 
  geom_line() +
  geom_point(size = 4, shape = 21) +
  scale_x_continuous(breaks = c(1,3,5,7,9,11,13,15,17,19,21,23,25),
    labels = 
      c("00","02","04","06","08","10","12","14","16","18","20","22","24")) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)) +
  labs(x = "Hour of Day (24-Hour Clock)", y = "Number of Service Operators") +
  scale_fill_manual(values = c("white","blue"), 
    guide = guide_legend(title = NULL)) +
  theme(legend.position = c(1,1), legend.justification = c(1,1)) +
  theme(legend.text = element_text(size=15)) +
  coord_fixed(ratio = 2/2.25)    
print(plotting.object)
dev.off()

# Text Analysis of Movie Tag Lines (R)

# Note. Results from this program may differ from those published
#       in the book due to changes in the tm package.
#       The original analysis used the tm Dictionary() function,
#       which is no longer available in tm. This function has
#       been replaced by c(as.character()) to set the dictionary
#       as a character vector. Another necessary change concerns
#       the tolower() function, which must now be embedded within
#       the tm content_transformer() function.
#       The original analysis used the tm dissimilarity() function
#       to compute cosine similarities. This is no longer available,
#       so we use the proxy package and its dist() function.

# install these packages before bringing them in by library()
library(tm)  # text mining and document management
library(proxy)  # dissimilarity calculations by dist()
library(stringr)  # character manipulation with regular expressions
library(grid)  # grid graphics utilities
library(ggplot2)  # graphics
library(latticeExtra)  # package used for text horizon plot
library(wordcloud)  # provides utility for plotting non-overlapping text
library(cluster)  # cluster analysis

# R preliminaries to get the user-defined utilities for plotting 
# place the plotting code file <R_utility_program_3.R>
# in your working directory and execute it by
#     source("R_utility_program_3.R")
# Or if you have the R binary file in your working directory, use
#     load("mtpa_split_plotting_utilities.Rdata")
load("mtpa_split_plotting_utilities.Rdata")

# standardization needed for text measures
standardize <- function(x) {(x - mean(x)) / sd(x)}

# to begin with the original movie taglines data, use the utility
# program for reading those data in and preparing the data for analysis
#     source(R_utility_program_7.R)
# otherwise read in the comma-delimited text file with the parsed data
# creating the movies data frame for analysis
movies = read.csv(file = "movie_tagline_data_parsed.csv", stringsAsFactors = FALSE) 

# plot frequency of movies by year
pdf(file = "fig_text_movies_by_year_histogram.pdf", width = 11, height = 8.5)
ggplot.object <- ggplot(data = movies, aes(x = year)) +
  geom_histogram(binwidth = 1, fill = "blue", colour = "black") +
    labs(x = "Year of Release", 
         y = "Number of Movies in Database") +
         coord_fixed(ratio = 1/50) 

ggplot.print.with.margins(ggplot.object.name = ggplot.object,
  left.margin.pct=10, right.margin.pct=10,
  top.margin.pct=10,bottom.margin.pct=10) 
dev.off()  
    
# let us work with movies from 1974 to 2013
# creating an aggregate tagline_text collection for each year of interest
years.list <- 1974:2013
document.collection <- NULL  # initialize
for (index.for.year in seq(along=years.list)) {
 
  working.year.data.frame = 
    subset(movies, subset = (year == years.list[index.for.year]))

  tagline_text <- NULL
  for(index.for.movie in seq(along = working.year.data.frame$movie)) 
    tagline_text <- 
      paste(tagline_text, working.year.data.frame$tagline[index.for.movie])
   
  document <- PlainTextDocument(x = tagline_text, author = "Tom",
    description = paste("movie taglines for ",
    as.character(years.list[index.for.year]),sep = ""),
    id = paste("movies_",as.character(years.list[index.for.year]),sep=""), 
    heading = "taglines",
    origin = "IMDb", language = "en_US", 
    localmetadata = list(year = years.list[index.for.year])) 

# give each created document a unique name  
  if (years.list[index.for.year] == 1974) Y1974 <- document  
  if (years.list[index.for.year] == 1975) Y1975 <- document  
  if (years.list[index.for.year] == 1976) Y1976 <- document  
  if (years.list[index.for.year] == 1977) Y1977 <- document  
  if (years.list[index.for.year] == 1978) Y1978 <- document  
  if (years.list[index.for.year] == 1979) Y1979 <- document     
  if (years.list[index.for.year] == 1980) Y1980 <- document  
  if (years.list[index.for.year] == 1981) Y1981 <- document   
  if (years.list[index.for.year] == 1982) Y1982 <- document  
  if (years.list[index.for.year] == 1983) Y1983 <- document  
  if (years.list[index.for.year] == 1984) Y1984 <- document  
  if (years.list[index.for.year] == 1985) Y1985 <- document  
  if (years.list[index.for.year] == 1986) Y1986 <- document  
  if (years.list[index.for.year] == 1987) Y1987 <- document  
  if (years.list[index.for.year] == 1988) Y1988 <- document  
  if (years.list[index.for.year] == 1989) Y1989 <- document  
  if (years.list[index.for.year] == 1990) Y1990 <- document  
  if (years.list[index.for.year] == 1991) Y1991 <- document   
  if (years.list[index.for.year] == 1992) Y1992 <- document  
  if (years.list[index.for.year] == 1993) Y1993 <- document  
  if (years.list[index.for.year] == 1994) Y1994 <- document  
  if (years.list[index.for.year] == 1995) Y1995 <- document  
  if (years.list[index.for.year] == 1996) Y1996 <- document  
  if (years.list[index.for.year] == 1997) Y1997 <- document  
  if (years.list[index.for.year] == 1998) Y1998 <- document  
  if (years.list[index.for.year] == 1999) Y1999 <- document  
  if (years.list[index.for.year] == 2000) Y2000 <- document  
  if (years.list[index.for.year] == 2001) Y2001 <- document   
  if (years.list[index.for.year] == 2002) Y2002 <- document  
  if (years.list[index.for.year] == 2003) Y2003 <- document  
  if (years.list[index.for.year] == 2004) Y2004 <- document  
  if (years.list[index.for.year] == 2005) Y2005 <- document  
  if (years.list[index.for.year] == 2006) Y2006 <- document  
  if (years.list[index.for.year] == 2007) Y2007 <- document  
  if (years.list[index.for.year] == 2008) Y2008 <- document  
  if (years.list[index.for.year] == 2009) Y2009 <- document  
  if (years.list[index.for.year] == 2010) Y2010 <- document  
  if (years.list[index.for.year] == 2011) Y2011 <- document  
  if (years.list[index.for.year] == 2012) Y2012 <- document  
  if (years.list[index.for.year] == 2013) Y2013 <- document  
  } # end of for-loop for selected years
  
document.collection <- c(Y1974,Y1975,Y1976,Y1977,Y1978,Y1979,
  Y1980,Y1981,Y1982,Y1983,Y1984,Y1985,Y1986,Y1987,Y1988,Y1989,
  Y1990,Y1991,Y1992,Y1993,Y1994,Y1995,Y1996,Y1997,Y1998,Y1999,
  Y2000,Y2001,Y2002,Y2003,Y2004,Y2005,Y2006,
  Y2007,Y2008,Y2009,Y2010,Y2011,Y2012,Y2013)

# strip whitespace from the documents in the collection
document.collection <- tm_map(document.collection, stripWhitespace)

# convert uppercase to lowercase in the document collection
document.collection <- tm_map(document.collection, content_transformer(tolower))

# remove numbers from the document collection
document.collection <- tm_map(document.collection, removeNumbers)

# remove punctuation from the document collection
document.collection <- tm_map(document.collection, removePunctuation)

# using a standard list, remove English stopwords from the document collection
document.collection <- tm_map(document.collection, 
  removeWords, stopwords("english"))

# there is more we could do in terms of data preparation 
# stemming... looking for contractions... pronoun possessives... 

# we take what is clearly a "bag of words" approach here
# the workhorse technique will be TermDocumentMatrix()
# for creating a terms-by-documents matrix across the document collection
initial.movies.tdm <- TermDocumentMatrix(document.collection)

# remove sparse terms from the matrix and report the most common terms
# looking for additional stop words and stop word contractions to drop
examine.movies.tdm <- removeSparseTerms(initial.movies.tdm, sparse = 0.25)
top.words <- Terms(examine.movies.tdm)
print(top.words)  

# an analysis of this initial list of top terms shows a number of word 
# contractions which we might like to drop from further analysis, 
# recognizing them as stop words to be dropped from the document collection
more.stop.words <- c("cant","didnt","doesnt","dont","goes","isnt","hes",
  "shes","thats","theres","theyre","wont","youll","youre","youve") 
document.collection <- tm_map(document.collection, 
  removeWords, more.stop.words)
  
# create terms-by-documents matrix across the final document collection
movies.tdm <- TermDocumentMatrix(document.collection)

# save movie documents and document collection (corpus)
save("movies","document.collection","movies.tdm",
  file = "000_movies_data.Rdata")  

# remove sparse terms from the matrix and report the most common terms
examine.movies.tdm <- removeSparseTerms(movies.tdm, sparse = 0.25)
top.words <- Terms(examine.movies.tdm)
print(top.words)  # the result of this is a bag of 200 words

# now comes a test... 
# does looking at taglines hold promise as a way of identifying movie trends?
# if it does, then years closest in time should be closest to one
# another in a text measurement space as reflected, say, 
# by multidimensional scaling... 
# create a dictionary of the top words from the corpus
top.words.dictionary <- c(as.character(top.words))
  
# create terms-by-documents matrix using the mtpa.Dictionary
top.words.movies.tdm <- TermDocumentMatrix(document.collection, 
  list(dictionary = top.words.dictionary))

# dissimilarity measures and multidimensional scaling
# with wordlayout from the wordcloud package for non-overlapping labels
pdf(file = "fig_text_mds_1974_2013.pdf", width = 7, height = 7)
# years.dissimilarity.matrix <- 
#  dissimilarity(x = top.words.movies.tdm, y = NULL, method = "cosine")
years.dissimilarity.matrix <- 
    dist(t(inspect(top.words.movies.tdm)), method = "cosine")
years.mds.solution <- cmdscale(years.dissimilarity.matrix, k = 2, eig = TRUE)
x <- years.mds.solution$points[,1]
y <- -years.mds.solution$points[,2]  # rotated to be consistent with biplot
w <- c("1974","1975","1976","1977","1978","1979",
  "1980","1981","1982","1983","1984","1985","1986",
  "1987","1988","1989","1990","1991","1992","1993",
  "1994","1995","1996","1997","1998","1999","2000",
  "2001","2002","2003","2004","2005","2006","2007",
  "2008","2009","2010","2011","2012","2013")
plot(x,y,type="n", xlim = c(-0.075,0.075), ylim = c(-0.075,0.075),
  xlab = "First Dimension", ylab = "Second Dimension") 
lay <- wordlayout(x, y, w, xlim = c(-0.075,0.075), ylim = c(-0.075,0.075)) 
text(lay[,1]+.5*lay[,3],lay[,2]+.5*lay[,4],w)
dev.off()

# classification of words into groups for further analysis
# use transpose of the terms-by-document matrix and cluster analysis
words.distance.object <- 
  dist(x = as.matrix(top.words.movies.tdm), method = "euclidean")

pdf(file = "fig_text_hcluster_top_words.pdf", width = 11, height = 8.5)
top.words.hierarchical.clustering <- 
  agnes(words.distance.object,diss=TRUE,
    metric = "euclidean", stand = FALSE, method = "ward") 
plot(top.words.hierarchical.clustering, cex.lab = 0.05)
dev.off()

# hierarchical solution suggests that four or five clusters may work
number.of.clusters.test <- NULL
for(number.of.clusters in 2:20) {
  try.words.clustering <- pam(words.distance.object,diss=TRUE,
    metric = "euclidean", stand = FALSE, k = number.of.clusters) 
  number.of.clusters.test <- 
    rbind(number.of.clusters.test,
      data.frame(number.of.clusters, 
        ave.sil.width = try.words.clustering$silinfo$avg.width)) 
   cat("\n\n","Number of clusters: ",number.of.clusters,
     " Average silhouette width: ",try.words.clustering$silinfo$avg.width,
     "\nKey identified concepts: ",try.words.clustering$medoids,
     "\nCluster average silhouette widths: ")
   print(try.words.clustering$silinfo$clus.avg.widths)
  }  # end of for-loop for number-of-clusters test 
print(number.of.clusters.test)

# results suggest that five clusters may work best here 
# we examine these clusters and give them names corresponding to medoids
top.words.clustering <- pam(words.distance.object,diss=TRUE,
    metric = "euclidean", stand = FALSE, k = 5)

# review the clustering results
print(summary(top.words.clustering))

# the medoid identified through the clustering process
# is an object at the center of the cluster... 
# it is used to define the cluster here we identify their names
cat("\nKey Words Identified by Cluster Analysis: \n")
key.word.set <- top.words.clustering$medoids
print(key.word.set)

# convert the distance object to an actual distance matrix 
# for doing word searches directly on the matrix calculations
words.distance.matrix <- as.matrix(words.distance.object)

# for each medoid... identify the closest words from distance matrix
# let us choose the two closest words to have five lists of three words
# for further analysis... note that there is some overlap in word sets
for(index.for.key.word in seq(along=key.word.set)) {
  # identify the column for the key word
  key.word.column <- 
    words.distance.matrix[,c(key.word.set[index.for.key.word])]
  # sort the key word column by distance
  sorted.key.word.column <- sort(key.word.column)
  # the smallest distance will be the distance of the key word to itself
  # so choose the second through tenth words in from the sorted column
  print(sorted.key.word.column[1:5])
  if (index.for.key.word == 1) 
    loved.word.set <- names(sorted.key.word.column[1:3])
  if (index.for.key.word == 2) 
    worlds.word.set <- names(sorted.key.word.column[1:3])
  if (index.for.key.word == 3) 
    truth.word.set <- names(sorted.key.word.column[1:3])
  if (index.for.key.word == 4) 
    life.word.set <- names(sorted.key.word.column[1:3])
  if (index.for.key.word == 5) 
    story.word.set <- names(sorted.key.word.column[1:3])
  }

# turn the word sets into dictionaries for analysis 
loved.words.dictionary <- c(as.character(loved.word.set))
worlds.words.dictionary <- c(as.character(worlds.word.set))
truth.words.dictionary <- c(as.character(truth.word.set))
life.words.dictionary <- c(as.character(life.word.set))
story.words.dictionary <- c(as.character(story.word.set))

# do word counts across the dictionaries
year <- 1974:2013
total.words <- integer(length(year))
loved.words <- integer(length(year))
worlds.words <- integer(length(year))
truth.words <- integer(length(year))
life.words <- integer(length(year))
story.words <- integer(length(year))

for(index.for.document in seq(along=year)) {
  loved.words[index.for.document] <- 
    sum(termFreq(document.collection[[index.for.document]], 
    control = list(dictionary = loved.words.dictionary)))
    
  worlds.words[index.for.document] <- 
    sum(termFreq(document.collection[[index.for.document]], 
    control = list(dictionary = worlds.words.dictionary)))  
    
  truth.words[index.for.document] <- 
    sum(termFreq(document.collection[[index.for.document]], 
    control = list(dictionary = truth.words.dictionary))) 
    
  life.words[index.for.document] <- 
    sum(termFreq(document.collection[[index.for.document]], 
    control = list(dictionary = life.words.dictionary)))     
    
  story.words[index.for.document] <- 
    sum(termFreq(document.collection[[index.for.document]], 
    control = list(dictionary = story.words.dictionary)))     
    
  total.words[index.for.document] <- 
    length(movies.tdm[,index.for.document][["i"]])
  }

# gather the results up in a data frame
movie.analytics.data.frame <- data.frame(year, total.words,
  loved.words, worlds.words, truth.words, life.words, story.words) 

# compute text measures as percentages of words in each set

movie.analytics.data.frame$LOVED <- 
  100 * movie.analytics.data.frame$loved.words / 
    movie.analytics.data.frame$total.words
LOVED <- standardize(movie.analytics.data.frame$LOVED)    
LOVED.ts <- ts(LOVED, start = c(1974,1), end = c(2013,1), frequency = 1)
  
movie.analytics.data.frame$WORLDS <- 
  100 * movie.analytics.data.frame$worlds.words / 
    movie.analytics.data.frame$total.words
WORLDS <- standardize(movie.analytics.data.frame$WORLDS)    
WORLDS.ts <- ts(WORLDS, start = c(1974,1), end = c(2013,1), frequency = 1)
  
movie.analytics.data.frame$TRUTH <- 
  100 * movie.analytics.data.frame$truth.words / 
    movie.analytics.data.frame$total.words
TRUTH <- standardize(movie.analytics.data.frame$TRUTH)    
TRUTH.ts <- ts(TRUTH, start = c(1974,1), end = c(2013,1), frequency = 1)
  
movie.analytics.data.frame$LIFE <- 
  100 * movie.analytics.data.frame$life.words / 
    movie.analytics.data.frame$total.words
LIFE <- standardize(movie.analytics.data.frame$LIFE)    
LIFE.ts <- ts(LIFE, start = c(1974,1), end = c(2013,1), frequency = 1)
  
movie.analytics.data.frame$STORY <- 
  100 * movie.analytics.data.frame$story.words / 
    movie.analytics.data.frame$total.words
STORY <- standardize(movie.analytics.data.frame$STORY)    
STORY.ts <- ts(STORY, start = c(1974,1), end = c(2013,1), frequency = 1)

# data frame of standardized text measures
text.measures.data.frame <- data.frame(LOVED,WORLDS,TRUTH,LIFE,STORY)
rownames(text.measures.data.frame) <- 1974:2013

principal.components.solution <- 
  princomp(text.measures.data.frame, cor = TRUE)
print(summary(principal.components.solution))  
# biplot rendering of text measures and documents by year
pdf(file = "fig_text_text_measures_biplot.pdf", width = 8.5, height = 11)
biplot(principal.components.solution, xlab = "First Pricipal Component",
  ylab = "Second Principal Component", col = c("black","darkblue"), las = 1)
dev.off()

# multiple time series object for text measures
text.measures.mts <- cbind(LOVED.ts, WORLDS.ts, TRUTH.ts, LIFE.ts, STORY.ts)
colnames(text.measures.mts) <- c("LOVED","WORLDS","TRUTH","LIFE","STORY")

# text horizons for forty years of movies
pdf(file = "fig_text_horizon_1974_2013.pdf", width = 8.5, height = 11)
print(horizonplot(text.measures.mts, colorkey = TRUE,
  layout = c(1,5), strip.left = FALSE, horizonscale = 1,
  origin = 0,
  ylab = list(rev(colnames(text.measures.mts)), rot = 0, cex = 0.7)) +
  layer_(panel.fill(col = "gray90"), panel.xblocks(..., col = "white")))
dev.off()

# wordcloud for R program code and comments through book chapter 6
R.code.text <- scan("mtpa_R_code.txt", what = "char", sep = "\n")
# replace uppercase with lowercase letters
R.code.text <- tolower(R.code.text)  
# strip out all non-letters and return vector
R.code.text.preword.vector <- unlist(strsplit(R.code.text, "\\W"))
# drop all empty words 
R.code.text.vector <- 
  R.code.text.preword.vector[which(nchar(R.code.text.preword.vector) > 0)]
pdf(file = "fig_text_wordcloud_of_R_code.pdf", width = 11, height = 8.5)
set.seed(1234) 
wordcloud(R.code.text.vector,   min.freq = 10,
  max.words = 300,
  random.order=FALSE,
  random.color=FALSE,
  rot.per=0.0,
  colors="black",
  ordered.colors=FALSE, 
  use.r.layout=FALSE,
  fixed.asp=TRUE)
dev.off()

# wordcloud for Python program code and comments through book chapter 6
Python.code.text <- scan("mtpa_Python_code.txt", what = "char", sep = "\n")
# replace uppercase with lowercase letters
Python.code.text <- tolower(Python.code.text)
# strip out all non-letters and return vector
Python.code.text.preword.vector <- unlist(strsplit(Python.code.text, "\\W"))
# drop all empty words 
Python.code.text.vector <- 
  Python.code.text.preword.vector[which(nchar(Python.code.text.preword.vector) > 0)]
pdf(file = "fig_text_wordcloud_of_Python_code.pdf", width = 11, height = 8.5)
set.seed(1234) 
wordcloud(Python.code.text.vector,   min.freq = 10,
  max.words = 300,
  random.order=FALSE,
  random.color=FALSE,
  rot.per=0.0,
  colors="black",
  ordered.colors=FALSE, 
  use.r.layout=FALSE,
  fixed.asp=TRUE)
dev.off()

# Sentiment Analysis Using the Movie Ratings Data (R)

# Note. Results from this program may differ from those published
#       in the book due to changes in the tm package.
#       The original analysis used the tm Dictionary() function,
#       which is no longer available in tm. This function has
#       been replaced by c(as.character()) to set the dictionary
#       as a character vector. Another necessary change concerns
#       the tolower() function, which must now be embedded within
#       the tm content_transformer() function.
# 
# Despite changes in the tm functions, we have retained the
# earlier positive and negative word lists for scoring, as
# implemented in the code and utilities appendix under the file
# name <R_utility_program_5.R>, which is brought in by source().

# install these packages before bringing them in by library()
library(tm)  # text mining and document management
library(stringr)  # character manipulation with regular expressions
library(grid)  # grid graphics utilities
library(ggplot2)  # graphics
library(latticeExtra) # package used for text horizon plot
library(caret)  # for confusion matrix function
library(rpart)  # tree-structured modeling
library(e1071)  # support vector machines
library(randomForest)  # random forests
library(rpart.plot)  # plot tree-structured model information

# R preliminaries to get the user-defined utilities for plotting 
# place the plotting code file <R_utility_program_3.R>
# in your working directory and execute it by
#     source("R_utility_program_3.R")
# Or if you have the R binary file in your working directory, use
#     load("mtpa_split_plotting_utilities.Rdata")
load("mtpa_split_plotting_utilities.Rdata")

# standardization needed for text measures
standardize <- function(x) {(x - mean(x)) / sd(x)}

# convert to bytecodes to avoid "invalid multibyte string" messages
bytecode.convert <- function(x) {iconv(enc2utf8(x), sub = "byte")}

# read in positive and negative word lists from Hu and Liu (2004)
positive.data.frame <- read.table(file = "Hu_Liu_positive_word_list.txt",
  header = FALSE, colClasses = c("character"), row.names = NULL, 
  col.names = "positive.words")
positive.data.frame$positive.words <- 
  bytecode.convert(positive.data.frame$positive.words)
  
negative.data.frame <- read.table(file = "Hu_Liu_negative_word_list.txt",
  header = FALSE, colClasses = c("character"), row.names = NULL, 
  col.names = "negative.words")  
negative.data.frame$negative.words <- 
  bytecode.convert(negative.data.frame$negative.words)

# we use movie ratings data from Mass et al. (2011) 
# available at http://ai.stanford.edu/~amaas/data/sentiment/
# we set up a directory under our working directory structure
# /reviews/train/unsup/ for the unsupervised reviews

directory.location <- 
  paste(getwd(),"/reviews/train/unsup/",sep = "")  
unsup.corpus <- Corpus(DirSource(directory.location, encoding = "UTF-8"),
  readerControl = list(language = "en_US"))
print(summary(unsup.corpus))

document.collection <- unsup.corpus

# strip whitspace from the documents in the collection
document.collection <- tm_map(document.collection, stripWhitespace)

# convert uppercase to lowercase in the document collection
document.collection <- tm_map(document.collection, content_transformer(tolower))

# remove numbers from the document collection
document.collection <- tm_map(document.collection, removeNumbers)

# remove punctuation from the document collection
document.collection <- tm_map(document.collection, removePunctuation)

# using a standard list, remove English stopwords from the document collection
document.collection <- tm_map(document.collection, 
  removeWords, stopwords("english"))

# there is more we could do in terms of data preparation 
# stemming... looking for contractions... possessives... 
# previous analysis of a list of top terms showed a number of word 
# contractions which we might like to drop from further analysis, 
# recognizing them as stop words to be dropped from the document collection
initial.tdm <- TermDocumentMatrix(document.collection)
examine.tdm <- removeSparseTerms(initial.tdm, sparse = 0.96)
top.words <- Terms(examine.tdm)
print(top.words)  

more.stop.words <- c("cant","didnt","doesnt","dont","goes","isnt","hes",
  "shes","thats","theres","theyre","wont","youll","youre","youve") 
document.collection <- tm_map(document.collection, 
  removeWords, more.stop.words)
  
some.proper.nouns.to.remove <- 
  c("dick","ginger","hollywood","jack","jill","john","karloff",
    "kudrow","orson","peter","tcm","tom","toni","welles","william","wolheim")
document.collection <- tm_map(document.collection, 
  removeWords, some.proper.nouns.to.remove)
  
# there is still more we could do in terms of data preparation  
# but we will work with the bag of words we have for now
  
# the workhorse technique will be TermDocumentMatrix()
# for creating a terms-by-documents matrix across the document collection
# in previous text analytics with the taglines data we let the data
# guide us to the text measures... with sentiment analysis we have
# positive and negative dictionaries (to a large extent) defined in 
# advance of looking at the data...
# positive.words and negative.words lists were read in earlier
# these come from the work of Hu and Liu (2004)   
# positive.words = list of  positive words
# negative.words = list of  negative words
# we will start with these lists to build dictionaries 
# that seem to make sense for movie reviews analysis
# Hu.Liu.positive.dictionary <- Dictionary(positive.data.frame$positive.words)
Hu.Liu.positive.dictionary <- 
    c(as.character(positive.data.frame$positive.words))
reviews.tdm.Hu.Liu.positive <- TermDocumentMatrix(document.collection, 
  list(dictionary = Hu.Liu.positive.dictionary))
examine.tdm <- removeSparseTerms(reviews.tdm.Hu.Liu.positive, 0.95)
top.words <- Terms(examine.tdm)
print(top.words)  
Hu.Liu.frequent.positive <- findFreqTerms(reviews.tdm.Hu.Liu.positive, 25)
# this provides a list positive words occurring at least 25 times
# a review of this list suggests that all make sense (have content validity)
# test.positive.dictionary <- Dictionary(Hu.Liu.frequent.positive)
test.positive.dictionary <- c(as.character(Hu.Liu.frequent.positive))

# .... now for the negative words
# Hu.Liu.negative.dictionary <- Dictionary(negative.data.frame$negative.words)
Hu.Liu.negative.dictionary <- 
    c(as.character(negative.data.frame$negative.words))
reviews.tdm.Hu.Liu.negative <- TermDocumentMatrix(document.collection, 
  list(dictionary = Hu.Liu.negative.dictionary))
examine.tdm <- removeSparseTerms(reviews.tdm.Hu.Liu.negative, 0.97)
top.words <- Terms(examine.tdm)
print(top.words)    
Hu.Liu.frequent.negative <- findFreqTerms(reviews.tdm.Hu.Liu.negative, 15)  
# this provides a short list negative words occurring at least 15 times
# across the document collection... one of these words seems out of place
# as they could be thought of as positive: "funny" 
test.negative <- setdiff(Hu.Liu.frequent.negative,c("funny"))
# test.negative.dictionary <- Dictionary(test.negative) 
test.negative.dictionary <- c(as.character(test.negative))  

# we need to evaluate the text measures we have defined
# for each of the documents count the total words 
# and the number of words that match the positive and negative dictionaries
total.words <- integer(length(names(document.collection)))
positive.words <- integer(length(names(document.collection)))
negative.words <- integer(length(names(document.collection)))
other.words <- integer(length(names(document.collection)))

reviews.tdm <- TermDocumentMatrix(document.collection)

for(index.for.document in seq(along=names(document.collection))) {
  positive.words[index.for.document] <- 
    sum(termFreq(document.collection[[index.for.document]], 
    control = list(dictionary = test.positive.dictionary)))
  negative.words[index.for.document] <- 
    sum(termFreq(document.collection[[index.for.document]], 
    control = list(dictionary = test.negative.dictionary)))  
  total.words[index.for.document] <- 
    length(reviews.tdm[,index.for.document][["i"]])
  other.words[index.for.document] <- total.words[index.for.document] -
    positive.words[index.for.document] - negative.words[index.for.document]
  }

document <- names(document.collection)
text.measures.data.frame <- data.frame(document,total.words,
  positive.words, negative.words, other.words, stringsAsFactors = FALSE) 
rownames(text.measures.data.frame) <- paste("D",as.character(0:499),sep="")

# compute text measures as percentages of words in each set
text.measures.data.frame$POSITIVE <- 
  100 * text.measures.data.frame$positive.words / 
  text.measures.data.frame$total.words
  
text.measures.data.frame$NEGATIVE <- 
  100 * text.measures.data.frame$negative.words / 
    text.measures.data.frame$total.words 
  
# let us look at the resulting text measures we call POSITIVE and NEGATIVE
# to see if negative and positive dimensions appear to be on a common scale
# that is... is this a single dimension in the document space
# we use principal component biplots to explore text measures 
# here we can use the technique to check on POSITIVE and NEGATIVE
principal.components.solution <- 
  princomp(text.measures.data.frame[,c("POSITIVE","NEGATIVE")], cor = TRUE)
print(summary(principal.components.solution))  
# biplot rendering of text measures and documents by year
pdf(file = "fig_sentiment_text_measures_biplot.pdf", width = 8.5, height = 11)
biplot(principal.components.solution, xlab = "First Pricipal Component",
  xlabs = rep("o", times = length(names(document.collection))),
  ylab = "Second Principal Component", expand = 0.7)
dev.off()

# results... the eigenvalues suggest that there are two underlying dimensions
# POSITIVE and NEGATIVE vectors rather than pointing in opposite directions
# they appear to be othogonal to one another... separate dimensions

# here we see the scatter plot for the two measures... 
# if they were on the same dimension, they would be negatively correlated
# in fact they are correlated negatively but the correlation is very small
with(text.measures.data.frame, print(cor(POSITIVE, NEGATIVE)))  
pdf(file = "fig_sentiment_text_measures_scatter_plot.pdf", 
  width = 8.5, height = 8.5)
ggplot.object <- ggplot(data = text.measures.data.frame,
  aes(x = NEGATIVE, y = POSITIVE)) + 
    geom_point(colour = "darkblue", shape = 1)
ggplot.print.with.margins(ggplot.object.name = ggplot.object,
  left.margin.pct=10, right.margin.pct=10,
  top.margin.pct=10,bottom.margin.pct=10)
dev.off()
  
# Perhaps POSITIVE and NEGATIVE can be combined in a way to yield effective 
# predictions of movie ratings. Let us move to a set of movie reviews for 
# supervised learning.  We select the 500 records from a set of positive 
# reviews (ratings between 7 and 10) and 500 records from a set of negative 
# reviews (ratings between 1 and 4).

# a set of 500 positive reviews... part of the training set
directory.location <- 
  paste(getwd(),"/reviews/train/pos/",sep = "")  

pos.train.corpus <- Corpus(DirSource(directory.location, encoding = "UTF-8"),
  readerControl = list(language = "en_US"))
print(summary(pos.train.corpus))

# a set of 500 negative reviews... part of the training set
directory.location <- 
  paste(getwd(),"/reviews/train/neg/",sep = "")  

neg.train.corpus <- Corpus(DirSource(directory.location, encoding = "UTF-8"),
  readerControl = list(language = "en_US"))
print(summary(neg.train.corpus))

# combine the positive and negative training sets
train.corpus <- c(pos.train.corpus, neg.train.corpus)

# strip whitspace from the documents in the collection
train.corpus <- tm_map(train.corpus, stripWhitespace)

# convert uppercase to lowercase in the document collection
train.corpus <- tm_map(train.corpus, content_transformer(tolower))

# remove numbers from the document collection
train.corpus <- tm_map(train.corpus, removeNumbers)

# remove punctuation from the document collection
train.corpus <- tm_map(train.corpus, removePunctuation)

# using a standard list, remove English stopwords from the document collection
train.corpus <- tm_map(train.corpus, 
  removeWords, stopwords("english"))

# there is more we could do in terms of data preparation 
# stemming... looking for contractions... possessives... 
# previous analysis of a list of top terms showed a number of word 
# contractions which we might like to drop from further analysis, 
# recognizing them as stop words to be dropped from the document collection
initial.tdm <- TermDocumentMatrix(train.corpus)
examine.tdm <- removeSparseTerms(initial.tdm, sparse = 0.96)
top.words <- Terms(examine.tdm)
print(top.words)  

more.stop.words <- c("cant","didnt","doesnt","dont","goes","isnt","hes",
  "shes","thats","theres","theyre","wont","youll","youre","youve") 
train.corpus <- tm_map(train.corpus, 
  removeWords, more.stop.words)
  
some.proper.nouns.to.remove <- 
  c("dick","ginger","hollywood","jack","jill","john","karloff",
    "kudrow","orson","peter","tcm","tom","toni","welles","william","wolheim")
train.corpus <- tm_map(train.corpus, 
  removeWords, some.proper.nouns.to.remove)

# compute list-based text measures for the training corpus
# for each of the documents count the total words 
# and the number of words that match the positive and negative dictionaries
total.words <- integer(length(names(train.corpus)))
positive.words <- integer(length(names(train.corpus)))
negative.words <- integer(length(names(train.corpus)))
other.words <- integer(length(names(train.corpus)))

reviews.tdm <- TermDocumentMatrix(train.corpus)

for(index.for.document in seq(along=names(train.corpus))) {
  positive.words[index.for.document] <- 
    sum(termFreq(train.corpus[[index.for.document]], 
    control = list(dictionary = test.positive.dictionary)))
  negative.words[index.for.document] <- 
    sum(termFreq(train.corpus[[index.for.document]], 
    control = list(dictionary = test.negative.dictionary)))  
  total.words[index.for.document] <- 
    length(reviews.tdm[,index.for.document][["i"]])
  other.words[index.for.document] <- total.words[index.for.document] -
    positive.words[index.for.document] - negative.words[index.for.document]
  }

document <- names(train.corpus)
train.data.frame <- data.frame(document,total.words,
  positive.words, negative.words, other.words, stringsAsFactors = FALSE) 
rownames(train.data.frame) <- paste("D",as.character(0:999),sep="")

# compute text measures as percentages of words in each set
train.data.frame$POSITIVE <- 
  100 * train.data.frame$positive.words / 
  train.data.frame$total.words
  
train.data.frame$NEGATIVE <- 
  100 * train.data.frame$negative.words / 
    train.data.frame$total.words 
    
# rating is embedded in the document name... extract with regular expressions
for(index.for.document in seq(along = train.data.frame$document)) {
  first_split <- strsplit(train.data.frame$document[index.for.document], 
    split = "[_]")
  second_split <- strsplit(first_split[[1]][2], split = "[.]")
  train.data.frame$rating[index.for.document] <- as.numeric(second_split[[1]][1])
  } # end of for-loop for defining ratings and thumbsupdown

train.data.frame$thumbsupdown <- ifelse((train.data.frame$rating > 5), 2, 1)
train.data.frame$thumbsupdown <- 
  factor(train.data.frame$thumbsupdown, levels = c(1,2), 
    labels = c("DOWN","UP"))

# a set of 500 positive reviews... part of the test set
directory.location <- 
  paste(getwd(),"/reviews/test/pos/",sep = "")  

pos.test.corpus <- Corpus(DirSource(directory.location, encoding = "UTF-8"),
  readerControl = list(language = "en_US"))
print(summary(pos.test.corpus))

# a set of 500 negative reviews... part of the test set
directory.location <- 
  paste(getwd(),"/reviews/test/neg/",sep = "")  
 
neg.test.corpus <- Corpus(DirSource(directory.location, encoding = "UTF-8"),
  readerControl = list(language = "en_US"))
print(summary(neg.test.corpus))

# combine the positive and negative testing sets
test.corpus <- c(pos.test.corpus, neg.test.corpus)

# strip whitspace from the documents in the collection
test.corpus <- tm_map(test.corpus, stripWhitespace)

# convert uppercase to lowercase in the document collection
test.corpus <- tm_map(test.corpus, content_transformer(tolower))

# remove numbers from the document collection
test.corpus <- tm_map(test.corpus, removeNumbers)

# remove punctuation from the document collection
test.corpus <- tm_map(test.corpus, removePunctuation)

# using a standard list, remove English stopwords from the document collection
test.corpus <- tm_map(test.corpus, 
  removeWords, stopwords("english"))

# there is more we could do in terms of data preparation 
# stemming... looking for contractions... possessives... 
# previous analysis of a list of top terms showed a number of word 
# contractions which we might like to drop from further analysis, 
# recognizing them as stop words to be dropped from the document collection
initial.tdm <- TermDocumentMatrix(test.corpus)
examine.tdm <- removeSparseTerms(initial.tdm, sparse = 0.96)
top.words <- Terms(examine.tdm)
print(top.words)  

more.stop.words <- c("cant","didnt","doesnt","dont","goes","isnt","hes",
  "shes","thats","theres","theyre","wont","youll","youre","youve") 
test.corpus <- tm_map(test.corpus, 
  removeWords, more.stop.words)
  
some.proper.nouns.to.remove <- 
  c("dick","ginger","hollywood","jack","jill","john","karloff",
    "kudrow","orson","peter","tcm","tom","toni","welles","william","wolheim")
test.corpus <- tm_map(test.corpus, 
  removeWords, some.proper.nouns.to.remove)

# compute list-based text measures for the testing corpus
# for each of the documents count the total words 
# and the number of words that match the positive and negative dictionaries
total.words <- integer(length(names(test.corpus)))
positive.words <- integer(length(names(test.corpus)))
negative.words <- integer(length(names(test.corpus)))
other.words <- integer(length(names(test.corpus)))

reviews.tdm <- TermDocumentMatrix(test.corpus)

for(index.for.document in seq(along=names(test.corpus))) {
  positive.words[index.for.document] <- 
    sum(termFreq(test.corpus[[index.for.document]], 
    control = list(dictionary = test.positive.dictionary)))
  negative.words[index.for.document] <- 
    sum(termFreq(test.corpus[[index.for.document]], 
    control = list(dictionary = test.negative.dictionary)))  
  total.words[index.for.document] <- 
    length(reviews.tdm[,index.for.document][["i"]])
  other.words[index.for.document] <- total.words[index.for.document] -
    positive.words[index.for.document] - negative.words[index.for.document]
  }

document <- names(test.corpus)
test.data.frame <- data.frame(document,total.words,
  positive.words, negative.words, other.words, stringsAsFactors = FALSE) 
rownames(test.data.frame) <- paste("D",as.character(0:999),sep="")

# compute text measures as percentages of words in each set
test.data.frame$POSITIVE <- 
  100 * test.data.frame$positive.words / 
  test.data.frame$total.words
  
test.data.frame$NEGATIVE <- 
  100 * test.data.frame$negative.words / 
    test.data.frame$total.words 
    
# rating is embedded in the document name... extract with regular expressions
for(index.for.document in seq(along = test.data.frame$document)) {
  first_split <- strsplit(test.data.frame$document[index.for.document], 
    split = "[_]")
  second_split <- strsplit(first_split[[1]][2], split = "[.]")
  test.data.frame$rating[index.for.document] <- as.numeric(second_split[[1]][1])
  } # end of for-loop for defining 

test.data.frame$thumbsupdown <- ifelse((test.data.frame$rating > 5), 2, 1)
test.data.frame$thumbsupdown <- 
  factor(test.data.frame$thumbsupdown, levels = c(1,2), 
    labels = c("DOWN","UP"))

# a set of 4 positive and 4 negative reviews... testing set of Tom's reviews
directory.location <- 
  paste(getwd(),"/reviews/test/tom/",sep = "")  

tom.corpus <- Corpus(DirSource(directory.location, encoding = "UTF-8"),
  readerControl = list(language = "en_US"))
print(summary(tom.corpus))

# strip whitspace from the documents in the collection
tom.corpus <- tm_map(tom.corpus, stripWhitespace)

# convert uppercase to lowercase in the document collection
tom.corpus <- tm_map(tom.corpus, content_transformer(tolower))

# remove numbers from the document collection
tom.corpus <- tm_map(tom.corpus, removeNumbers)

# remove punctuation from the document collection
tom.corpus <- tm_map(tom.corpus, removePunctuation)

# using a standard list, remove English stopwords from the document collection
tom.corpus <- tm_map(tom.corpus, 
  removeWords, stopwords("english"))

# there is more we could do in terms of data preparation 
# stemming... looking for contractions... possessives... 
# previous analysis of a list of top terms showed a number of word 
# contractions which we might like to drop from further analysis, 
# recognizing them as stop words to be dropped from the document collection
initial.tdm <- TermDocumentMatrix(tom.corpus)
examine.tdm <- removeSparseTerms(initial.tdm, sparse = 0.96)
top.words <- Terms(examine.tdm)
print(top.words)  

more.stop.words <- c("cant","didnt","doesnt","dont","goes","isnt","hes",
  "shes","thats","theres","theyre","wont","youll","youre","youve") 
tom.corpus <- tm_map(tom.corpus, 
  removeWords, more.stop.words)
  
some.proper.nouns.to.remove <- 
  c("dick","ginger","hollywood","jack","jill","john","karloff",
    "kudrow","orson","peter","tcm","tom","toni","welles","william","wolheim")
tom.corpus <- tm_map(tom.corpus, 
  removeWords, some.proper.nouns.to.remove)

# compute list-based text measures for tom corpus
# for each of the documents count the total words 
# and the number of words that match the positive and negative dictionaries
total.words <- integer(length(names(tom.corpus)))
positive.words <- integer(length(names(tom.corpus)))
negative.words <- integer(length(names(tom.corpus)))
other.words <- integer(length(names(tom.corpus)))

reviews.tdm <- TermDocumentMatrix(tom.corpus)

for(index.for.document in seq(along=names(tom.corpus))) {
  positive.words[index.for.document] <- 
    sum(termFreq(tom.corpus[[index.for.document]], 
    control = list(dictionary = test.positive.dictionary)))
  negative.words[index.for.document] <- 
    sum(termFreq(tom.corpus[[index.for.document]], 
    control = list(dictionary = test.negative.dictionary)))  
  total.words[index.for.document] <- 
    length(reviews.tdm[,index.for.document][["i"]])
  other.words[index.for.document] <- total.words[index.for.document] -
    positive.words[index.for.document] - negative.words[index.for.document]
  }

document <- names(tom.corpus)
tom.data.frame <- data.frame(document,total.words,
  positive.words, negative.words, other.words, stringsAsFactors = FALSE) 
rownames(tom.data.frame) <- paste("D",as.character(0:7),sep="")

# compute text measures as percentages of words in each set
tom.data.frame$POSITIVE <- 
  100 * tom.data.frame$positive.words / 
  tom.data.frame$total.words
  
tom.data.frame$NEGATIVE <- 
  100 * tom.data.frame$negative.words / 
    tom.data.frame$total.words 
    
# rating is embedded in the document name... extract with regular expressions
for(index.for.document in seq(along = tom.data.frame$document)) {
  first_split <- strsplit(tom.data.frame$document[index.for.document], 
    split = "[_]")
  second_split <- strsplit(first_split[[1]][2], split = "[.]")
  tom.data.frame$rating[index.for.document] <- as.numeric(second_split[[1]][1])
  } # end of for-loop for defining 

tom.data.frame$thumbsupdown <- ifelse((tom.data.frame$rating > 5), 2, 1)
tom.data.frame$thumbsupdown <- 
  factor(tom.data.frame$thumbsupdown, levels = c(1,2), 
    labels = c("DOWN","UP"))

tom.movies <- data.frame(movies = 
  c("The Effect of Gamma Rays on Man-in-the-Moon Marigolds",
    "Blade Runner","My Cousin Vinny","Mars Attacks",
    "Fight Club","Miss Congeniality 2","Find Me Guilty","Moneyball"))

# check out the measures on Tom's ratings
tom.data.frame.review <- 
  cbind(tom.movies,tom.data.frame[,names(tom.data.frame)[2:9]])
print(tom.data.frame.review)

# develop predictive models using the training data
# --------------------------------------
# Simple difference method
# --------------------------------------
train.data.frame$simple <- 
     train.data.frame$POSITIVE - train.data.frame$NEGATIVE

# check out simple difference method... is there a correlation with ratings?
with(train.data.frame, print(cor(simple, rating)))  

# we use the training data to define an optimal cutoff... 
# trees can help with finding the optimal split point for simple.difference
try.tree <- rpart(thumbsupdown ~ simple, data = train.data.frame)
print(try.tree)  # note that the first split value
# an earlier analysis had this value as -0.7969266
# create a user-defined function for the simple difference method
predict.simple <- function(x) {
  if (x >= -0.7969266) return("UP")
  if (x < -0.7969266) return("DOWN")
  }

# evaluate predictive accuracy in the training data
train.data.frame$pred.simple <- character(nrow(train.data.frame))
for (index.for.review in seq(along = train.data.frame$pred.simple)) {
  train.data.frame$pred.simple[index.for.review] <- 
    predict.simple(train.data.frame$simple[index.for.review])
  }   
train.data.frame$pred.simple <- 
  factor(train.data.frame$pred.simple)

train.pred.simple.performance <- 
  confusionMatrix(data = train.data.frame$pred.simple, 
  reference = train.data.frame$thumbsupdown, positive = "UP") 
  
# report full set of statistics relating to predictive accuracy
print(train.pred.simple.performance)

cat("\n\nTraining set percentage correctly predicted by",
  " simple difference method = ",
  sprintf("%1.1f",train.pred.simple.performance$overall[1]*100)," Percent",sep="")

# evaluate predictive accuracy in the test data
# SIMPLE DIFFERENCE METHOD
test.data.frame$simple <- 
     test.data.frame$POSITIVE - train.data.frame$NEGATIVE

test.data.frame$pred.simple <- character(nrow(test.data.frame))
for (index.for.review in seq(along = test.data.frame$pred.simple)) {
  test.data.frame$pred.simple[index.for.review] <- 
    predict.simple(test.data.frame$simple[index.for.review])
  }   
test.data.frame$pred.simple <- 
  factor(test.data.frame$pred.simple)

test.pred.simple.performance <- 
  confusionMatrix(data = test.data.frame$pred.simple, 
  reference = test.data.frame$thumbsupdown, positive = "UP") 
  
# report full set of statistics relating to predictive accuracy
print(test.pred.simple.performance)

cat("\n\nTest set percentage correctly predicted = ",
  sprintf("%1.1f",test.pred.simple.performance$overall[1]*100)," 
    Percent",sep="")

# --------------------------------------
# Regression difference method
# --------------------------------------
# regression method for determining weights on POSITIVE AND NEGATIVE
# fit a regression model to the training data
regression.model <- lm(rating ~ POSITIVE + NEGATIVE, data = train.data.frame)
print(regression.model)  # provides 5.5386 + 0.2962(POSITIVE) -0.3089(NEGATIVE)

train.data.frame$regression <- 
  predict(regression.model, newdata = train.data.frame)

# determine the cutoff for regression.difference
  try.tree <- rpart(thumbsupdown ~ regression, data = train.data.frame)
print(try.tree)  # note that the first split is at 5.264625
# create a user-defined function for the simple difference method
predict.regression <- function(x) {
  if (x >= 5.264625) return("UP")
  if (x < 5.264625) return("DOWN")
  }

train.data.frame$pred.regression <-  character(nrow(train.data.frame))
for (index.for.review in seq(along = train.data.frame$pred.simple)) {
  train.data.frame$pred.regression[index.for.review] <- 
    predict.regression(train.data.frame$regression[index.for.review])
  }   
train.data.frame$pred.regression <- 
  factor(train.data.frame$pred.regression)

train.pred.regression.performance <- 
  confusionMatrix(data = train.data.frame$pred.regression, 
  reference = train.data.frame$thumbsupdown, positive = "UP") 
  
# report full set of statistics relating to predictive accuracy
print(train.pred.regression.performance)  # result 67.3 Percent

cat("\n\nTraining set percentage correctly predicted by regression = ",
  sprintf("%1.1f",train.pred.regression.performance$overall[1]*100),
    " Percent",sep="")

# regression method for determining weights on POSITIVE AND NEGATIVE
# for the test set we use the model developed on the training set
test.data.frame$regression <- 
  predict(regression.model, newdata = test.data.frame)

test.data.frame$pred.regression <-  character(nrow(test.data.frame))
for (index.for.review in seq(along = test.data.frame$pred.simple)) {
  test.data.frame$pred.regression[index.for.review] <- 
    predict.regression(test.data.frame$regression[index.for.review])
  }   
test.data.frame$pred.regression <- 
  factor(test.data.frame$pred.regression)

test.pred.regression.performance <- 
  confusionMatrix(data = test.data.frame$pred.regression, 
  reference = test.data.frame$thumbsupdown, positive = "UP") 
  
# report full set of statistics relating to predictive accuracy
print(test.pred.regression.performance)  # result 67.3 Percent

cat("\n\nTest set percentage correctly predicted = ",
  sprintf("%1.1f",test.pred.regression.performance$overall[1]*100),
    " Percent",sep="")

# --------------------------------------------
# Word/item analysis method for train.corpus
# --------------------------------------------
# return to the training corpus to develop simple counts
# for each of the words in the sentiment list
# these new variables will be given the names of the words
# to keep things simple.... there are 50 such variables/words
# identified from an earlier analysis, as published in the book
working.corpus <- train.corpus
# run common code from utilities for scoring the working corpus
# this common code uses 25 positive and 25 negative words
# identified in an earlier analysis of these data
source("R_utility_program_5.R")

add.data.frame <- data.frame(amazing,beautiful,classic,enjoy,       
  enjoyed,entertaining,excellent,fans,favorite,fine,fun,humor,       
  lead,liked,love,loved,modern,nice,perfect,pretty,      
  recommend,strong,top,wonderful,worth,bad,boring,cheap,creepy,dark,dead,    
  death,evil,hard,kill,killed,lack,lost,miss,murder,mystery,plot,poor,    
  sad,scary,slow,terrible,waste,worst,wrong)  
  
train.data.frame <- cbind(train.data.frame,add.data.frame)  

# --------------------------------------------
# Word/item analysis method for test.corpus
# --------------------------------------------
# return to the testing corpus to develop simple counts
# for each of the words in the sentiment list
# these new variables will be given the names of the words
# to keep things simple.... there are 50 such variables/words
working.corpus <- test.corpus
# run common code from utilities for scoring the working corpus
source("R_utility_program_5.R")

add.data.frame <- data.frame(amazing,beautiful,classic,enjoy,       
  enjoyed,entertaining,excellent,fans,favorite,fine,fun,humor,       
  lead,liked,love,loved,modern,nice,perfect,pretty,      
  recommend,strong,top,wonderful,worth,bad,boring,cheap,creepy,dark,dead,    
  death,evil,hard,kill,killed,lack,lost,miss,murder,mystery,plot,poor,    
  sad,scary,slow,terrible,waste,worst,wrong)  
  
test.data.frame <- cbind(test.data.frame,add.data.frame)  

# --------------------------------------------
# Word/item analysis method for tom.corpus
# --------------------------------------------
# return to the toming corpus to develop simple counts
# for each of the words in the sentiment list
# these new variables will be given the names of the words
# to keep things simple.... there are 50 such variables/words
working.corpus <- tom.corpus
# run common code from utilities for scoring the working corpus
source("R_utility_program_5.R")

add.data.frame <- data.frame(amazing,beautiful,classic,enjoy,       
  enjoyed,entertaining,excellent,fans,favorite,fine,fun,humor,       
  lead,liked,love,loved,modern,nice,perfect,pretty,      
  recommend,strong,top,wonderful,worth,bad,boring,cheap,creepy,dark,dead,    
  death,evil,hard,kill,killed,lack,lost,miss,murder,mystery,plot,poor,    
  sad,scary,slow,terrible,waste,worst,wrong)  
  
tom.data.frame <- cbind(tom.data.frame,add.data.frame)  


# use phi coefficient... correlation with rating as index of item value
# again we draw upon the earlier positive and negative lists 
phi <- numeric(50)
item <- c("amazing","beautiful","classic","enjoy",       
  "enjoyed","entertaining","excellent","fans","favorite","fine","fun","humor",       
  "lead","liked","love","loved","modern","nice","perfect","pretty",      
  "recommend","strong","top","wonderful","worth",
  "bad","boring","cheap","creepy","dark","dead",    
  "death","evil","hard","kill","killed","lack",
  "lost","miss","murder","mystery","plot","poor",    
  "sad","scary","slow","terrible","waste","worst","wrong")
item.analysis.data.frame <- data.frame(item,phi)
item.place <- 14:63
for (index.for.column in 1:50) {
  item.analysis.data.frame$phi[index.for.column] <- 
    cor(train.data.frame[, item.place[index.for.column]],train.data.frame[,8])
  }

# sort by absolute value of the phi coefficient with the rating  
item.analysis.data.frame$absphi <- abs(item.analysis.data.frame$phi)
item.analysis.data.frame <- 
  item.analysis.data.frame[sort.list(item.analysis.data.frame$absphi,
    decreasing = TRUE),]
    
# subset of words with phi coefficients greater than 0.05 in absolute value    
selected.items.data.frame <- 
  subset(item.analysis.data.frame, subset = (absphi > 0.05))
  
# use the sign of the phi coefficient as the item weight
selected.positive.data.frame <-
  subset(selected.items.data.frame, subset = (phi > 0.0))
selected.positive.words <- as.character(selected.positive.data.frame$item)  
  
selected.negative.data.frame <-
  subset(selected.items.data.frame, subset = (phi < 0.0))  
selected.negative.words <- as.character(selected.negative.data.frame$item)    

# these lists define new dictionaries for scoring 

reviews.tdm <- TermDocumentMatrix(train.corpus)

temp.positive.score <- integer(length(names(train.corpus)))
temp.negative.score <- integer(length(names(train.corpus)))
for(index.for.document in seq(along=names(train.corpus))) {
  temp.positive.score[index.for.document] <- 
    sum(termFreq(train.corpus[[index.for.document]], 
    control = list(dictionary = selected.positive.words)))
  temp.negative.score[index.for.document] <- 
    sum(termFreq(train.corpus[[index.for.document]], 
    control = list(dictionary = selected.negative.words)))  
  }
  
train.data.frame$item.analysis.score <- 
  temp.positive.score - temp.negative.score
  
# use the training set and tree-structured modeling to determine the cutoff  
  try.tree<-rpart(thumbsupdown ~ item.analysis.score, data = train.data.frame)
print(try.tree)  # note that the first split is at -0.5
# create a user-defined function for the simple difference method
predict.item.analysis <- function(x) {
  if (x >= -0.5) return("UP")
  if (x < -0.5) return("DOWN")
  }
  
train.data.frame$pred.item.analysis <-  character(nrow(train.data.frame))
for (index.for.review in seq(along = train.data.frame$pred.simple)) {
  train.data.frame$pred.item.analysis[index.for.review] <- 
  predict.item.analysis(train.data.frame$item.analysis.score[index.for.review])
  }   
train.data.frame$pred.item.analysis <- 
  factor(train.data.frame$pred.item.analysis)

train.pred.item.analysis.performance <- 
  confusionMatrix(data = train.data.frame$pred.item.analysis, 
  reference = train.data.frame$thumbsupdown, positive = "UP") 
  
# report full set of statistics relating to predictive accuracy
print(train.pred.item.analysis.performance)  # result 73.9 Percent

cat("\n\nTraining set percentage correctly predicted by item analysis = ",
  sprintf("%1.1f",train.pred.item.analysis.performance$overall[1]*100),
    " Percent",sep="")  
    
# use item analysis method of scoring with the test set

reviews.tdm <- TermDocumentMatrix(test.corpus)

temp.positive.score <- integer(length(names(test.corpus)))
temp.negative.score <- integer(length(names(test.corpus)))
for(index.for.document in seq(along=names(test.corpus))) {
  temp.positive.score[index.for.document] <- 
    sum(termFreq(test.corpus[[index.for.document]], 
    control = list(dictionary = selected.positive.words)))
  temp.negative.score[index.for.document] <- 
    sum(termFreq(test.corpus[[index.for.document]], 
    control = list(dictionary = selected.negative.words)))  
  }
  
test.data.frame$item.analysis.score <- 
  temp.positive.score - temp.negative.score
    
test.data.frame$pred.item.analysis <-  character(nrow(test.data.frame))
for (index.for.review in seq(along = test.data.frame$pred.simple)) {
  test.data.frame$pred.item.analysis[index.for.review] <- 
  predict.item.analysis(test.data.frame$item.analysis.score[index.for.review])
  }   
test.data.frame$pred.item.analysis <- 
  factor(test.data.frame$pred.item.analysis)

test.pred.item.analysis.performance <- 
  confusionMatrix(data = test.data.frame$pred.item.analysis, 
  reference = test.data.frame$thumbsupdown, positive = "UP") 
  
# report full set of statistics relating to predictive accuracy
print(test.pred.item.analysis.performance)  # result 74 Percent

cat("\n\nTest set percentage correctly predicted by item analysis = ",
  sprintf("%1.1f",test.pred.item.analysis.performance$overall[1]*100),
    " Percent",sep="")  
    
# --------------------------------------
# Logistic regression method
# --------------------------------------
text.classification.model <- {thumbsupdown ~ amazing + beautiful + 
  classic + enjoy + enjoyed + 
  entertaining + excellent +   
  fans + favorite + fine + fun + humor + lead + liked +       
  love + loved + modern + nice + perfect + pretty + 
  recommend + strong + top + wonderful + worth +       
  bad + boring + cheap + creepy + dark + dead + 
  death + evil + hard + kill +    
  killed + lack + lost + miss + murder + mystery + 
  plot + poor + sad + scary +   
  slow + terrible + waste + worst + wrong}

# full logistic regression model
logistic.regression.fit <- glm(text.classification.model, 
  family=binomial(link=logit), data = train.data.frame)
print(summary(logistic.regression.fit))

# obtain predicted probability values for training set
logistic.regression.pred.prob <- 
  as.numeric(predict(logistic.regression.fit, newdata = train.data.frame,
  type="response")) 

train.data.frame$pred.logistic.regression <- 
  ifelse((logistic.regression.pred.prob > 0.5),2,1)

train.data.frame$pred.logistic.regression <- 
  factor(train.data.frame$pred.logistic.regression, levels = c(1,2), 
    labels = c("DOWN","UP"))

train.pred.logistic.regression.performance <- 
  confusionMatrix(data = train.data.frame$pred.logistic.regression, 
  reference = train.data.frame$thumbsupdown, positive = "UP") 

# report full set of statistics relating to predictive accuracy
print(train.pred.logistic.regression.performance)  # result 75.2 Percent

cat("\n\nTraining set percentage correct by logistic regression = ",
  sprintf("%1.1f",train.pred.logistic.regression.performance$overall[1]*100),
    " Percent",sep="")
    
# now we use the model developed on the training set with the test set
# obtain predicted probability values for test set
logistic.regression.pred.prob <- 
  as.numeric(predict(logistic.regression.fit, newdata = test.data.frame,
  type="response")) 

test.data.frame$pred.logistic.regression <- 
  ifelse((logistic.regression.pred.prob > 0.5),2,1)

test.data.frame$pred.logistic.regression <- 
  factor(test.data.frame$pred.logistic.regression, levels = c(1,2), 
    labels = c("DOWN","UP"))

test.pred.logistic.regression.performance <- 
  confusionMatrix(data = test.data.frame$pred.logistic.regression, 
  reference = test.data.frame$thumbsupdown, positive = "UP") 

# report full set of statistics relating to predictive accuracy
print(test.pred.logistic.regression.performance)  # result 72.6 Percent

cat("\n\nTest set percentage correctly predicted by logistic regression = ",
  sprintf("%1.1f",test.pred.logistic.regression.performance$overall[1]*100),
    " Percent",sep="")

# --------------------------------------
# Support vector machines
# --------------------------------------
# determine tuning parameters prior to fitting model
train.tune <- tune(svm, text.classification.model, data = train.data.frame,
                   ranges = list(gamma = 2^(-8:1), cost = 2^(0:4)),
                   tunecontrol = tune.control(sampling = "fix"))
# display the tuning results (in text format)
print(train.tune)

# fit the support vector machine to the training data using tuning parameters
train.data.frame.svm <- svm(text.classification.model, data = train.data.frame, 
  cost=4, gamma=0.00390625, probability = TRUE)
  
train.data.frame$pred.svm <- predict(train.data.frame.svm, type="class",
newdata=train.data.frame)

train.pred.svm.performance <- 
  confusionMatrix(data = train.data.frame$pred.svm, 
  reference = train.data.frame$thumbsupdown, positive = "UP") 

# report full set of statistics relating to predictive accuracy
print(train.pred.svm.performance)  # result 79.0 Percent

cat("\n\nTraining set percentage correctly predicted by SVM = ",
  sprintf("%1.1f",train.pred.svm.performance$overall[1]*100),
    " Percent",sep="")

# use the support vector machine model identified in the training set
# to do text classification on the test set
test.data.frame$pred.svm <- predict(train.data.frame.svm, type="class",
newdata=test.data.frame)

test.pred.svm.performance <- 
  confusionMatrix(data = test.data.frame$pred.svm, 
  reference = test.data.frame$thumbsupdown, positive = "UP") 

# report full set of statistics relating to predictive accuracy
print(test.pred.svm.performance)  # result 71.6 Percent

cat("\n\nTest set percentage correctly predicted by SVM = ",
  sprintf("%1.1f",test.pred.svm.performance$overall[1]*100),
    " Percent",sep="")

# --------------------------------------
# Random forests
# --------------------------------------
# fit random forest model to the training data
set.seed (9999)  # for reproducibility
train.data.frame.rf <- randomForest(text.classification.model, 
  data=train.data.frame, mtry=3, importance=TRUE, na.action=na.omit) 

# review the random forest solution      
print(train.data.frame.rf)  

# check importance of the individual explanatory variables 
pdf(file = "fig_sentiment_random_forest_importance.pdf", 
width = 11, height = 8.5)
varImpPlot(train.data.frame.rf, main = "")
dev.off()

train.data.frame$pred.rf <- predict(train.data.frame.rf, type="class", 
  newdata = train.data.frame)

train.pred.rf.performance <- 
  confusionMatrix(data = train.data.frame$pred.rf, 
  reference = train.data.frame$thumbsupdown, positive = "UP") 

# report full set of statistics relating to predictive accuracy
print(train.pred.rf.performance)  # result 82.2 Percent

cat("\n\nTraining set percentage correctly predicted by random forests = ",
  sprintf("%1.1f",train.pred.rf.performance$overall[1]*100),
    " Percent",sep="")
    
# use the model fit to the training data to predict the the test data     
test.data.frame$pred.rf <- predict(train.data.frame.rf, type="class", 
  newdata = test.data.frame)

test.pred.rf.performance <- 
  confusionMatrix(data = test.data.frame$pred.rf, 
  reference = test.data.frame$thumbsupdown, positive = "UP") 

# report full set of statistics relating to predictive accuracy
print(test.pred.rf.performance)  # result 74.0 Percent

cat("\n\nTest set percentage correctly predicted by random forests = ",
  sprintf("%1.1f",test.pred.rf.performance$overall[1]*100),
    " Percent",sep="")    
 
# measurement model performance summary
methods <- c("Simple difference","Regression difference",
  "Word/item analysis","Logistic regression",
  "Support vector machines","Random forests")
methods.performance.data.frame <- data.frame(methods)

methods.performance.data.frame$training <- 
  c(train.pred.simple.performance$overall[1]*100,
    train.pred.regression.performance$overall[1]*100,
    train.pred.item.analysis.performance$overall[1]*100,
    train.pred.logistic.regression.performance$overall[1]*100,
    train.pred.svm.performance$overall[1]*100,
    train.pred.rf.performance$overall[1]*100)
  
methods.performance.data.frame$test <-
  c(test.pred.simple.performance$overall[1]*100,
    test.pred.regression.performance$overall[1]*100,
    test.pred.item.analysis.performance$overall[1]*100,
    test.pred.logistic.regression.performance$overall[1]*100,
    test.pred.svm.performance$overall[1]*100,
    test.pred.rf.performance$overall[1]*100)

# random forest predictions for Tom's movie reviews
tom.data.frame$pred.rf <- predict(train.data.frame.rf, type="class", 
  newdata = tom.data.frame)
  
print(tom.data.frame[,c("thumbsupdown","pred.rf")])  

tom.pred.rf.performance <- 
  confusionMatrix(data = tom.data.frame$pred.rf, 
  reference = tom.data.frame$thumbsupdown, positive = "UP") 

# report full set of statistics relating to predictive accuracy
print(tom.pred.rf.performance)  # result 74.0 Percent

cat("\n\nTraining set percentage correctly predicted by random forests = ",    
  sprintf("%1.1f",tom.pred.rf.performance$overall[1]*100),
    " Percent",sep="")    

# building a simple tree to classify reviews
simple.tree <- rpart(text.classification.model, 
  data=train.data.frame,)

# plot the regression tree result from rpart
pdf(file = "fig_sentiment_simple_tree_classifier.pdf", width = 8.5, height = 8.5)
prp(simple.tree, main="",
  digits = 3,  # digits to display in terminal nodes
  nn = TRUE,  # display the node numbers
  fallen.leaves = TRUE,  # put the leaves on the bottom of the page
  branch = 0.5,  # change angle of branch lines
  branch.lwd = 2,  # width of branch lines
  faclen = 0,  # do not abbreviate factor levels
  trace = 1,  # print the automatically calculated cex
  shadow.col = 0,  # no shadows under the leaves
  branch.lty = 1,  # draw branches using dotted lines
  split.cex = 1.2,  # make the split text larger than the node text
  split.prefix = "is ",  # put "is" before split text
  split.suffix = "?",  # put "?" after split text
  split.box.col = "blue",  # lightgray split boxes (default is white)
  split.col = "white",  # color of text in split box 
  split.border.col = "blue",  # darkgray border on split boxes
  split.round = .25)  # round the split box corners a tad
dev.off()

# simple tree predictions for Tom's movie reviews
tom.data.frame$pred.simple.tree <- predict(simple.tree, type="class", 
  newdata = tom.data.frame)

print(tom.data.frame[,c("thumbsupdown","pred.rf","pred.simple.tree")])  

# Game-day Simulator for Baseball (R)
library(lattice)  # graphics package for probability matrix visual
simulator <- function(home_mean,away_mean,niterations) { 
     # input runs scored means, output probability of winning for home team
     set.seed(1234)  # set to obtain reproducible results 
     away_game_score <- numeric(niterations)
     home.game.score <- numeric(niterations)
     home_win <- numeric(niterations)
     i <- 1
     while (i < niterations + 1) { 
         away_game_score[i] <- rnbinom(1,mu=away_mean, size = 4)
         home.game.score[i] <- rnbinom(1,mu=home_mean, size = 4)
         if(away_game_score[i] > home.game.score[i]) home_win[i] <- 1
         if(away_game_score[i] > home.game.score[i] || 
         away_game_score[i] < home.game.score[i]) i <- i + 1 
         }
     n_home_win <- sum(home_win)
     n_home_win/niterations  # return probability of away team winning 
     } 

niterations <- 100000  # use smaller number for testing
# probability matrix for results... home team is rows, away team is columns
probmat <- matrix(data = NA, nrow = 9, ncol = 9,  
  dimnames = list(c(as.character(1:9)), c(as.character(1:9)))) 
for (index_home in 1:9)
for (index_away in 1:9)
if (index_home != index_away) {
     probmat[index_home,index_away] <- 
        simulator(index_home, index_away, niterations) 
     }
pdf(file = "fig_sports_analytics_prob_matrix.pdf", width = 8.5, height = 8.5)
x <- rep(1:nrow(probmat),times=ncol(probmat))
y <- NULL
for (i in 1:ncol(probmat)) y <- c(y,rep(i,times=nrow(probmat)))
probtext <- sprintf("%0.3f", as.numeric(probmat))  # fixed format 0.XXX
text_data_frame <- data.frame(x, y, probtext)
text_data_frame$probtext <- as.character(text_data_frame$probtext)
text_data_frame$probtext <- ifelse((text_data_frame$probtext == "NA"),
    NA,text_data_frame$probtext)  # define diagonal cells as missing
text_data_frame <- na.omit(text_data_frame)  # diagonal cells
print(levelplot(probmat, cuts = 25, tick.number = 9,
    col.regions=colorRampPalette(c("violet", "white", "light blue")),
    xlab = "Visiting Team Runs Expected", 
    ylab = "Home Team Runs Expected",
    panel = function(...) {
        panel.levelplot(...)  
        panel.text(text_data_frame$x, text_data_frame$y, 
        labels = text_data_frame$probtext)
        }))
dev.off()        

# Regression Modeling with California Housing Values (R)

library(maps)  # making making 
library(mapproj)  # projections for map making
library(spgwr)  # spatially-weighted regression
library(rpart)  # tree-structured modeling
library(randomForest)  # random forests
library(rpart.plot)  # plot tree-structured model information
library(lattice)  # statistical graphics
library(cvTools)  # cross-validation tools including rmspe 

# read in the housing data
houses <-  read.table("houses_data.txt", header = FALSE, sep = "", 
   dec = ".", row.names = NULL, 
   col.names = c("value", "income", "age", "rooms", "bedrooms", 
   "pop", "hh", "latitude", "longitude"))          

# computed variables for linear model used by Pace and Barry (1997)   
houses$log_value <- log(houses$value)  
houses$income_squared <- houses$income^2 
houses$income_cubed <- houses$income^3 
houses$log_age <- log(houses$age)
houses$log_pc_rooms <- log(houses$rooms / houses$pop)
houses$log_pc_bedrooms <- log(houses$bedrooms / houses$pop)
houses$log_pop_hh <- log(houses$pop / houses$hh)
houses$log_hh <- log(houses$hh)

# structure of the Pace and Barry (1997) model for baseline for comparisons
pace.barry.model <- {log_value ~ income + income_squared + 
  income_cubed + log_age + log_pc_rooms + log_pc_bedrooms + 
  log_pop_hh + log_hh}

# for comparison lets look at a simple model with the original variables
simple.model <-  {log_value ~ income + age + rooms + bedrooms +
  pop + hh} 
  
# original variables plus variables that add value for trees 
# that is... variables that are not simple monotonic transformations
# of the original explanatory variables
full.model <- {log_value ~ income + age + rooms + bedrooms +
  pop + hh + log_pc_rooms + log_pc_bedrooms + log_pop_hh}  
  
# define variable for selecting a geographically defined
# subset of the data... San Diego area  
# we use nested ifelse statements to do this

# define the bounding box for selecting the area
# here we are selecting the San Diego region
BB.TOP <- 33
BB.BOTTOM <- 32
BB.RIGHT <- -116.75
BB.LEFT <- -125

houses$select <- ifelse(((houses$latitude < BB.TOP)),
  ifelse((houses$longitude < BB.RIGHT),
  ifelse((houses$latitude > BB.BOTTOM),
  ifelse((houses$longitude > BB.LEFT),1,2),2),2),2)
houses$select <- factor(houses$select, levels = c(1,2), 
  labels = c("Selected","Not Selected"))
houses.selected <- subset(houses, subset = (select == "Selected"))
houses.notselected <- subset(houses, subset = (select == "Not Selected"))  
    
# plot the locations of block groups red in the selected area, blue otherwise
pdf(file = "fig_spatial_map_selected_region.pdf", width = 8.5, height = 8.5)
pointsize <- 0.5
map("state", region = c("california"), project="albers",par=c(39,45)) 
  points(mapproject(houses.selected$longitude, houses.selected$latitude,
  projection=""),pch=20,cex=pointsize,col="red")
  points(mapproject(houses.notselected$longitude, houses.notselected$latitude,
  projection=""),pch=20,cex=pointsize,col="darkblue")
legend("right", legend = c("Selected Region","Not Selected"), 
  col = c("red","darkblue"), pch = 20)
map.scale()  
dev.off()
        
# define training and test sets for the selected houses
set.seed(4444)
partition <- sample(nrow(houses.selected)) # permuted list of row index numbers
houses.selected$Group <- 
  ifelse((partition < nrow(houses.selected)/(3/2)),1,2)
houses.selected$Group <- 
  factor(houses.selected$Group,levels=c(1,2),labels=c("TRAIN","TEST"))
print(table(houses.selected$Group))  # review the split into training and test
print(head(houses.selected))  # review the selected data

houses.train <- 
  subset(houses.selected, subset = (Group == "TRAIN"))
houses.test <- 
  subset(houses.selected, subset = (Group == "TEST")) 
  
# examine the correlations across the variables before we begin modeling
houses.train.df.vars <- houses.train[,c("log_value","income","age",
  "rooms","bedrooms","pop","hh","log_pc_rooms",
  "log_pc_bedrooms","log_pop_hh")]
  
houses.train.cormat <- cor(as.matrix(houses.train.df.vars))
houses.train.cormat.line <- houses.train.cormat["log_value",]
# explanatory variables ordered by correlation with the response variable
ordered.houses.train.cormat <- 
  houses.train.cormat[names(sort(houses.train.cormat.line,decreasing=TRUE)),
  names(sort(houses.train.cormat.line,decreasing=FALSE))]

# code to obtain default colors from ggplot2...
number.of.default.colors <- 2  # two end-points for our scale
end.point.colors <- hcl(h=seq(15, 375-360/number.of.default.colors,
  length=number.of.default.colors)%%360, c=100, l=65)
# end.point.colors[1] and [2] used to define the three-point color scale

pdf(file = "fig_spatial_correlation_heat_map.pdf", width = 11, 
  height = 8.5)
x <- rep(1:nrow(ordered.houses.train.cormat),
  times=ncol(ordered.houses.train.cormat))
y <- NULL
for (i in 1:ncol(ordered.houses.train.cormat)) 
  y <- c(y,rep(i,times=nrow(ordered.houses.train.cormat)))
# use fixed format 0.XXX in cells of correlation matrix
cortext <- sprintf("%0.3f", as.numeric(ordered.houses.train.cormat))  
text.data.frame <- data.frame(x, y, cortext)
text.data.frame$cortext <- as.character(text.data.frame$cortext)
text.data.frame$cortext <- ifelse((text.data.frame$cortext == "1.000"),
    NA,text.data.frame$cortext)  # define diagonal cells as missing
text.data.frame <- na.omit(text.data.frame)  # diagonal cells have no text

print(levelplot(ordered.houses.train.cormat, cuts = 25, tick.number = 9,
  col.regions = 
    colorRampPalette(c(end.point.colors[1], "white", end.point.colors[2])),
  scales=list(tck=0, x=list(rot=90)),
  xlab = "", 
  ylab = "",
  panel = function(...) {
    panel.levelplot(...)  
    panel.text(text.data.frame$x, text.data.frame$y, 
    labels = text.data.frame$cortext)
    }))
dev.off()    

# scatter plot matrix (splom) demonstration
houses.train.splom.vars <- 
  houses.train[,c("log_value","income","age","rooms")]
pdf(file = "fig_spatial_sample_splom.pdf", width = 8.5, 
  height = 8.5)
pairs(houses.train.splom.vars, cex = 0.65, col = "darkblue")
dev.off()

# define spatial objects as needed for spatial modeling work 
# explanation of spatial objects may be found in chapter 2 of
# Bivand, R. S., Pebesma, E. J., and Gomez-Rubio, V. (2008) 
# Applied Spatial Data Analysis, New York: Springer.
# this involves adding coordinate objects to data frame objects
# training set coordinates to add
houses.coord <- cbind(houses.train$longitude,houses.train$latitude) 
# define spatial points data frame object
houses.train <- SpatialPointsDataFrame(houses.coord,houses.train,bbox = NULL) 

# test set coordinates to add
houses.coord <- cbind(houses.test$longitude,houses.test$latitude) 
# define spatial points data frame object
houses.test <- SpatialPointsDataFrame(houses.coord,houses.test,bbox = NULL) 

# examine the struction of the spatial points data frame
print(str(houses.train))  

# --------------------------------------------
# Linear regression a la Pace and Barry (1997)
# --------------------------------------------
pace.barry.train.fit <- lm(pace.barry.model, data = houses.train)

print(pace.barry.train.fit)
print(summary(pace.barry.train.fit))

# direct calculation of root-mean-squared prediction error 
# obtained directly on the training data
print(rmspe(houses.train$log_value, predict(pace.barry.train.fit))) 
# report R-squared on training data
print(cor(houses.train$log_value,predict(pace.barry.train.fit))^2)

cat("\n\nTraining set proportion of variance accounted",
  " for by linear regression = ",
  sprintf("%1.3f",cor(houses.train$log_value,
  predict(pace.barry.train.fit))^2),sep=" ")

# test model fit to training set on the test set
print(rmspe(houses.test$log_value, predict(pace.barry.train.fit, 
  newdata = houses.test))) 
print(cor(houses.test$log_value,
  predict(pace.barry.train.fit, newdata = houses.test))^2)
  
cat("\n\nTest set proportion of variance accounted",
  " for by linear regression = ",
  sprintf("%1.3f",cor(houses.test$log_value,
  predict(pace.barry.train.fit, newdata = houses.test))^2),sep=" ")  
  
# demonstrate cross-validation within the training set
# specify ten-fold cross-validation within the training set
# K = folds   R = replications of K-fold cross-validation
set.seed(1234)  # for reproducibility
folds <- cvFolds(nrow(houses.train), K = 10, R = 50)  
cv.pace.barry.train.fit <- cvLm(pace.barry.train.fit, cost = rtmspe, 
  folds = folds, trim = 0.1)
# root-mean-squared prediction error estimated by cross-validation
print(cv.pace.barry.train.fit)

# --------------------------------------
# Tree-structured regression (simple)
# --------------------------------------
# try tree-structured regression on the original explantory variables
# note that one of the advantages of trees is no need for transformations
# of the explanatory variables 
rpart.train.fit <- rpart(simple.model, data = houses.train)
print(summary(rpart.train.fit))  # tree summary statistics and split detail
houses.train$rpart.train.fit.pred <- predict(rpart.train.fit, 
  data = houses.train)

# root-mean-squared for trees on training set
print(rmspe(houses.train$log_value, houses.train$rpart.train.fit.pred)) 
# report R-squared on training data
print(cor(houses.train$log_value,houses.train$rpart.train.fit.pred)^2)

cat("\n\nTraining set proportion of variance accounted",
  " for by tree-structured regression = ",
  sprintf("%1.3f",cor(houses.train$log_value,
  houses.train$rpart.train.fit.pred)^2),sep=" ")

# root-mean-squared for trees on test set
houses.test$rpart.train.fit.pred <- predict(rpart.train.fit, newdata = houses.test)
print(rmspe(houses.test$log_value, houses.test$rpart.train.fit.pred)) 
# report R-squared on training data
print(cor(houses.test$log_value,houses.test$rpart.train.fit.pred)^2)

cat("\n\nTest set proportion of variance accounted",
  " for by tree-structured regression = ",
  sprintf("%1.3f",
  cor(houses.test$log_value,houses.test$rpart.train.fit.pred)^2),sep=" ")

# plot the regression tree result from rpart
pdf(file = "fig_spatial_rpart_model.pdf", width = 8.5, height = 8.5)
prp(rpart.train.fit, main="",
  digits = 3,  # digits to display in terminal nodes
  nn = TRUE,  # display the node numbers
  fallen.leaves = TRUE,  # put the leaves on the bottom of the page
  branch = 0.5,  # change angle of branch lines
  branch.lwd = 2,  # width of branch lines
  faclen = 0,  # do not abbreviate factor levels
  trace = 1,  # print the automatically calculated cex
  shadow.col = 0,  # no shadows under the leaves
  branch.lty = 1,  # draw branches using dotted lines
  split.cex = 1.2,  # make the split text larger than the node text
  split.prefix = "is ",  # put "is " before split text
  split.suffix = "?",  # put "?" after split text
  split.box.col = "blue",  # lightgray split boxes (default is white)
  split.col = "white",  # color of text in split box 
  split.border.col = "blue",  # darkgray border on split boxes
  split.round = .25)  # round the split box corners a tad
dev.off()

# --------------------------------------
# Tree-structured regression (full)
# --------------------------------------
# try tree-structured regression on the expanded set of variables 
rpart.train.fit.full <- rpart(full.model, data = houses.train)
print(summary(rpart.train.fit.full))  # tree summary statistics and split detail
houses.train$rpart.train.fit.full.pred <- 
  predict(rpart.train.fit.full, data = houses.train)

# root-mean-squared for trees on training set
print(rmspe(houses.train$log_value, houses.train$rpart.train.fit.full.pred)) 
# report R-squared on training data
print(cor(houses.train$log_value,houses.train$rpart.train.fit.full.pred)^2)

cat("\n\nTraining set proportion of variance accounted",
   " for by tree-structured regression (full model) = ",
  sprintf("%1.3f",cor(houses.train$log_value,
  houses.train$rpart.train.fit.full.pred)^2),sep=" ")

# root-mean-squared for trees on test set
houses.test$rpart.train.fit.full.pred <- predict(rpart.train.fit.full, 
  newdata = houses.test)
print(rmspe(houses.test$log_value, houses.test$rpart.train.fit.full.pred)) 
# report R-squared on training data
print(cor(houses.test$log_value,houses.test$rpart.train.fit.full.pred)^2)

cat("\n\nTest set proportion of variance accounted",
    " for by tree-structured regression (full model) = ",
  sprintf("%1.3f",cor(houses.test$log_value,
  houses.test$rpart.train.fit.full.pred)^2),sep=" ")

# plot the regression tree result from rpart
pdf(file = "fig_spatial_rpart_model_full.pdf", width = 8.5, height = 8.5)
prp(rpart.train.fit.full, main="",
  digits = 3,  # digits to display in terminal nodes
  nn = TRUE,  # display the node numbers
  fallen.leaves = TRUE,  # put the leaves on the bottom of the page
  branch = 0.5,  # change angle of branch lines
  branch.lwd = 2,  # width of branch lines
  faclen = 0,  # do not abbreviate factor levels
  trace = 1,  # print the automatically calculated cex
  shadow.col = 0,  # no shadows under the leaves
  branch.lty = 1,  # draw branches using dotted lines
  split.cex = 1.2,  # make the split text larger than the node text
  split.prefix = "is ",  # put "is" before split text
  split.suffix = "?",  # put "?" after split text
  split.box.col = "blue",  # lightgray split boxes (default is white)
  split.col = "white",  # color of text in split box 
  split.border.col = "blue",  # darkgray border on split boxes
  split.round = .25)  # round the split box corners a tad
dev.off()

# --------------------------------------
# Random forests (simple)
# --------------------------------------
set.seed (9999)  # for reproducibility
rf.train.fit <- randomForest(simple.model, 
  data=houses.train, mtry=3, importance=TRUE, na.action=na.omit) 

# review the random forest solution      
print(rf.train.fit)  

# check importance of the individual explanatory variables 
pdf(file = "fig_spatial_random_forest_simple_importance.pdf", 
width = 11, height = 8.5)
varImpPlot(rf.train.fit, main = "", pch = 20, col = "darkblue")
dev.off()

# random forest predictions for the training set
houses.train$rf.train.fit.pred <- predict(rf.train.fit, type="class", 
  newdata = houses.train)

# root-mean-squared for random forest on training set
print(rmspe(houses.train$log_value, houses.train$rf.train.fit.pred)) 
# report R-squared on training data
print(cor(houses.train$log_value,houses.train$rf.train.fit.pred)^2)

cat("\n\nTraining set proportion of variance accounted",
    "for by random forests (simple model) = ",
  sprintf("%1.3f",
  cor(houses.train$log_value,houses.train$rf.train.fit.pred)^2),sep=" ")
    
# random forest predictions for the test set using model from training set
houses.test$rf.train.fit.pred <- predict(rf.train.fit, 
  type="class", newdata = houses.test)

# root-mean-squared for random forest on test set
print(rmspe(houses.test$log_value, houses.test$rf.train.fit.pred)) 
# report R-squared on training data
print(cor(houses.test$log_value,houses.test$rf.train.fit.pred)^2)

cat("\n\nTest set proportion of variance accounted",
    " for by random forests (simple model) = ",
  sprintf("%1.3f",
  cor(houses.test$log_value,houses.test$rf.train.fit.pred)^2),sep=" ")

# --------------------------------------
# Random forests (full)
# --------------------------------------
set.seed (9999)  # for reproducibility
rf.train.fit.full <- randomForest(full.model, 
  data=houses.train, mtry=3, importance=TRUE, na.action=na.omit) 

# review the random forest solution      
print(rf.train.fit.full)  

# check importance of the individual explanatory variables 
pdf(file = "fig_spatial_random_forest_full_importance.pdf", 
width = 11, height = 8.5)
varImpPlot(rf.train.fit.full, main = "", pch = 20, 
  cex = 1.25, col = "darkblue", lcolor = "black")
dev.off()

# random forest predictions for the training set
houses.train$rf.train.fit.full.pred <- predict(rf.train.fit.full, type="class", 
  newdata = houses.train)

# root-mean-squared for random forest on training set
print(rmspe(houses.train$log_value, houses.train$rf.train.fit.full.pred)) 
# report R-squared on training data
print(cor(houses.train$log_value,houses.train$rf.train.fit.full.pred)^2)

cat("\n\nTraining set proportion of variance accounted",
    " for by random forests (full model) = ",
  sprintf("%1.3f",cor(houses.train$log_value,
    houses.train$rf.train.fit.full.pred)^2),sep=" ")
    
# random forest predictions for the test set using model from training set
houses.test$rf.train.fit.full.pred <- predict(rf.train.fit.full, type="class", 
  newdata = houses.test)

# root-mean-squared for random forest on test set
print(rmspe(houses.test$log_value, houses.test$rf.train.fit.full.pred)) 
# report R-squared on training data
print(cor(houses.test$log_value,houses.test$rf.train.fit.full.pred)^2)

cat("\n\nTest set proportion of variance accounted",
    " for by random forests (full model) = ",
  sprintf("%1.3f",cor(houses.test$log_value,
    houses.test$rf.train.fit.full.pred)^2),sep=" ")
           
# --------------------------------------
# Geographically weighted regression
# --------------------------------------    
# bandwidth calculation may take a while
set.bandwidth <-  gwr.sel(pace.barry.model, 
  data=houses.train, verbose = FALSE, show.error.messages = FALSE) 

# fit the geographically-weighted regression with bandwidth value set.bandwidth
gwr.train.fit <- gwr(pace.barry.model, bandwidth = set.bandwidth, 
  predictions = TRUE, data=houses.train, fit.points = houses.train)
# extract training set predictions
houses.train$grw.train.fit.pred <- gwr.train.fit$SDF$pred  

# root-mean-squared for grw on training set
print(rmspe(houses.train$log_value, houses.train$grw.train.fit.pred)) 
# report R-squared on training data
print(cor(houses.train$log_value,houses.train$grw.train.fit.pred)^2)

cat("\n\nTraining set proportion of variance accounted",
  " for by geographically-weighted regression = ",
  sprintf("%1.3f",cor(houses.train$log_value,
  houses.train$grw.train.fit.pred)^2),sep=" ")

# fit the geographically-weighted regression with bandwidth value set.bandwidth
# fit to training data and specify test data
gwr.train.fit <- gwr(pace.barry.model, bandwidth = set.bandwidth, 
  predictions = TRUE, data=houses.train, fit.points = houses.test)
# extract test set predictions
houses.test$grw.train.fit.pred <- gwr.train.fit$SDF$pred  

# root-mean-squared for grw on test set
print(rmspe(houses.test$log_value, houses.test$grw.train.fit.pred)) 
# report R-squared on training data
print(cor(houses.test$log_value,houses.test$grw.train.fit.pred)^2)

cat("\n\nTest set proportion of variance accounted",
  " for by geographically-weighted regression = ",
  sprintf("%1.3f",cor(houses.test$log_value,
  houses.test$grw.train.fit.pred)^2),sep=" ")

# --------------------------------------
# Gather results for a single report
# --------------------------------------     
# measurement model performance summary
methods <- c("Linear regression Pace and Barry (1997)",
  "Tree-structured regression (simple model)",
  "Tree-structured regression (full model)",
  "Random forests (simple model)",
  "Random forests (full model)",
  "Geographically weighted regression (GWR)",
  "Hybrid Random Forests and GWR")
methods.performance.data.frame <- data.frame(methods)

methods.performance.data.frame$training <- 
  c(round(cor(houses.train$log_value,predict(pace.barry.train.fit))^2
    ,digits=3),
    round(cor(houses.train$log_value,
    houses.train$rpart.train.fit.pred)^2,digits=3),
    round(cor(houses.train$log_value,
    houses.train$rpart.train.fit.full.pred)^2,digits=3),
    round(cor(houses.train$log_value,
    houses.train$rf.train.fit.pred)^2,digits=3),
     round(cor(houses.train$log_value,
     houses.train$rf.train.fit.full.pred)^2,digits=3),
    round(cor(houses.train$log_value,
    houses.train$grw.train.fit.pred)^2,digits=3))
  
methods.performance.data.frame$test <-
  c(round(cor(houses.test$log_value,
  predict(pace.barry.train.fit, newdata = houses.test))^2,digits=3),
    round(cor(houses.test$log_value,
    houses.test$rpart.train.fit.pred)^2,digits=3),
    round(cor(houses.test$log_value,
    houses.test$rpart.train.fit.full.pred)^2,digits=3),
    round(cor(houses.test$log_value,
    houses.test$rf.train.fit.pred)^2,digits=3),
    round(cor(houses.test$log_value,
    houses.test$rf.train.fit.full.pred)^2,digits=3),
    round(cor(houses.test$log_value,
    houses.test$grw.train.fit.pred)^2,digits=3))

print(methods.performance.data.frame)

# Hierarchical Bayes Part-Worth Estimation: Training and Test

# load market simulation utilities
load(file="mtpa_market_simulation_utilities.RData")

library(ChoiceModelR)  # for Hierarchical Bayes Estimation

library(caret)  # for confusion matrix function

# read in the data from a case study in computer choice.
complete.data.frame <- read.csv("computer_choice_study.csv")

print.digits <- 2
# user-defined function for printing conjoint measures
if (print.digits == 2) 
  pretty.print <- function(x) {sprintf("%1.2f",round(x,digits = 2))} 
if (print.digits == 3) 
  pretty.print <- function(x) {sprintf("%1.3f",round(x,digits = 3))} 
 
# set up sum contrasts for effects coding
options(contrasts=c("contr.sum","contr.poly"))

# employ a training-and-test regimen across survey sets/items 
test.set.ids <- c("3","7","11","15")  # select four sets/items
training.set.ids <- setdiff(unique(complete.data.frame$setid),test.set.ids)
training.data.frame <- 
  subset(complete.data.frame,subset=(setid %in% training.set.ids))
test.data.frame <- 
  subset(complete.data.frame,subset=(setid %in% test.set.ids))

UniqueID <- unique(training.data.frame$id)
# set up zero priors
cc.priors <- matrix(0,nrow=length(UniqueID),ncol=13) 

# we could use coefficients from aggregate model as starting values
# here we comment out the code needed to do that
# aggregate.cc.betas <- c(as.numeric(conjoint.results$coefficients)[2:7],
#  -sum(as.numeric(conjoint.results$coefficients)[2:7]),
#  as.numeric(conjoint.results$coefficients)[8:13])
# clone aggregate part-worths across the individuals in the study
# set up Bayesian priors
# cc.priors <- matrix(0,nrow=length(UniqueID),ncol=length(aggregate.cc.betas)) 
# for(index.for.ID in seq(along=UniqueID))
# cc.priors[index.for.ID,] <- aggregate.cc.betas

colnames(cc.priors) <- c("A1B1","A1B2","A1B3","A1B4","A1B5","A1B6","A1B7",
  "A1B8","A2B1","A3B1","A4B1","A5B1","A6B1")

# note that the actual names are as follows: 
AB.names <- c("Apple","Compaq","Dell","Gateway","HP","IBM","Sony","Sun",
  "Compatibility","Performance","Reliability","Learning","Price")

# set up run parameters for the MCMC
# using aggregate beta estimates to get started
truebetas <- cc.priors
cc.xcoding <- c(0,1,1,1,1,1)  # first variable categorical others continuous
cc.attlevels <- c(8,8,4,2,8,8) # test run with all attributes and levels
# no constraint for order on brand so 8x8 matrix of zeroes
c1 <- matrix(0,ncol=8,nrow=8)

# compatibility is ordered higher numbers are better
# continuous attributes have 1x1 matrix representation
c2 <- matrix(1, ncol = 1, nrow = 1, byrow = TRUE)

# performance is ordered higher numbers are better
# continuous attributes have 1x1 matrix representation
c3 <- matrix(1, ncol = 1, nrow = 1, byrow = TRUE)

# reliability is ordered higher numbers are better
# continuous attributes have 1x1 matrix representation
c4 <- matrix(1, ncol = 1, nrow = 1, byrow = TRUE)

# learning has expected order... higher learning times less valued
# continuous attributes have 1x1 matrix representation
c5 <- matrix(-1, ncol = 1, nrow = 1, byrow = TRUE)

# price has expected order... higher prices less valued
# continuous attributes have 1x1 matrix representation
c6 <- matrix(-1, ncol = 1, nrow = 1, byrow = TRUE)

cc.constraints <- list(c1,c2,c3,c4,c5,c6)

# controls for length of run and sampling from end of run
# cc.mcmc <- list(R = 10, use = 10) # fast trial run
# set run parameters 10000 total iterations with estimates based on last 2000
cc.mcmc <- list(R = 10000, use = 2000) # run parameters

# run options
cc.options <- list(none=FALSE, save=TRUE, keep=1)

# set up the data frame for analysis
# redefine set ids so they are a complete set 1-12 as needed for HB functions
training.data.frame$newsetid <- training.data.frame$setid
training.data.frame$newsetid <- ifelse((training.data.frame$newsetid == 16),
  3,training.data.frame$newsetid)
training.data.frame$newsetid <- ifelse((training.data.frame$newsetid == 14),
  7,training.data.frame$newsetid)
training.data.frame$newsetid <- ifelse((training.data.frame$newsetid == 13),
  11,training.data.frame$newsetid)

UnitID <- training.data.frame$id
Set <- as.integer(training.data.frame$newsetid)
Alt <- as.integer(training.data.frame$position)
X_1 <- as.integer(training.data.frame$brand) # categories by brand
X_2 <- as.integer(training.data.frame$compat)  # integer values 1 to 8
X_3 <- as.integer(training.data.frame$perform)  # integer values 1 to 4
X_4 <- as.integer(training.data.frame$reliab)  # integer values 1 to 2
X_5 <- as.integer(training.data.frame$learn)  # integer values 1 to 8
X_6 <- as.integer(training.data.frame$price)  # integer values 1 to 8
y <- as.numeric(training.data.frame$choice)  # using special response coding

cc.data <- data.frame(UnitID,Set,Alt,X_1,X_2,X_3,X_4,X_5,X_6,y)

# now for the estimation... be patient
set.seed(9999)  # for reproducible results
out <- choicemodelr(data=cc.data, xcoding = cc.xcoding, 
  mcmc = cc.mcmc, options = cc.options, constraints = cc.constraints)

# out provides a list for the posterior parameter estimates 
# for the runs sampled (use = 2000)

# the MCMC beta parameter estimates are traced on the screen as it runs

# individual part-worth estimates are provided in the output file RBetas.csv
# the final estimates are printed to RBetas.csv with columns labeled as
#  A1B1 = first attribute first level
#  A1B2 = first attribute second level
#  ....
#  A2B1 = second attribute first level
#  ....
# gather data from HB posterior parameter distributions
# we imposed constraints on all continuous parameters so we use betadraw.c
posterior.mean <- matrix(0, nrow = dim(out$betadraw.c)[1], 
  ncol = dim(out$betadraw.c)[2])
posterior.sd <- matrix(0, nrow = dim(out$betadraw.c)[1], 
  ncol = dim(out$betadraw.c)[2])
for(index.row in 1:dim(out$betadraw.c)[1])
for(index.col in 1:dim(out$betadraw.c)[2]) { 
  posterior.mean[index.row,index.col] <- 
    mean(out$betadraw.c[index.row,index.col,])
  posterior.sd[index.row,index.col] <- 
    sd(out$betadraw.c[index.row,index.col,])
  }

# HB program uses effects coding for categorical variables and
# mean-centers continuous variables across the levels appearing in the data
# working with data for one respondent at a time we compute predicted choices
# for both the training and test choice sets

create.design.matrix <- function(input.data.frame.row) {
  xdesign.row <- numeric(12)
  if (input.data.frame.row$brand == "Apple") 
    xdesign.row[1:7] <- c(1,0,0,0,0,0,0)  
  if (input.data.frame.row$brand == "Compaq") 
    xdesign.row[1:7] <- c(0,1,0,0,0,0,0)  
  if (input.data.frame.row$brand == "Dell") 
    xdesign.row[1:7] <- c(0,0,1,0,0,0,0)  
  if (input.data.frame.row$brand == "Gateway") 
    xdesign.row[1:7] <- c(0,0,0,1,0,0,0)  
  if (input.data.frame.row$brand == "HP") 
    xdesign.row[1:7] <- c(0,0,0,0,1,0,0)  
  if (input.data.frame.row$brand == "IBM") 
    xdesign.row[1:7] <- c(0,0,0,0,0,1,0)  
  if (input.data.frame.row$brand == "Sony") 
    xdesign.row[1:7] <- c(0,0,0,0,0,0,1)  
  if (input.data.frame.row$brand == "Sun") 
    xdesign.row[1:7] <- c(-1,-1,-1,-1,-1,-1,-1)    
  
  xdesign.row[8] <- input.data.frame.row$compat -4.5 
  xdesign.row[9] <- input.data.frame.row$perform -2.5
  xdesign.row[10] <- input.data.frame.row$reliab -1.5 
  xdesign.row[11] <- input.data.frame.row$learn -4.5
  xdesign.row[12] <- input.data.frame.row$price -4.5 
  t(as.matrix(xdesign.row))  # return row of design matrix
  }

# evaluate performance in the training set
training.choice.utility <- NULL  # initialize utility vector
# work with one row of respondent training data frame at a time
# create choice prediction using the individual part-worths
list.of.ids <- unique(training.data.frame$id)
for (index.for.id in seq(along=list.of.ids)) {
  this.id.part.worths <- posterior.mean[index.for.id,] 
  this.id.data.frame <- subset(training.data.frame, 
    subset=(id == list.of.ids[index.for.id]))
  for (index.for.profile in 1:nrow(this.id.data.frame)) {   
    training.choice.utility <- c(training.choice.utility,
      create.design.matrix(this.id.data.frame[index.for.profile,]) %*%
      this.id.part.worths)
    }  
  }  

training.predicted.choice <- 
  choice.set.predictor(training.choice.utility)
training.actual.choice <- factor(training.data.frame$choice, levels = c(0,1), 
  labels = c("NO","YES"))  
# look for sensitivity > 0.25 for four-profile choice sets 
training.set.performance <- confusionMatrix(data = training.predicted.choice, 
  reference = training.actual.choice, positive = "YES")
# report choice prediction sensitivity for training data
cat("\n\nTraining choice set sensitivity = ",
  sprintf("%1.1f",training.set.performance$byClass[1]*100)," Percent",sep="")

# evaluate performance in the test set
test.choice.utility <- NULL  # initialize utility vector
# work with one row of respondent test data frame at a time
# create choice prediction using the individual part-worths
list.of.ids <- unique(test.data.frame$id)
for (index.for.id in seq(along=list.of.ids)) {
  this.id.part.worths <- posterior.mean[index.for.id,] 
  this.id.data.frame <- subset(test.data.frame, 
    subset=(id == list.of.ids[index.for.id]))
  for (index.for.profile in 1:nrow(this.id.data.frame)) {    
    test.choice.utility <- c(test.choice.utility,
      create.design.matrix(this.id.data.frame[index.for.profile,]) %*%
      this.id.part.worths)
    }  
  }  

test.predicted.choice <- 
  choice.set.predictor(test.choice.utility)
test.actual.choice <- factor(test.data.frame$choice, levels = c(0,1), 
  labels = c("NO","YES"))  
# look for sensitivity > 0.25 for four-profile choice sets 
test.set.performance <- confusionMatrix(data = test.predicted.choice, 
  reference = test.actual.choice, positive = "YES")
# report choice prediction sensitivity for test data
cat("\n\nTest choice set sensitivity = ",
  sprintf("%1.1f",test.set.performance$byClass[1]*100)," Percent",sep="")

# Hierarchical Bayes Part-Worth Estimation and Study of Consumer Preferences

# having demonstrated the predictive power of the HB model...
# we now return to the complete set of 16 choice sets to obtain 
# individual-level part-worths for further analysis 
# analysis guided by ternary model of consumer preference and market response
# brand loyalty... price sensitivity... and feature focus... are key aspects
# to consider in determining pricing policy

library(lattice)  # package for lattice graphics 
library(vcd)  # graphics package with mosaic plots for mosaic and ternary plots
library(ggplot2)  # package ggplot implements Grammar of Graphics approach
library(ChoiceModelR)  # for Hierarchical Bayes Estimation
library(caret)  # for confusion matrix... evaluation of choice set predictions

# load split-plotting utilities for work with ggplot
load("mtpa_split_plotting_utilities.Rdata")

# load market simulation utilities 
load(file="mtpa_market_simulation_utilities.RData")

# read in the data from a case study in computer choice.
complete.data.frame <- read.csv("computer_choice_study.csv")
# we employed a training-and-test regimen in previous research work
# here we will be using the complete data from the computer choice study
working.data.frame <- complete.data.frame


# user-defined function for plotting descriptive attribute names 
effect.name.map <- function(effect.name) { 
  if(effect.name=="brand") return("Manufacturer/Brand")
  if(effect.name=="compat") return("Compatibility with Windows 95")
  if(effect.name=="perform") return("Performance")
  if(effect.name=="reliab") return("Reliability")
  if(effect.name=="learn") return("Learning Time (4 to 32 hours)")
  if(effect.name=="price") return("Price ($1,000 to $2,750)")
  } 

print.digits <- 2
# user-defined function for printing conjoint measures
if (print.digits == 2) 
  pretty.print <- function(x) {sprintf("%1.2f",round(x,digits = 2))} 
if (print.digits == 3) 
  pretty.print <- function(x) {sprintf("%1.3f",round(x,digits = 3))} 
 
# set up sum contrasts for effects coding
options(contrasts=c("contr.sum","contr.poly"))

UniqueID <- unique(working.data.frame$id)
# set up zero priors
cc.priors <- matrix(0,nrow=length(UniqueID),ncol=13) 

colnames(cc.priors) <- c("A1B1","A1B2","A1B3","A1B4","A1B5","A1B6","A1B7",
  "A1B8","A2B1","A3B1","A4B1","A5B1","A6B1")

# note that the actual names are as follows: 
AB.names <- c("Apple","Compaq","Dell","Gateway","HP","IBM","Sony","Sun",
  "Compatibility","Performance","Reliability","Learning","Price")

# set up run parameters for the MCMC
# using aggregate beta estimates to get started
truebetas <- cc.priors
cc.xcoding <- c(0,1,1,1,1,1)  # first variable categorical others continuous
cc.attlevels <- c(8,8,4,2,8,8) # test run with all attributes and levels
# no constraint for order on brand so 8x8 matrix of zeroes
c1 <- matrix(0,ncol=8,nrow=8)

# compatibility is ordered higher numbers are better
# continuous attributes have 1x1 matrix representation
c2 <- matrix(1, ncol = 1, nrow = 1, byrow = TRUE)

# performance is ordered higher numbers are better
# continuous attributes have 1x1 matrix representation
c3 <- matrix(1, ncol = 1, nrow = 1, byrow = TRUE)

# reliability is ordered higher numbers are better
# continuous attributes have 1x1 matrix representation
c4 <- matrix(1, ncol = 1, nrow = 1, byrow = TRUE)

# learning has expected order... higher learning times less valued
# continuous attributes have 1x1 matrix representation
c5 <- matrix(-1, ncol = 1, nrow = 1, byrow = TRUE)

# price has expected order... higher prices less valued
# continuous attributes have 1x1 matrix representation
c6 <- matrix(-1, ncol = 1, nrow = 1, byrow = TRUE)

cc.constraints <- list(c1,c2,c3,c4,c5,c6)

# controls for length of run and sampling from end of run
# cc.mcmc <- list(R = 10, use = 10) # fast trial run
# set run parameters 10000 total iterations with estimates based on last 2000
cc.mcmc <- list(R = 10000, use = 2000) # run parameters

# run options
cc.options <- list(none=FALSE, save=TRUE, keep=1)

# set up the data frame for analysis
UnitID <- working.data.frame$id
Set <- as.integer(working.data.frame$setid)
Alt <- as.integer(working.data.frame$position)
X_1 <- as.integer(working.data.frame$brand) # categories by brand
X_2 <- as.integer(working.data.frame$compat)  # integer values 1 to 8
X_3 <- as.integer(working.data.frame$perform)  # integer values 1 to 4
X_4 <- as.integer(working.data.frame$reliab)  # integer values 1 to 2
X_5 <- as.integer(working.data.frame$learn)  # integer values 1 to 8
X_6 <- as.integer(working.data.frame$price)  # integer values 1 to 8
y <- as.numeric(working.data.frame$choice)  # using special response coding

cc.data <- data.frame(UnitID,Set,Alt,X_1,X_2,X_3,X_4,X_5,X_6,y)

# the estimation begins here... be patient
set.seed(9999)  # for reproducible results
out <- choicemodelr(data=cc.data, xcoding = cc.xcoding, 
  mcmc = cc.mcmc, options = cc.options, constraints = cc.constraints)

# out provides a list for the posterior parameter estimates 
# for the runs sampled (use = 2000)

# the MCMC beta parameter estimates are traced on the screen as it runs

# individual part-worth estimates are provided in the output file RBetas.csv
# the final estimates are printed to RBetas.csv with columns labeled as
#  A1B1 = first attribute first level
#  A1B2 = first attribute second level
#  ....
#  A2B1 = second attribute first level
#  ....

# gather data from HB posterior parameter distributions
# we imposed constraints on all continuous parameters so we use betadraw.c
posterior.mean <- matrix(0, nrow = dim(out$betadraw.c)[1], 
  ncol = dim(out$betadraw.c)[2])
posterior.sd <- matrix(0, nrow = dim(out$betadraw.c)[1], 
  ncol = dim(out$betadraw.c)[2])
for(index.row in 1:dim(out$betadraw.c)[1])
for(index.col in 1:dim(out$betadraw.c)[2]) { 
  posterior.mean[index.row,index.col] <- 
    mean(out$betadraw.c[index.row,index.col,])
  posterior.sd[index.row,index.col] <- 
    sd(out$betadraw.c[index.row,index.col,])
  }

# HB program uses effects coding for categorical variables and
# mean-centers continuous variables across the levels appearing in the data
# working with data for one respondent at a time we compute predicted choices
# for the full set of consumer responses

create.design.matrix <- function(input.data.frame.row) {
  xdesign.row <- numeric(12)
  if (input.data.frame.row$brand == "Apple") 
    xdesign.row[1:7] <- c(1,0,0,0,0,0,0)  
  if (input.data.frame.row$brand == "Compaq") 
    xdesign.row[1:7] <- c(0,1,0,0,0,0,0)  
  if (input.data.frame.row$brand == "Dell") 
    xdesign.row[1:7] <- c(0,0,1,0,0,0,0)  
  if (input.data.frame.row$brand == "Gateway") 
    xdesign.row[1:7] <- c(0,0,0,1,0,0,0)  
  if (input.data.frame.row$brand == "HP") 
    xdesign.row[1:7] <- c(0,0,0,0,1,0,0)  
  if (input.data.frame.row$brand == "IBM") 
    xdesign.row[1:7] <- c(0,0,0,0,0,1,0)  
  if (input.data.frame.row$brand == "Sony") 
    xdesign.row[1:7] <- c(0,0,0,0,0,0,1)  
  if (input.data.frame.row$brand == "Sun") 
    xdesign.row[1:7] <- c(-1,-1,-1,-1,-1,-1,-1)    
  
  xdesign.row[8] <- input.data.frame.row$compat -4.5 
  xdesign.row[9] <- input.data.frame.row$perform -2.5
  xdesign.row[10] <- input.data.frame.row$reliab -1.5 
  xdesign.row[11] <- input.data.frame.row$learn -4.5
  xdesign.row[12] <- input.data.frame.row$price -4.5 
  t(as.matrix(xdesign.row))  # return row of design matrix
  }

# evaluate performance in the full set of consumer responses
working.choice.utility <- NULL  # initialize utility vector
# work with one row of respondent training data frame at a time
# create choice prediction using the individual part-worths
list.of.ids <- unique(working.data.frame$id)
for (index.for.id in seq(along=list.of.ids)) {
  this.id.part.worths <- posterior.mean[index.for.id,] 
  this.id.data.frame <- subset(working.data.frame, 
    subset=(id == list.of.ids[index.for.id]))
  for (index.for.profile in 1:nrow(this.id.data.frame)) {   
    working.choice.utility <- c(working.choice.utility,
      create.design.matrix(this.id.data.frame[index.for.profile,]) %*%
      this.id.part.worths)
    }  
  }  

working.predicted.choice <- 
  choice.set.predictor(working.choice.utility)
working.actual.choice <- factor(working.data.frame$choice, levels = c(0,1), 
  labels = c("NO","YES"))  
# look for sensitivity > 0.25 for four-profile choice sets 
working.set.performance <- confusionMatrix(data = working.predicted.choice, 
  reference = working.actual.choice, positive = "YES")
# report choice prediction sensitivity for the full data
cat("\n\nFull data set choice set sensitivity = ",
  sprintf("%1.1f",working.set.performance$byClass[1]*100)," Percent",sep="")
# 
# results: Full data set choice set sensitivity = 89.1 Percent
#

# to continue with our analysis of consumer preferences...
# we build a data frame for the consumers with the full set of eight brands
ID <- unique(working.data.frame$id)
Apple <- posterior.mean[,1]
Compaq <- posterior.mean[,2]
Dell <- posterior.mean[,3]
Gateway <- posterior.mean[,4]
HP <- posterior.mean[,5]
IBM <- posterior.mean[,6]
Sony <- posterior.mean[,7]

Sun <- -1 * (Apple + Compaq + Dell + Gateway + HP + IBM + Sony)

Compatibility <- posterior.mean[,8]
Performance <- posterior.mean[,9]
Reliability <- posterior.mean[,10]
Learning <- posterior.mean[,11]
Price <- posterior.mean[,12]

# creation of data frame for analysis of consumer preferences and choice
# starting with individual-level part-worths... more to be added shortly
id.data <- data.frame(ID,Apple,Compaq,Dell,Gateway,HP,IBM,Sony,Sun,
  Compatibility,Performance,Reliability,Learning,Price)

# compute attribute importance values for each attribute
id.data$brand.range <- numeric(nrow(id.data))
id.data$compatibility.range <- numeric(nrow(id.data))
id.data$performance.range <- numeric(nrow(id.data))
id.data$reliability.range <- numeric(nrow(id.data))
id.data$learning.range <- numeric(nrow(id.data))
id.data$price.range <- numeric(nrow(id.data))
id.data$sum.range <- numeric(nrow(id.data))
id.data$brand.importance <- numeric(nrow(id.data))
id.data$compatibility.importance <- numeric(nrow(id.data))
id.data$performance.importance <- numeric(nrow(id.data))
id.data$reliability.importance <- numeric(nrow(id.data))
id.data$learning.importance <- numeric(nrow(id.data))
id.data$price.importance <- numeric(nrow(id.data))

for(id in seq(along=id.data$ID)) {
  id.data$brand.range[id] <- max(id.data$Apple[id],
    id.data$Compaq[id],id.data$Dell[id],
    id.data$Gateway[id],id.data$HP[id],
    id.data$IBM[id],id.data$Sony[id],
    id.data$Sun[id]) - 
    min(id.data$Apple[id],
    id.data$Compaq[id],id.data$Dell[id],
    id.data$Gateway[id],id.data$HP[id],
    id.data$IBM[id],id.data$Sony[id],
    id.data$Sun[id])
 
  id.data$compatibility.range[id] <- abs(8*id.data$Compatibility[id])  
  id.data$performance.range[id] <- abs(4*id.data$Performance[id]) 
  id.data$reliability.range[id] <- abs(2*id.data$Reliability[id]) 
  id.data$learning.range[id] <- abs(8*id.data$Learning[id])
  id.data$price.range[id] <-  abs(8*id.data$Price[id])

  id.data$sum.range[id] <- id.data$brand.range[id] + 
    id.data$compatibility.range[id] +
    id.data$performance.range[id] +
    id.data$reliability.range[id] +
    id.data$learning.range[id] +
    id.data$price.range[id]
 
  id.data$brand.importance[id] <- 
    id.data$brand.range[id]/id.data$sum.range[id]
  id.data$compatibility.importance[id] <- 
    id.data$compatibility.range[id]/id.data$sum.range[id]
  id.data$performance.importance[id] <- 
    id.data$performance.range[id]/id.data$sum.range[id]
  id.data$reliability.importance[id] <- 
    id.data$reliability.range[id]/id.data$sum.range[id]
  id.data$learning.importance[id] <- 
    id.data$learning.range[id]/id.data$sum.range[id]
  id.data$price.importance[id] <- 
    id.data$price.range[id]/id.data$sum.range[id]
  
# feature importance relates to the most important product feature
# considering product features as not brand and not price
  id.data$feature.importance[id] <- max(id.data$compatibility.importance[id],
    id.data$performance.importance[id], 
    id.data$reliability.importance[id], 
    id.data$learning.importance[id])
  }
 
# identify each individual's top brand defining top.brand factor variable
id.data$top.brand <- integer(nrow(id.data)) 
for(id in seq(along=id.data$ID)) {
  brand.index <- 1:8
  brand.part.worth <- c(id.data$Apple[id],id.data$Compaq[id],
    id.data$Dell[id],id.data$Gateway[id],id.data$HP[id],id.data$IBM[id],
    id.data$Sony[id],id.data$Sun[id])
  temp.data <- data.frame(brand.index,brand.part.worth)
  temp.data <- temp.data[sort.list(temp.data$brand.part.worth, decreasing = TRUE),]
  id.data$top.brand[id] <- temp.data$brand.index[1]
  }
id.data$top.brand <- factor(id.data$top.brand, levels = 1:8,
  labels = c("Apple","Compaq","Dell","Gateway",
  "HP","IBM","Sony","Sun"))

# note that the standard importance measures from conjoint methods are
# ipsative... their sum is always 1 for proportions or 100 for percentages 
# this has advantages for triplots (ternary plots) but because importance
# is so dependent upon the levels of attributes, it has significant
# disadvantages as well... so we consider a relative-value-based measure
# lets us define an alternative to importance called "attribute value"
# compute "attribute value" relative to the consumer group 
# it is a standardized measure... let "attribute value" be mean 50 sd 10
# here are user-defined functions to use to obtain "value"

standardize <- function(x) {
# standardize x so it has mean zero and standard deviation 1
  (x - mean(x))/sd(x)
  }
compute.value <- function(x) {
# rescale x so it has the same mean and standard deviation as y  
  standardize(x) * 10 + 50
 }

id.data$brand.value <- compute.value(id.data$brand.range)
id.data$compatibility.value <- compute.value(id.data$compatibility.range)
id.data$performance.value <- compute.value(id.data$performance.range)
id.data$reliability.value <- compute.value(id.data$reliability.range)
id.data$learning.value <- compute.value(id.data$learning.range)
id.data$price.value <- compute.value(id.data$price.range)

# identify each individual's top value using computed relative attribute values 
id.data$top.attribute <- integer(nrow(id.data)) 
for(id in seq(along=id.data$ID)) {
  attribute.index <- 1:6
  attribute.value <- c(id.data$brand.value[id],id.data$compatibility.value[id],
    id.data$performance.value[id],id.data$reliability.value[id],
    id.data$learning.value[id],id.data$price.value[id])
  temp.data <- data.frame(attribute.index,attribute.value)
  temp.data <- 
    temp.data[sort.list(temp.data$attribute.value, decreasing = TRUE),]
  id.data$top.attribute[id] <- temp.data$attribute.index[1]
  }
id.data$top.attribute <- factor(id.data$top.attribute, levels = 1:6,
  labels = c("Brand","Compatibility","Performance","Reliability",
  "Learning","Price"))

# mosaic plot of joint frequencies top ranked brand by top value
pdf(file="fig_price_top_top_mosaic_plot.pdf", width = 8.5, height = 11)
  mosaic( ~ top.brand + top.attribute, data = id.data, 
  highlighting = "top.attribute",
  highlighting_fill = 
    c("blue", "white", "green","lightgray","magenta","black"),
  labeling_args = 
  list(set_varnames = c(top.brand = "", top.attribute = ""),
  rot_labels = c(left = 90, top = 45),
  pos_labels = c("center","center"),
  just_labels = c("left","center"),
  offset_labels = c(0.0,0.0)))  
dev.off()  

# an alternative representation that is often quite useful in pricing studies
# is a triplot/ternary plot with three features identified for each consumer
# using the idea from importance caluclations we now use price, brand, and 
# feature importance measures to obtain data for three-way plots
# as the basis for three relative measures, which we call brand.loyalty,
# price.sensitivity, and feature_focus...

id.data$brand.loyalty <- numeric(nrow(id.data)) 
id.data$price.sensitivity <- numeric(nrow(id.data)) 
id.data$feature.focus <- numeric(nrow(id.data)) 
for(id in seq(along=id.data$ID)) {
  sum.importances <- id.data$brand.importance[id] + 
  id.data$price.importance[id] +
  id.data$feature.importance[id]  # less than 1.00 feature is an average
  id.data$brand.loyalty[id] <- id.data$brand.importance[id]/sum.importances
  id.data$price.sensitivity[id] <- id.data$price.importance[id]/sum.importances
  id.data$feature.focus[id] <- id.data$feature.importance[id]/sum.importances
  }

# ternary model of consumer response... the plot  
pdf("fig_price_ternary_three_brands.pdf", width = 11, height = 8.5)  
ternaryplot(id.data[,c("brand.loyalty","price.sensitivity","feature.focus")], 
dimnames = c("Brand Loyalty","Price Sensitivity","Feature Focus"),
prop_size = ifelse((id.data$top.brand == "Apple"), 0.8, 
            ifelse((id.data$top.brand == "Dell"),0.7,
            ifelse((id.data$top.brand == "HP"),0.7,0.5))),
pch = ifelse((id.data$top.brand == "Apple"), 20, 
      ifelse((id.data$top.brand == "Dell"),17,
      ifelse((id.data$top.brand == "HP"),15,1))),
col = ifelse((id.data$top.brand == "Apple"), "red",
      ifelse((id.data$top.brand == "Dell"),"mediumorchid4",
      ifelse((id.data$top.brand == "HP"),"blue","darkblue"))),
grid_color = "#626262",
bg = "#E6E6E6",
dimnames_position = "corner", main = ""
) 
grid_legend(0.725, 0.8, pch = c(20, 17, 15, 1),
col = c("red", "mediumorchid4", "blue", "darkblue"), 
c("Apple", "Dell", "HP", "Other"), title = "Top-Ranked Brand")
dev.off()   

# another way of looking at these data is to employ comparative densities
# for the three selected brands: Apple, Dell, and HP
# using those individual how selected these as the top brand
selected.brands <- c("Apple","Dell","HP")
selected.data <- subset(id.data, subset = (top.brand %in% selected.brands))

# plotting objects for brand.loyalty, price.sensitivity, and feature.focus
# create these three objects and then plot them together on one page
pdf("fig_price_density_three_brands.pdf", width = 8.5, height = 11)  
first.object <- ggplot(selected.data, 
  aes(x = brand.loyalty, fill = top.brand))  +
  labs(x = "Brand Loyalty", 
       y = "f(x)") +
  theme(axis.title.y = element_text(angle = 0, face = "italic", size = 10)) +     
  geom_density(alpha = 0.4) +
  coord_fixed(ratio = 1/15) +
  theme(legend.position = "none") +
  scale_fill_manual(values = c("red","white","blue"), 
    guide = guide_legend(title = NULL)) +
  scale_x_continuous(limits = c(0,1)) +
  scale_y_continuous(limits = c(0,5)) 

second.object <- ggplot(selected.data, 
  aes(x = price.sensitivity, fill = top.brand))  +
  labs(x = "Price Sensitivity", 
       y = "f(x)") +
  theme(axis.title.y = element_text(angle = 0, face = "italic", size = 10)) +      
  geom_density(alpha = 0.4) +
  coord_fixed(ratio = 1/15) +
  theme(legend.position = "none") +
  scale_fill_manual(values = c("red","white","blue"), 
    guide = guide_legend(title = NULL)) +
  scale_x_continuous(limits = c(0,1))  +
  scale_y_continuous(limits = c(0,5))  

third.object <- ggplot(selected.data, 
  aes(x = feature.focus, fill = top.brand))  +
  labs(x = "Feature Focus", 
       y = "f(x)") +
  theme(axis.title.y = element_text(angle = 0, face = "italic", size = 10)) +      
  geom_density(alpha = 0.4) +
  coord_fixed(ratio = 1/15) +
  theme(legend.position = "bottom") +
  scale_fill_manual(values = c("red","white","blue"), 
    guide = guide_legend(title = NULL)) +
  scale_x_continuous(limits = c(0,1)) +
  scale_y_continuous(limits = c(0,5))  

three.part.ggplot.print.with.margins(ggfirstplot.object.name = first.object,
  ggsecondplot.object.name = second.object,
  ggthirdplot.object.name = third.object,
  left.margin.pct=5,right.margin.pct=5,
  top.margin.pct=10,bottom.margin.pct=9,
  first.plot.pct=25,second.plot.pct=25,
  third.plot.pct=31)
dev.off()

# to what extent are consumers open to switching from one brand to another
# can see this trough parallel coordinates plots for the brand part-worths
pdf(file = "fig_price_parallel_coordinates_individuals.pdf", 
  width = 8.5, height = 11)
print(parallelplot(~selected.data[,c("Apple","Compaq","Dell","Gateway",
  "HP","IBM","Sony","Sun")] | top.brand, selected.data, layout = c (3,1)))
dev.off()  
  
# these get a little messy or cluttered...  
# more easily interpreted are parallel coordinate plots of mean part-worths
# for brand part-worth columns and aggregate by top brand (Apple, Dell, or HP)
brands.data <- aggregate(x = selected.data[,2:9], 
  by = selected.data[29], mean)

pdf(file = "fig_price_parallel_coordinates_groups.pdf", 
  width = 8.5, height = 11)
print(parallelplot(~brands.data[,c("Apple","Compaq","Dell","Gateway",
  "HP","IBM","Sony","Sun")] | top.brand, brands.data, layout = c (3,1), 
   lwd = 3, col = "mediumorchid4")) 
dev.off()

# market simulation for hypothetical set of products in the marketplace  
# suppose we work for Apple and we focus upon a market with three 
# competitors: Dell, Gateway, and HP.... we define the products in the
# market using values from the computer choice study just as we did
# in fitting the HB model... we create the simuation input data frame
# and use the previously designed function create.design.matrix
# along with simulation utility functions 

# first product in market is Dell Computer defined as follows:
brand <- "Dell"
compat <- 8  # 100 percent compatibility
perform <- 4 # four times as fast as earlier generation system
reliab <- 2  # Less likely to fail 
learn <- 4  # 16 hours to learn
price <- 4  # $1750
dell.competitor <- 
  data.frame(brand,compat,perform,reliab,learn,price)

# second product in market is Gateway defined as follows:
brand <- "Gateway"
compat <- 6  # 90 percent compatibility
perform <- 2 # twice as fast as earlier generation system
reliab <- 1  # just as likely to fail 
learn <- 2  # 8 hours to learn
price <- 2  # $1250
gateway.competitor <- 
  data.frame(brand,compat,perform,reliab,learn,price)

# third product in market is HP defined as follows:
brand <- "HP"
compat <- 6  # 90 percent compatibility
perform <- 3 # three times as fast as earlier generation system
reliab <- 2  # less likely to fail 
learn <- 2  # 8 hours to learn
price <- 3  # $1500
hp.competitor <- 
  data.frame(brand,compat,perform,reliab,learn,price)

# Apple product has price varying across many choice sets:
brand <- "Apple"
compat <- 5  # 50 percent compatibility
perform <- 4 # four times as fast as earlier generation system
reliab <- 2  # less likely to fail 
learn <- 1  # 4 hours to learn
price <- 1  # $1000 Apple price in first choice set 
apple1000 <- 
  data.frame(brand,compat,perform,reliab,learn,price)

price <- 2  # $1250 Apple price in second choice set
apple1250 <- 
  data.frame(brand,compat,perform,reliab,learn,price)
  
price <- 3  # $1500 Apple price in third choice set
apple1500 <- 
  data.frame(brand,compat,perform,reliab,learn,price)
  
price <- 4  # $1750 Apple price in fourth choice set
apple1750 <- 
  data.frame(brand,compat,perform,reliab,learn,price)
  
price <- 5  # $2000 Apple price in fifth choice set
apple2000 <- 
  data.frame(brand,compat,perform,reliab,learn,price)
  
price <- 6  # $2250 Apple price in sixth choice set
apple2250 <- 
  data.frame(brand,compat,perform,reliab,learn,price)
  
price <- 7  # $2500 Apple price in seventh choice set
apple2500 <- 
  data.frame(brand,compat,perform,reliab,learn,price)

price <- 8  # $2750 Apple price in eighth choice set
apple2750 <- 
  data.frame(brand,compat,perform,reliab,learn,price)

# the competitive products are fixed from one choice set to the next
competition <- rbind(dell.competitor,gateway.competitor,hp.competitor)

# build the simulation choice sets with Apple varying across choice sets
simulation.choice.sets <- 
  rbind(competition, apple1000, competition, apple1250,
  competition, apple1500, competition, apple1750, competition, apple2000, 
  competition, apple2250, competition, apple2500, competition, apple2750)

# add set id to the simuation.choice sets for ease of analysis
setid <- NULL
for(index.for.set in 1:8) setid <- c(setid,rep(index.for.set, times = 4))
simulation.choice.sets <- cbind(setid,simulation.choice.sets)
  
# list the simulation data frame to check it out
cat("\n\n ----- Simulation Choice Sets -----\n\n")
print(simulation.choice.sets)

# create the simulation data frame for all individuals in the study
# by cloning the simulation choice sets for each individual
simulation.data.frame <- NULL  # initialize
list.of.ids <- unique(working.data.frame$id)  # ids from original study
for (index.for.id in seq(along=list.of.ids)) {
  id <- rep(list.of.ids[index.for.id], times = nrow(simulation.choice.sets))
  this.id.data <- cbind(data.frame(id),simulation.choice.sets)
  simulation.data.frame <- rbind(simulation.data.frame, this.id.data)
  }
  
# check structure of simulation data frame
cat("\n\n ----- Structure of Simulation Data frame -----\n\n")
print(str(simulation.data.frame))
print(head(simulation.data.frame))
print(tail(simulation.data.frame))
  
# using create.design.matrix function we evalutate the utility 
# of each product profile in each choice set for each individual 
# in the study... HP part-worths are used for individuals
# this code is similar to that used previously for original data
# from the computer choice study... except now we have simulation data
simulation.choice.utility <- NULL  # initialize utility vector
# work with one row of respondent training data frame at a time
# create choice prediction using the individual part-worths
list.of.ids <- unique(simulation.data.frame$id)
simulation.choice.utility <- NULL  # intitialize
for (index.for.id in seq(along=list.of.ids)) {
  this.id.part.worths <- posterior.mean[index.for.id,] 
  this.id.data.frame <- subset(simulation.data.frame, 
    subset=(id == list.of.ids[index.for.id]))
  for (index.for.profile in 1:nrow(this.id.data.frame)) {   
    simulation.choice.utility <- c(simulation.choice.utility,
      create.design.matrix(this.id.data.frame[index.for.profile,]) %*%
      this.id.part.worths)
    }  
  }  

# use choice.set.predictor function to predict choices in market simulation
simulation.predicted.choice <- 
  choice.set.predictor(simulation.choice.utility)
  
# add simulation predictions to simulation data frame for analysis
# of the results from the market simulation
simulation.analysis.data.frame <- 
  cbind(simulation.data.frame,simulation.predicted.choice)

# contingency table shows results of market simulation  
with(simulation.analysis.data.frame,
  table(setid,brand,simulation.predicted.choice))
  
# summary table of preference shares
YES.data.frame <- subset(simulation.analysis.data.frame, 
  subset = (simulation.predicted.choice == "YES"), select = c("setid","brand"))

# check YES.data.frame to see that it reproduces the information
# from the contingency table 
print(with(YES.data.frame,table(setid,brand)))

# create market share estimates by dividing by number of individuals
# no need for a spreasheet program to work with tables
table.work <- with(YES.data.frame,as.matrix(table(setid,brand)))
table.work <- table.work[,c("Apple","Dell","Gateway","HP")] # order columns
table.work <- round(100 *table.work/length(list.of.ids), digits = 1)  # percent 
Apple.Price <- c(1000,1250,1500,1750,2000,2250,2500,2750)  # new column
table.work <- cbind(Apple.Price,table.work) # add price column to table
cat("\n\n ----- Simulation Results: Preference Share Table -----\n\n")
print(table.work)  # print the market/preference share table

# data visualization of market/preference share estimates from the simulation 
mosaic.data.frame <- YES.data.frame
mosaic.data.frame$setid <- factor(mosaic.data.frame$setid, levels = 1:8,
  labels = c("$1,000","$1,250","$1,500","$1,750",
  "$2,000","$2,250","2,500","$2,750"))

# mosaic plot of joint frequencies from the market simulation
# length/width of the tiles in each row reflects market share
# rows relate to Apple prices... simulation choice sets
pdf(file="fig_price_market_simulation_results.pdf", width = 8.5, height = 11)
  mosaic( ~ setid + brand, data = mosaic.data.frame, 
  highlighting = "brand",
  highlighting_fill = 
    c("mediumorchid4", "green", "blue","red"),
  labeling_args = 
  list(set_varnames = c(brand = "", setid = "Price of Apple Computer"),
  rot_labels = c(left = 90, top = 45),
  pos_labels = c("center","center"),
  just_labels = c("left","center"),
  offset_labels = c(0.0,0.0)))  
dev.off()  

# Conjoint Analysis Spine Chart (R)

# spine chart accommodates up to 45 part-worths on one page
# |part-worth| <= 40 can be plotted directly on the spine chart
# |part-worths| > 40 can be accommodated through standardization

print.digits <- 2  # set number of digits on print and spine chart

# user-defined function for printing conjoint measures
if (print.digits == 2) 
  pretty.print <- function(x) {sprintf("%1.2f",round(x,digits = 2))} 
if (print.digits == 3) 
  pretty.print <- function(x) {sprintf("%1.3f",round(x,digits = 3))} 
 
# --------------------------------------------------
# user-defined function for spine chart
# --------------------------------------------------
spine.chart <- function(conjoint.results,  
  color.for.part.worth.point = "blue",
  color.for.part.worth.line = "blue",
  left.side.symbol.to.print.around.part.worths = "(",
  right.side.symbol.to.print.around.part.worths = ")",
  left.side.symbol.to.print.around.importance = "",
  right.side.symbol.to.print.around.importance = "",
  color.for.printing.importance.text = "dark red",  
  color.for.printing.part.worth.text = "black",  
  draw.gray.background = TRUE,  
  draw.optional.grid.lines = TRUE,  
  print.internal.consistency = TRUE,  
  fix.max.to.4 = FALSE,  
  put.title.on.spine.chart = FALSE,
  title.on.spine.chart = paste("TITLE GOES HERE IF WE ASK FOR ONE",sep=""),
  plot.framing.box = TRUE,  
  do.standardization = TRUE,  
  do.ordered.attributes = TRUE) {

  # fix.max.to.4  option to override the range for part-worth plotting 
   
  if(!do.ordered.attributes) effect.names <- conjoint.results$attributes   
  if(do.ordered.attributes) effect.names <- 
    conjoint.results$ordered.attributes      
   
  number.of.levels.of.attribute <- NULL
  for(index.for.factor in seq(along=effect.names))
    number.of.levels.of.attribute <- c(number.of.levels.of.attribute,
      length(conjoint.results$xlevels[[effect.names[index.for.factor]]]))
    
  # total number of levels needed for vertical length of spine the spine plot
  total.number.of.levels <- sum(number.of.levels.of.attribute)

  # define size of spaces based upon the number of part-worth levels to plot
  if(total.number.of.levels <= 20) {
    smaller.space <- 0.01
    small.space <- 0.02
    medium.space <- 0.03
    large.space <- 0.04
    }

  if(total.number.of.levels > 20) {
    smaller.space <- 0.01 * 0.9
    small.space <- 0.02 * 0.9
    medium.space <- 0.03 * 0.9
    large.space <- 0.04 * 0.9
    }

  if(total.number.of.levels > 22) {
    smaller.space <- 0.01 * 0.85
    small.space <- 0.02 * 0.85
    medium.space <- 0.03 * 0.825
    large.space <- 0.04 * 0.8
    }

  if(total.number.of.levels > 25) {
    smaller.space <- 0.01 * 0.8
    small.space <- 0.02 * 0.8
    medium.space <- 0.03 * 0.75
    large.space <- 0.04 * 0.75
    }

  if(total.number.of.levels > 35) {
    smaller.space <- 0.01 * 0.65
    small.space <- 0.02 * 0.65
    medium.space <- 0.03 * 0.6
    large.space <- 0.04 * 0.6
    }
  
  # of course there is a limit to how much we can plot on one page  
  if (total.number.of.levels > 45) 
    stop("\n\nTERMINATED: More than 45 part-worths on spine chart\n")
 
  if(!do.standardization) 
    part.worth.plotting.list <- conjoint.results$part.worths
 
  if(do.standardization) 
    part.worth.plotting.list <- conjoint.results$standardized.part.worths 
  
  # check the range of part-worths to see which path to go down for plotting
  # initialize these toggles to start
  max.is.less.than.40 <- FALSE 
  max.is.less.than.20 <- FALSE
  max.is.less.than.10 <- FALSE
  max.is.less.than.4 <- FALSE
  max.is.less.than.2 <- FALSE
  max.is.less.than.1 <- FALSE 

  if (max(abs(min(unlist(part.worth.plotting.list),na.rm=TRUE)),
    max(unlist(part.worth.plotting.list),na.rm=TRUE)) <= 40) {
    max.is.less.than.40 <- TRUE 
    max.is.less.than.20 <- FALSE
    max.is.less.than.10 <- FALSE
    max.is.less.than.4 <- FALSE
    max.is.less.than.2 <- FALSE
    max.is.less.than.1 <- FALSE
    }  
  
  if (max(abs(min(unlist(part.worth.plotting.list),na.rm=TRUE)),
    max(unlist(part.worth.plotting.list),na.rm=TRUE)) <= 20) {
    max.is.less.than.40 <- FALSE 
    max.is.less.than.20 <- TRUE
    max.is.less.than.10 <- FALSE
    max.is.less.than.4 <- FALSE
    max.is.less.than.2 <- FALSE
    max.is.less.than.1 <- FALSE
    }

  if(max(abs(min(unlist(part.worth.plotting.list),na.rm=TRUE)),
    max(unlist(part.worth.plotting.list),na.rm=TRUE)) <= 10) {
    max.is.less.than.40 <- FALSE 
    max.is.less.than.20 <- FALSE
    max.is.less.than.10 <- TRUE
    max.is.less.than.4 <- FALSE
    max.is.less.than.2 <- FALSE
    max.is.less.than.1 <- FALSE
    }

  if (max(abs(min(unlist(part.worth.plotting.list),na.rm=TRUE)),
    max(unlist(part.worth.plotting.list),na.rm=TRUE)) <= 4) {
    max.is.less.than.40 <- FALSE 
    max.is.less.than.20 <- FALSE
    max.is.less.than.4 <- TRUE
    max.is.less.than.10 <- FALSE
    max.is.less.than.2 <- FALSE
    max.is.less.than.1 <- FALSE
    }
  
  if(max(abs(min(unlist(part.worth.plotting.list),na.rm=TRUE)),
    max(unlist(part.worth.plotting.list),na.rm=TRUE)) <= 2) {
    max.is.less.than.40 <- FALSE 
    max.is.less.than.20 <- FALSE
    max.is.less.than.4 <- FALSE
    max.is.less.than.10 <- FALSE
    max.is.less.than.2 <- TRUE
    max.is.less.than.1 <- FALSE
    }
 
  if(max(abs(min(unlist(part.worth.plotting.list),na.rm=TRUE)),
    max(unlist(part.worth.plotting.list),na.rm=TRUE)) <= 1) {
    max.is.less.than.40 <- FALSE 
    max.is.less.than.20 <- FALSE
    max.is.less.than.4 <- FALSE
    max.is.less.than.10 <- FALSE
    max.is.less.than.2 <- FALSE
    max.is.less.than.1 <- TRUE
    }

  # sometimes we override the range for part-worth plotting
  # this is not usually done... but it is an option
  if (fix.max.to.4) {
    max.is.less.than.40 <- FALSE 
    max.is.less.than.20 <- FALSE
    max.is.less.than.10 <- FALSE
    max.is.less.than.4 <- TRUE
    max.is.less.than.2 <- FALSE
    max.is.less.than.1 <- FALSE 
    }
  
  if (!max.is.less.than.1 & !max.is.less.than.2 & !max.is.less.than.4 & 
    !max.is.less.than.10 & !max.is.less.than.20 & !max.is.less.than.40) 
      stop("\n\nTERMINATED: Spine chart cannot plot |part-worth| > 40")

  # determine point positions for plotting part-worths on spine chart  
  if (max.is.less.than.1 | max.is.less.than.2 | max.is.less.than.4 | 
    max.is.less.than.10 | max.is.less.than.20 | max.is.less.than.40) {
  # begin if-block plotting when all part-worths in absolute value 
  # are less than one of the tested range values
  # part-worth positions for plottting 
  # end if-block plotting when all part-worths in absolute value 
  # are less than one of the tested range values
  # offsets for plotting vary with the max.is.less.than setting
    if(max.is.less.than.1) {
      list.scaling <- function(x) {0.75 + x/5}
      part.worth.point.position <- 
        lapply(part.worth.plotting.list,list.scaling)
      }

    if(max.is.less.than.2) {
      list.scaling <- function(x) {0.75 + x/10}
      part.worth.point.position <- 
        lapply(part.worth.plotting.list,list.scaling)
      }  
  
    if(max.is.less.than.4) {
      list.scaling <- function(x) {0.75 + x/20}
      part.worth.point.position <- 
        lapply(part.worth.plotting.list,list.scaling)
      }
    
     if(max.is.less.than.10) {
      list.scaling <- function(x) {0.75 + x/50}
      part.worth.point.position <- 
        lapply(part.worth.plotting.list,list.scaling)
      }
    
    if(max.is.less.than.20) {
      list.scaling <- function(x) {0.75 + x/100}
      part.worth.point.position <- 
        lapply(part.worth.plotting.list,list.scaling)
      }
    
    if(max.is.less.than.40) {
      list.scaling <- function(x) {0.75 + x/200}
      part.worth.point.position <- 
        lapply(part.worth.plotting.list,list.scaling)
      }
      
    part.worth.point.position <- lapply(part.worth.plotting.list,list.scaling)
    } 

  if (plot.framing.box) plot(c(0,0,1,1),c(0,1,0,1),xlab="",ylab="",
    type="n",xaxt="n",yaxt="n")

  if (!plot.framing.box) plot(c(0,0,1,1),c(0,1,0,1),xlab="",ylab="",
    type="n",xaxt="n",yaxt="n", bty="n")

  if (put.title.on.spine.chart) {
    text(c(0.50),c(0.975),pos=3,labels=title.on.spine.chart,cex=01.5)
    y.location <- 0.925  # starting position with title
    }

  if (!put.title.on.spine.chart) y.location <- 0.975  # no-title start 
  
  # store top of vertical line for later plotting needs
  y.top.of.vertical.line <- y.location 

  x.center.position <- 0.75  # horizontal position of spine

  # begin primary plotting loop 
  # think of a plot as a collection of text and symbols on screen or paper
  # we are going to construct a plot one text string and symbol at a time
  # (note that we may have to repeat this process at the end of the program)
  for(k in seq(along=effect.names)) { 
    y.location <- y.location - large.space
    text(c(0.4),c(y.location),pos=2,
      labels=paste(effect.name.map(effect.names[k])," ",sep=""),cex=01.0)
    text(c(0.525),c(y.location),pos=2,col=color.for.printing.importance.text,
    labels=paste(" ",left.side.symbol.to.print.around.importance,
    pretty.print(
      unlist(conjoint.results$attribute.importance[effect.names[k]])),"%",
      right.side.symbol.to.print.around.importance,sep=""),cex=01.0)

  # begin loop for printing part-worths
    for(m in seq(1:number.of.levels.of.attribute[k])) { 
      y.location <- y.location - medium.space
      text(c(0.4),c(y.location),pos=2,
      conjoint.results$xlevel[[effect.names[k]]][m],cex=01.0)
   #   part.worth.label.data.frame[k,m],cex=01.0)

      text(c(0.525),c(y.location),pos=2,
      col=color.for.printing.part.worth.text,
      labels=paste(" ",left.side.symbol.to.print.around.part.worths,
      pretty.print(part.worth.plotting.list[[effect.names[k]]][m]),
      right.side.symbol.to.print.around.part.worths,sep=""),cex=01.0)

      points(part.worth.point.position[[effect.names[k]]][m],y.location, 
        type = "p", pch = 20, col = color.for.part.worth.point, cex = 2)
      segments(x.center.position, y.location, 
      part.worth.point.position[[effect.names[k]]][m], y.location,
         col = color.for.part.worth.line, lty = 1, lwd = 2)      
      } 
    } 

  y.location <- y.location - medium.space

  # begin center axis and bottom plotting
  y.bottom.of.vertical.line <- y.location  # store top of vertical line

  below.y.bottom.of.vertical.line <- y.bottom.of.vertical.line - small.space/2

  if (!draw.gray.background) {
  # four optional grid lines may be drawn on the plot parallel to the spine
    if (draw.optional.grid.lines) {
      segments(0.55, y.top.of.vertical.line, 0.55, 
        y.bottom.of.vertical.line, col = "black", lty = "solid", lwd = 1)  

      segments(0.65, y.top.of.vertical.line, 0.65, 
        y.bottom.of.vertical.line, col = "gray", lty = "solid", lwd = 1)  
  
      segments(0.85, y.top.of.vertical.line, 0.85, 
        y.bottom.of.vertical.line, col = "gray", lty = "solid", lwd = 1)    

      segments(0.95, y.top.of.vertical.line, 0.95, 
        y.bottom.of.vertical.line, col = "black", lty = "solid", lwd = 1)  
      }
    }
       
  # gray background for plotting area of the points
  if (draw.gray.background) {
    rect(xleft = 0.55, ybottom = y.bottom.of.vertical.line, 
      xright = 0.95, ytop = y.top.of.vertical.line, density = -1, angle = 45,
      col = "light gray", border = NULL, lty = "solid", lwd = 1)
     
  # four optional grid lines may be drawn on the plot parallel to the spine
    if (draw.optional.grid.lines) {
      segments(0.55, y.top.of.vertical.line, 0.55, 
        y.bottom.of.vertical.line, col = "black", lty = "solid", lwd = 1)  

      segments(0.65, y.top.of.vertical.line, 0.65, 
        y.bottom.of.vertical.line, col = "white", lty = "solid", lwd = 1)  
  
      segments(0.85, y.top.of.vertical.line, 0.85, 
        y.bottom.of.vertical.line, col = "white", lty = "solid", lwd = 1)    

      segments(0.95, y.top.of.vertical.line, 0.95, 
        y.bottom.of.vertical.line, col = "black", lty = "solid", lwd = 1)  
      }     
    }   

  # draw the all-important spine on the plot
  segments(x.center.position, y.top.of.vertical.line, x.center.position, 
    y.bottom.of.vertical.line, col = "black", lty = "dashed", lwd = 1)  
       
  # horizontal line at top           
  segments(0.55, y.top.of.vertical.line, 0.95, y.top.of.vertical.line,
       col = "black", lty = 1, lwd = 1)      

  # horizontal line at bottom       
  segments(0.55, y.bottom.of.vertical.line, 0.95, y.bottom.of.vertical.line,
         col = "black", lty = 1, lwd = 1)          
       
  # plot for ticks and labels 
  segments(0.55, y.bottom.of.vertical.line, 
    0.55, below.y.bottom.of.vertical.line,
    col = "black", lty = 1, lwd = 1)   # tick line at bottom

  segments(0.65, y.bottom.of.vertical.line, 
    0.65, below.y.bottom.of.vertical.line,
    col = "black", lty = 1, lwd = 1)   # tick line at bottom
       
  segments(0.75, y.bottom.of.vertical.line, 
    0.75, below.y.bottom.of.vertical.line,
    col = "black", lty = 1, lwd = 1)   # tick line at bottom      

  segments(0.85, y.bottom.of.vertical.line, 
    0.85, below.y.bottom.of.vertical.line,
    col = "black", lty = 1, lwd = 1)   # tick line at bottom      
       
  segments(0.95, y.bottom.of.vertical.line, 
    0.95, below.y.bottom.of.vertical.line,
    col = "black", lty = 1, lwd = 1)   # tick line at bottom      
              
  # axis labels vary with the max.is.less.than range being used
  if (max.is.less.than.1) text(c(0.55,0.65,0.75,0.85,0.95),
    rep(below.y.bottom.of.vertical.line,times=5),
    pos=1,labels=c("-1","-0.5","0","+0.5","+1"),cex=0.75)

  if (max.is.less.than.2) text(c(0.55,0.65,0.75,0.85,0.95),
    rep(below.y.bottom.of.vertical.line,times=5),
    pos=1,labels=c("-2","-1","0","+1","+2"),cex=0.75)

  if (max.is.less.than.4) text(c(0.55,0.65,0.75,0.85,0.95),
    rep(below.y.bottom.of.vertical.line,times=5),
    pos=1,labels=c("-4","-2","0","+2","+4"),cex=0.75)

  if (max.is.less.than.10) text(c(0.55,0.65,0.75,0.85,0.95),
    rep(below.y.bottom.of.vertical.line,times=5),
    pos=1,labels=c("-10","-5","0","+5","+10"),cex=0.75)

  if (max.is.less.than.20) text(c(0.55,0.65,0.75,0.85,0.95),
    rep(below.y.bottom.of.vertical.line,times=5),
    pos=1,labels=c("-20","-10","0","+10","+20"),cex=0.75)

  if (max.is.less.than.40) text(c(0.55,0.65,0.75,0.85,0.95),
    rep(below.y.bottom.of.vertical.line,times=5),
    pos=1,labels=c("-40","-20","0","+20","+40"),cex=0.75)

  y.location <- below.y.bottom.of.vertical.line - small.space

  if(do.standardization) 
    text(.75,y.location,pos=1,labels=c("Standardized Part-Worth"),cex=0.95)
   
  if(!do.standardization) text(.75,y.location,pos=1,labels=c("Part-Worth"),
    cex=0.95)

  y.location <- below.y.bottom.of.vertical.line - small.space

  if(do.standardization) 
    text(0.75,y.location,pos=1,labels=c("Standardized Part-Worth"),cex=0.95)
   
  if(!do.standardization) text(0.75,y.location,pos=1,labels=c("Part-Worth"),
    cex=0.95) 

  if(print.internal.consistency) {
    y.location <- y.location - medium.space
    text(c(0.525),c(y.location),pos=2,labels=paste("Internal consistency: ",
    pretty.print(conjoint.results$internal.consistency),
    sep=""))
    }

  # if we have grid lines we may have plotted over part-worth points
  # if we have a gray background then we have plotted over part-worth points
  # so let us plot those all-important part-worth points and lines once again
  if(draw.gray.background || draw.optional.grid.lines) {
    y.location <- y.top.of.vertical.line  # retreive the starting value 

  # repeat the primary plotting loop 
  for(k in seq(along=effect.names)) { 
    y.location <- y.location - large.space
    text(c(0.4),c(y.location),pos=2,
      labels=paste(effect.name.map(effect.names[k])," ",sep=""),cex=01.0)
    text(c(0.525),c(y.location),pos=2,col=color.for.printing.importance.text,
      labels=paste(" ",left.side.symbol.to.print.around.importance,
      pretty.print(
      unlist(conjoint.results$attribute.importance[effect.names[k]])),"%",
      right.side.symbol.to.print.around.importance,sep=""),cex=01.0)

 # begin loop for printing part-worths
      for(m in seq(1:number.of.levels.of.attribute[k])) { 
         y.location <- y.location - medium.space
         text(c(0.4),c(y.location),pos=2,
         conjoint.results$xlevel[[effect.names[k]]][m],cex=01.0)

         text(c(0.525),c(y.location),
           pos=2,col=color.for.printing.part.worth.text,
           labels=paste(" ",left.side.symbol.to.print.around.part.worths,
           pretty.print(part.worth.plotting.list[[effect.names[k]]][m]),
           right.side.symbol.to.print.around.part.worths,sep=""),cex=01.0)

      points(part.worth.point.position[[effect.names[k]]][m],y.location, 
         type = "p", pch = 20, col = color.for.part.worth.point, cex = 2)
      segments(x.center.position, y.location, 
      part.worth.point.position[[effect.names[k]]][m], y.location,
         col = color.for.part.worth.line, lty = 1, lwd = 2)      
      } 
    } 
  } 
}

# save spine.chart function for future work
save(spine.chart,file="mtpa_spine_chart.Rdata")

# Market Simulation Utilities (R)

# user-defined function for first-choice simulation rule
first.choice.simulation.rule <- function(response, alpha = 1) {
  # begin function for first-choice rule
  # returns binary vector or response vector with equal division 
  # of 1 across all locations at the maximum
  # use alpha for desired sum across respondents
  # alpha useful when the set of tested profiles is not expected to be one
  if(alpha < 0 || alpha > 1) stop("alpha must be between zero and one")
  response.vector <- numeric(length(response))
  for(k in seq(along=response))
    if(response[k] == max(response)) response.vector[k] <- 1
  alpha*(response.vector/sum(response.vector))  
  }  # end first-choice rule function

# user-defined function for predicted choices from four-profile choice sets 
choice.set.predictor <- function(predicted.probability) {
  predicted.choice <- length(predicted.probability)  # initialize 
  index.fourth <- 0  # initialize block-of-four choice set indices
  while (index.fourth < length(predicted.probability)) {
    index.first  <- index.fourth + 1
    index.second <- index.fourth + 2
    index.third  <- index.fourth + 3
    index.fourth <- index.fourth + 4
    this.choice.set.probability.vector <- 
      c(predicted.probability[index.first],
      predicted.probability[index.second],
      predicted.probability[index.third],
      predicted.probability[index.fourth])
    predicted.choice[index.first:index.fourth] <- 
      first.choice.simulation.rule(this.choice.set.probability.vector)  
    }
  predicted.choice <- factor(predicted.choice, levels = c(0,1), 
    labels = c("NO","YES"))
  predicted.choice  
  } # end choice.set.predictor function 
  
  
# save market simulation utilities for future work
save(first.choice.simulation.rule,
  choice.set.predictor,
  file="mtpa_market_simulation_utilities.Rdata")

# Split-Plotting Utilities with grid Graphics (R)

library(grid)  # grid graphics foundation of split-plotting utilities

# functions used with ggplot2 graphics to split the plotting region
# to set margins and to plot more than one ggplot object on one page/screen
vplayout <- function(x, y) 
viewport(layout.pos.row=x, layout.pos.col=y) 

# grid graphics utility plots one plot with margins
ggplot.print.with.margins <- function(ggplot.object.name,left.margin.pct=10,
  right.margin.pct=10,top.margin.pct=10,bottom.margin.pct=10)
{ # begin function for printing ggplot objects with margins
  # margins expressed as percentages of total... use integers
 grid.newpage() 
pushViewport(viewport(layout=grid.layout(100,100)))
print(ggplot.object.name, 
  vp=vplayout((0 + top.margin.pct):(100 - bottom.margin.pct),
  (0 + left.margin.pct):(100 - right.margin.pct))) 
} # end function for printing ggplot objects with margins

# grid graphics utility plots two ggplot plotting objects in one column
special.top.bottom.ggplot.print.with.margins <- 
  function(ggplot.object.name,ggplot.text.tagging.object.name,
  left.margin.pct=5,right.margin.pct=5,top.margin.pct=5,
  bottom.margin.pct=5,plot.pct=80,text.tagging.pct=10) { 
# begin function for printing ggplot objects with margins 
# and text tagging at bottom of plot
# margins expressed as percentages of total... use integers
  if((top.margin.pct + bottom.margin.pct + plot.pct + text.tagging.pct) != 100) 
    stop(paste("function special.top.bottom.ggplot.print.with.margins()",
    "execution terminated:\n   top.margin.pct + bottom.margin.pct + ",
    "plot.pct + text.tagging.pct not equal to 100 percent",sep=""))  
  grid.newpage() 
  pushViewport(viewport(layout=grid.layout(100,100)))
  print(ggplot.object.name, 
  vp=vplayout((0 + top.margin.pct):
    (100 - (bottom.margin.pct + text.tagging.pct)),
  (0 + left.margin.pct):(100 - right.margin.pct))) 

  print(ggplot.text.tagging.object.name, 
    vp=vplayout((0 + (top.margin.pct + plot.pct)):(100 - bottom.margin.pct),
    (0 + left.margin.pct):(100 - right.margin.pct))) 
} # end function for printing ggplot objects with margins and text tagging

# grid graphics utility plots three ggplot plotting objects in one column
three.part.ggplot.print.with.margins <- function(ggfirstplot.object.name,
ggsecondplot.object.name,
ggthirdplot.object.name,
left.margin.pct=5,right.margin.pct=5,
top.margin.pct=10,bottom.margin.pct=10,
first.plot.pct=25,second.plot.pct=25,
third.plot.pct=30) { 
# function for printing ggplot objects with margins and top and bottom plots
# margins expressed as percentages of total... use integers
if((top.margin.pct + bottom.margin.pct + first.plot.pct + 
  second.plot.pct  + third.plot.pct) != 100) 
    stop(paste("function special.top.bottom.ggplot.print.with.margins()",
         "execution terminated:\n   top.margin.pct + bottom.margin.pct",
         "+ first.plot.pct + second.plot.pct  + third.plot.pct not equal",
         "to 100 percent",sep=""))  
grid.newpage() 
pushViewport(viewport(layout=grid.layout(100,100)))

print(ggfirstplot.object.name, vp=vplayout((0 + top.margin.pct):
  (100 - (second.plot.pct  + third.plot.pct + bottom.margin.pct)),
  (0 + left.margin.pct):(100 - right.margin.pct))) 

print(ggsecondplot.object.name, 
  vp=vplayout((0 + top.margin.pct + first.plot.pct):
  (100 - (third.plot.pct + bottom.margin.pct)),
  (0 + left.margin.pct):(100 - right.margin.pct))) 

print(ggthirdplot.object.name, 
  vp=vplayout((0 + top.margin.pct + first.plot.pct + second.plot.pct):
  (100 - (bottom.margin.pct)),(0 + left.margin.pct):
  (100 - right.margin.pct))) 
} 

# grid graphics utility plots two ggplot plotting objects in one row
# primary plot graph at left... legend at right
special.left.right.ggplot.print.with.margins <- 
  function(ggplot.object.name, ggplot.text.legend.object.name,
  left.margin.pct=5, right.margin.pct=5, top.margin.pct=5,
  bottom.margin.pct=5, plot.pct=85, text.legend.pct=5) { 
# begin function for printing ggplot objects with margins 
# and text legend at bottom of plot
# margins expressed as percentages of total... use integers
  if((left.margin.pct + right.margin.pct + plot.pct + text.legend.pct) != 100) 
    stop(paste("function special.left.right.ggplot.print.with.margins()",
    "execution terminated:\n   left.margin.pct + right.margin.pct + ",
    "plot.pct + text.legend.pct not equal to 100 percent",sep=""))  
  grid.newpage() 
  pushViewport(viewport(layout=grid.layout(100,100)))
  print(ggplot.object.name, 
  vp=vplayout((0 + top.margin.pct):(100 - (bottom.margin.pct)),
  (0 + left.margin.pct + text.legend.pct):(100 - right.margin.pct))) 

  print(ggplot.text.legend.object.name, 
    vp=vplayout((0 + (top.margin.pct)):(100 - bottom.margin.pct),
    (0 + left.margin.pct + plot.pct):(100 - right.margin.pct))) 
} # end function for printing ggplot objects with margins and text legend
 
# save split-plotting utilities for future work
save(vplayout,
  ggplot.print.with.margins,
  special.top.bottom.ggplot.print.with.margins,
  three.part.ggplot.print.with.margins,
  special.left.right.ggplot.print.with.margins,
  file="mtpa_split_plotting_utilities.Rdata")

# Wait-Time Ribbon Plot (R)

wait.time.ribbon <- function(wait.service.data, title = "", 
  wait.time.goal = 30, wait.time.max = 90, 
  plotting.min = 0, plotting.max = 250,
  use.text.tagging = TRUE) {
  
#  requires ggplot2 package
#  data visualization for operations management
#  wait.service.data is input data frame with the named columns as follows: 
#    hour: integer hour of the day on 24-hour clock
#    wait: integer call wait time in seconds
#    service:  integer call service time in seconds (NA for no service)
#    server:  character string for server name or code
#             assumes that there is a distict character string for no server
#             this string is coded as NO_SERVER
#  wait.time.goal:  desired maximum wait time (30 seconds default)
#                   represented as bottom of yellow region
#  wait.time.max: when wait time becomes intolerable (90 seconds default)
#                 represented as top of yellow region
# use.text.tagging default is TRUE for added text at bottom of plot

# set constants for ribbon plotting
MIN.SAMPLE <- 5  # min sample size for hourly calcuations
PERCENTILE.MIN <- 0.50  # used for bottom of acceptable wait time
PERCENTILE.MAX <- 0.90  # used for bottom of acceptable wait time

add_footnote_at_bottom_of_ribbon_plot <- TRUE
percentile.footnote <- paste("Bottom of ribbon = ",
  100*PERCENTILE.MIN, "th percentile of wait times",
  "    Top of ribbon = ", 100*PERCENTILE.MAX, "th percentile of wait times.", 
  sep = "")

x.hour <- seq(from=0,to=23) # for horixontal axis scale

# code for ribbon region counts
calls.per.hour <- numeric(24)  # total calls initialized as zero
served.calls <- numeric(24)  # served calls initialized as zero
dropped.calls <- numeric(24)  # dropped/abandoned calls initialize as zero
ymin.percentile <- rep(NA,times=24)  # store for minimum percentile values
ymax.percentile <- rep(NA,times=24)  # store maximum percentile values

# compute number of calls per hour
# code more versatile than table command
# to accommodate hours with no calls

for(index.for.hour in 1:24) { 
# begin for-loop for wait-time data call counts and percentile calculations

# 24-hour clock has first hour coded as zero in input data file
  coded.index.for.hour <- index.for.hour - 1  
  temporary.vector <- na.omit(wait.service.data$hour)
  calls.per.hour[index.for.hour] <- 
    sum(ifelse(temporary.vector==coded.index.for.hour,1,0))

    if(calls.per.hour[index.for.hour] >= MIN.SAMPLE) { 
# begin if-block for computing ymin and ymax values and number of servers
# when there are at least MIN.SAMPLE calls in the hour
    this.hour.wait.service.data <- 
      wait.service.data[(wait.service.data$hour == coded.index.for.hour),]

    ymin.percentile[index.for.hour] <- 
      quantile(this.hour.wait.service.data$wait,
      probs=c(PERCENTILE.MIN),na.rm = TRUE,names=FALSE,type=8)

    ymax.percentile[index.for.hour] <- 
      quantile(this.hour.wait.service.data$wait,
      probs=c(PERCENTILE.MAX),na.rm = TRUE,names=FALSE,type=8)      
    } # end if-block for computing ymin and ymax values 

# if insufficient data we set min and max to be wait.time.goal
  if(calls.per.hour[index.for.hour] < MIN.SAMPLE) {
    ymin.percentile[index.for.hour] <- wait.time.goal
    ymax.percentile[index.for.hour] <- wait.time.goal
    }
  } # end for-loop for wait-time data call counts and percentile calculations  

# compute number.of.servers data and served and dropped calls
number.of.servers <- numeric(24)  # initialize to zero
for(index.for.hour in 1:24) { 
# begin for-loop for obtaining server data for the ribbon plot
# 24-hour clock has first hour coded as zero in input data file
  coded.index.for.hour <- index.for.hour - 1  
  temporary.vector <- na.omit(wait.service.data$hour)
  calls.per.hour[index.for.hour] <- 
    sum(ifelse(temporary.vector==coded.index.for.hour,1,0))
  this.hour.wait.service.data <- 
      wait.service.data[(wait.service.data$hour == coded.index.for.hour),]   
      
  served.calls[index.for.hour] <- 
    nrow(subset(this.hour.wait.service.data, subset=(server != "NO_SERVER")))   
  dropped.calls[index.for.hour] <- 
    nrow(subset(this.hour.wait.service.data, subset=(server == "NO_SERVER")))    
      
  if (nrow(this.hour.wait.service.data) > 0) {
# count is based upon the number of unique server names less NO_SERVER
    servers <- 
      na.omit((unique(this.hour.wait.service.data$server)))
    valid.servers <- setdiff(servers, "NO_SERVER")
    number.of.servers[index.for.hour] <- length(valid.servers)
    }
  } # end for-loop for obtaining server data for the ribbon plot
  
       
greenmin <- rep(plotting.min, length=24)
greenmax <- rep(wait.time.goal, length=24)

yellowmin <- rep(wait.time.goal, length=24)
yellowmax <- rep(wait.time.max, length=24)

redmin <- rep(wait.time.max, length=24)
redmax <- rep(plotting.max, length=24)

ymax.topwhite <- rep(plotting.max,length=24)
ymin.topwhite <- ymax.percentile

ymax.bottomwhite <- ymin.percentile
ymin.bottomwhite <- rep(plotting.min,length=24)

# define data frame for plotting wait and service information for this day  
call.center.plotting.frame <- 
  data.frame(x.hour, ymin.percentile, ymax.percentile, 
    calls.per.hour, number.of.servers,
    greenmin,greenmax,
    yellowmin,yellowmax,
    redmin,redmax,
    ymin.bottomwhite,ymax.bottomwhite,
    ymin.topwhite,ymax.topwhite)  

#cat("\n\n","------------- ",title," -------------","\n")
#print(call.center.plotting.frame)

ggobject <- ggplot() + 
geom_ribbon(data=call.center.plotting.frame, 
mapping=aes(x=x.hour, ymin=greenmin, ymax=greenmax), 
stat="identity",colour="white",fill="darkgreen") + 
geom_ribbon(data=call.center.plotting.frame, 
mapping=aes(x=x.hour, ymin=yellowmin, ymax=yellowmax), 
stat="identity",colour="white",fill="yellow") + 
geom_ribbon(data=call.center.plotting.frame, 
mapping=aes(x=x.hour, ymin=redmin, ymax=redmax), 
stat="identity",colour="white",fill="red") + 
geom_ribbon(data=call.center.plotting.frame, 
mapping=aes(x=x.hour, ymin=ymin.topwhite, ymax=ymax.topwhite), 
stat="identity",colour="white",fill="white") + 
geom_ribbon(data=call.center.plotting.frame, 
mapping=aes(x=x.hour, ymin=ymin.bottomwhite, ymax=ymax.bottomwhite), 
stat="identity",colour="white",fill="white") + 
geom_hline(data=call.center.plotting.frame, 
mapping=aes(yintercept=yellowmin[1])) + 
geom_hline(data=call.center.plotting.frame, 
mapping=aes(yintercept=redmin[1])) + 
labs(title = title) + theme_bw(base_size = 12) + 
scale_y_continuous(limits = c(greenmin[1], redmax[1])) + 
xlab("Hour of Day (24-Hour Clock)") + 
ylab("Wait Time (Seconds)") 

# plotting with all default margins no text at bottom
if(!use.text.tagging) ggplot.print.with.margins(ggobject) 

# plotting with text tagging requires the creation of a ggplot text object
if(use.text.tagging) 
{

# define character data for the text taggging at bottom of plot
hour.title <- "Hour:"
hour.00 <- "00"
hour.01 <- "01"
hour.02 <- "02"
hour.03 <- "03"
hour.04 <- "04"
hour.05 <- "05"
hour.06 <- "06"
hour.07 <- "07"
hour.08 <- "08"
hour.09 <- "09"
hour.10 <- "10"
hour.11 <- "11"
hour.12 <- "12"
hour.13 <- "13"
hour.14 <- "14"
hour.15 <- "15"
hour.16 <- "16"
hour.17 <- "17"
hour.18 <- "18"
hour.19 <- "19"
hour.20 <- "20"
hour.21 <- "21"
hour.22 <- "22"
hour.23 <- "23"

calls.title <- "Calls:"  
calls.00 <- as.character(calls.per.hour[1])
calls.01 <- as.character(calls.per.hour[2])
calls.02 <- as.character(calls.per.hour[3])
calls.03 <- as.character(calls.per.hour[4])
calls.04 <- as.character(calls.per.hour[5])
calls.05 <- as.character(calls.per.hour[6])
calls.06 <- as.character(calls.per.hour[7])
calls.07 <- as.character(calls.per.hour[8])
calls.08 <- as.character(calls.per.hour[9])
calls.09 <- as.character(calls.per.hour[10])
calls.10 <- as.character(calls.per.hour[11])
calls.11 <- as.character(calls.per.hour[12])
calls.12 <- as.character(calls.per.hour[13])
calls.13 <- as.character(calls.per.hour[14])
calls.14 <- as.character(calls.per.hour[15])
calls.15 <- as.character(calls.per.hour[16])
calls.16 <- as.character(calls.per.hour[17])
calls.17 <- as.character(calls.per.hour[18])
calls.18 <- as.character(calls.per.hour[19])
calls.19 <- as.character(calls.per.hour[20])
calls.20 <- as.character(calls.per.hour[21])
calls.21 <- as.character(calls.per.hour[22])
calls.22 <- as.character(calls.per.hour[23])
calls.23 <- as.character(calls.per.hour[24])

servers.title <- "Servers:" 
servers.00 <- as.character(number.of.servers[1])
servers.01 <- as.character(number.of.servers[2])
servers.02 <- as.character(number.of.servers[3])
servers.03 <- as.character(number.of.servers[4])
servers.04 <- as.character(number.of.servers[5])
servers.05 <- as.character(number.of.servers[6])
servers.06 <- as.character(number.of.servers[7])
servers.07 <- as.character(number.of.servers[8])
servers.08 <- as.character(number.of.servers[9])
servers.09 <- as.character(number.of.servers[10])
servers.10 <- as.character(number.of.servers[11])
servers.11 <- as.character(number.of.servers[12])
servers.12 <- as.character(number.of.servers[13])
servers.13 <- as.character(number.of.servers[14])
servers.14 <- as.character(number.of.servers[15])
servers.15 <- as.character(number.of.servers[16])
servers.16 <- as.character(number.of.servers[17])
servers.17 <- as.character(number.of.servers[18])
servers.18 <- as.character(number.of.servers[19])
servers.19 <- as.character(number.of.servers[20])
servers.20 <- as.character(number.of.servers[21])
servers.21 <- as.character(number.of.servers[22])
servers.22 <- as.character(number.of.servers[23])
servers.23 <- as.character(number.of.servers[24])

served.title <- "Served:" 
served.00 <- as.character(served.calls[1])
served.01 <- as.character(served.calls[2])
served.02 <- as.character(served.calls[3])
served.03 <- as.character(served.calls[4])
served.04 <- as.character(served.calls[5])
served.05 <- as.character(served.calls[6])
served.06 <- as.character(served.calls[7])
served.07 <- as.character(served.calls[8])
served.08 <- as.character(served.calls[9])
served.09 <- as.character(served.calls[10])
served.10 <- as.character(served.calls[11])
served.11 <- as.character(served.calls[12])
served.12 <- as.character(served.calls[13])
served.13 <- as.character(served.calls[14])
served.14 <- as.character(served.calls[15])
served.15 <- as.character(served.calls[16])
served.16 <- as.character(served.calls[17])
served.17 <- as.character(served.calls[18])
served.18 <- as.character(served.calls[19])
served.19 <- as.character(served.calls[20])
served.20 <- as.character(served.calls[21])
served.21 <- as.character(served.calls[22])
served.22 <- as.character(served.calls[23])
served.23 <- as.character(served.calls[24])

dropped.title <- "Dropped:" 
dropped.00 <- as.character(dropped.calls[1])
dropped.01 <- as.character(dropped.calls[2])
dropped.02 <- as.character(dropped.calls[3])
dropped.03 <- as.character(dropped.calls[4])
dropped.04 <- as.character(dropped.calls[5])
dropped.05 <- as.character(dropped.calls[6])
dropped.06 <- as.character(dropped.calls[7])
dropped.07 <- as.character(dropped.calls[8])
dropped.08 <- as.character(dropped.calls[9])
dropped.09 <- as.character(dropped.calls[10])
dropped.10 <- as.character(dropped.calls[11])
dropped.11 <- as.character(dropped.calls[12])
dropped.12 <- as.character(dropped.calls[13])
dropped.13 <- as.character(dropped.calls[14])
dropped.14 <- as.character(dropped.calls[15])
dropped.15 <- as.character(dropped.calls[16])
dropped.16 <- as.character(dropped.calls[17])
dropped.17 <- as.character(dropped.calls[18])
dropped.18 <- as.character(dropped.calls[19])
dropped.19 <- as.character(dropped.calls[20])
dropped.20 <- as.character(dropped.calls[21])
dropped.21 <- as.character(dropped.calls[22])
dropped.22 <- as.character(dropped.calls[23])
dropped.23 <- as.character(dropped.calls[24])

# set up spacing and positioning for the table
y.current.level <- 1.0  # initialze position
y.large.space <- 0.175
y.medium.space <- 0.125
y.small.space <- 0.075

table.left.margin <- 0.1  # needed for row labels at left
horizontal.offset <- (1-table.left.margin)/24  # spacing in the text table

y.current.level <- y.current.level - y.medium.space

ggtextobject <- ggplot(data=data.frame(x = 0.5,y = y.current.level),
  aes(x=x,y=y,xmin=0,xmax=1,ymin=0,ymax=1), 
  stat="identity", position="identity") + labs(x=NULL,y=NULL) +  
geom_text(x = 0.025,y = y.current.level,label = hour.title,
size = 4.25,colour="black")  + 
geom_text(x = (00*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.00, size = 4.25,colour="black") + 
geom_text(x = (01*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.01, size = 4.25,colour="black") + 
geom_text(x = (02*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.02, size = 4.25,colour="black") + 
geom_text(x = (03*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.03, size = 4.25,colour="black") + 
geom_text(x = (04*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.04, size = 4.25,colour="black") + 
geom_text(x = (05*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.05, size = 4.25,colour="black") + 
geom_text(x = (06*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.06, size = 4.25,colour="black") + 
geom_text(x = (07*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.07, size = 4.25,colour="black") + 
geom_text(x = (08*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.08, size = 4.25,colour="black") + 
geom_text(x = (09*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.09, size = 4.25,colour="black") + 
geom_text(x = (10*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.10, size = 4.25,colour="black") + 
geom_text(x = (11*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.11, size = 4.25,colour="black") + 
geom_text(x = (12*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.12, size = 4.25,colour="black") + 
geom_text(x = (13*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.13, size = 4.25,colour="black") + 
geom_text(x = (14*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.14, size = 4.25,colour="black") + 
geom_text(x = (15*horizontal.offset + table.left.margin),
y= y.current.level,label = hour.15, size = 4.25,colour="black") + 
geom_text(x = (16*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.16, size = 4.25,colour="black") + 
geom_text(x = (17*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.17, size = 4.25,colour="black") + 
geom_text(x = (18*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.18, size = 4.25,colour="black") + 
geom_text(x = (19*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.19, size = 4.25,colour="black") + 
geom_text(x = (20*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.20, size = 4.25,colour="black") + 
geom_text(x = (21*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.21, size = 4.25,colour="black") + 
geom_text(x = (22*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.22, size = 4.25,colour="black") + 
geom_text(x = (23*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.23, size = 4.25,colour="black")   


y.current.level <- y.current.level - y.medium.space

ggtextobject <- ggtextobject + geom_text(x = 0.025,
y = y.current.level, label = servers.title,size = 4.25,colour="black") + 
geom_text(x = (00*horizontal.offset + table.left.margin), 
y = y.current.level, label = servers.00,size = 4.25,colour="black") + 
geom_text(x = (01*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.01,size = 4.25,colour="black") + 
geom_text(x = (02*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.02,size = 4.25,colour="black") + 
geom_text(x = (03*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.03,size = 4.25,colour="black") + 
geom_text(x = (04*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.04,size = 4.25,colour="black") + 
geom_text(x = (05*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.05,size = 4.25,colour="black") + 
geom_text(x = (06*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.06,size = 4.25,colour="black") + 
geom_text(x = (07*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.07,size = 4.25,colour="black") + 
geom_text(x = (08*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.08,size = 4.25,colour="black") + 
geom_text(x = (09*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.09,size = 4.25,colour="black") + 
geom_text(x = (10*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.10,size = 4.25,colour="black") + 
geom_text(x = (11*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.11,size = 4.25,colour="black") + 
geom_text(x = (12*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.12,size = 4.25,colour="black") + 
geom_text(x = (13*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.13,size = 4.25,colour="black") + 
geom_text(x = (14*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.14,size = 4.25,colour="black") + 
geom_text(x = (15*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.15,size = 4.25,colour="black") + 
geom_text(x = (16*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.16,size = 4.25,colour="black") + 
geom_text(x = (17*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.17,size = 4.25,colour="black") + 
geom_text(x = (18*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.18,size = 4.25,colour="black") + 
geom_text(x = (19*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.19,size = 4.25,colour="black") + 
geom_text(x = (20*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.20,size = 4.25,colour="black") + 
geom_text(x = (21*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.21,size = 4.25,colour="black") + 
geom_text(x = (22*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.22,size = 4.25,colour="black") + 
geom_text(x = (23*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.23,size = 4.25,colour="black")  

# store line position for bottom of text segment of the visualization

y.level.divider.line <- y.current.level - y.medium.space
# temporary data frame needed to input to geom_hline later
middle.line.data <- data.frame(y.level.divider.line) 

y.current.level <- y.level.divider.line - y.medium.space

ggtextobject <- ggtextobject +  
geom_text(x = 0.025,y = y.current.level,
label = calls.title, size = 4.25,colour="black") + 
geom_text(x = (00*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.00, size = 4.25,colour="black") + 
geom_text(x = (01*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.01, size = 4.25,colour="black") + 
geom_text(x = (02*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.02, size = 4.25,colour="black") + 
geom_text(x = (03*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.03, size = 4.25,colour="black") + 
geom_text(x = (04*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.04, size = 4.25,colour="black") + 
geom_text(x = (05*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.05, size = 4.25,colour="black") + 
geom_text(x = (06*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.06, size = 4.25,colour="black") + 
geom_text(x = (07*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.07, size = 4.25,colour="black") + 
geom_text(x = (08*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.08, size = 4.25,colour="black") + 
geom_text(x = (09*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.09, size = 4.25,colour="black") + 
geom_text(x = (10*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.10, size = 4.25,colour="black") + 
geom_text(x = (11*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.11, size = 4.25,colour="black") + 
geom_text(x = (12*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.12, size = 4.25,colour="black") + 
geom_text(x = (13*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.13, size = 4.25,colour="black") + 
geom_text(x = (14*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.14, size = 4.25,colour="black") + 
geom_text(x = (15*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.15, size = 4.25,colour="black") + 
geom_text(x = (16*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.16, size = 4.25,colour="black") + 
geom_text(x = (17*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.17, size = 4.25,colour="black") + 
geom_text(x = (18*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.18, size = 4.25,colour="black") + 
geom_text(x = (19*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.19, size = 4.25,colour="black") + 
geom_text(x = (20*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.20, size = 4.25,colour="black") + 
geom_text(x = (21*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.21, size = 4.25,colour="black") + 
geom_text(x = (22*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.22, size = 4.25,colour="black") + 
geom_text(x = (23*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.23, size = 4.25,colour="black")   

y.current.level <- y.current.level - y.medium.space

ggtextobject <- ggtextobject +  
geom_text(x = 0.025,y = y.current.level,
label = served.title, size = 4.25,colour="black") + 
geom_text(x = (00*horizontal.offset + table.left.margin),
y = y.current.level, label = served.00, size = 4.25,colour="black") + 
geom_text(x = (01*horizontal.offset + table.left.margin),
y = y.current.level, label = served.01, size = 4.25,colour="black") + 
geom_text(x = (02*horizontal.offset + table.left.margin),
y = y.current.level, label = served.02, size = 4.25,colour="black") + 
geom_text(x = (03*horizontal.offset + table.left.margin),
y = y.current.level, label = served.03, size = 4.25,colour="black") + 
geom_text(x = (04*horizontal.offset + table.left.margin),
y = y.current.level, label = served.04, size = 4.25,colour="black") + 
geom_text(x = (05*horizontal.offset + table.left.margin),
y = y.current.level, label = served.05, size = 4.25,colour="black") + 
geom_text(x = (06*horizontal.offset + table.left.margin),
y = y.current.level, label = served.06, size = 4.25,colour="black") + 
geom_text(x = (07*horizontal.offset + table.left.margin),
y = y.current.level, label = served.07, size = 4.25,colour="black") + 
geom_text(x = (08*horizontal.offset + table.left.margin),
y = y.current.level, label = served.08, size = 4.25,colour="black") + 
geom_text(x = (09*horizontal.offset + table.left.margin),
y = y.current.level, label = served.09, size = 4.25,colour="black") + 
geom_text(x = (10*horizontal.offset + table.left.margin),
y = y.current.level, label = served.10, size = 4.25,colour="black") + 
geom_text(x = (11*horizontal.offset + table.left.margin),
y = y.current.level, label = served.11, size = 4.25,colour="black") + 
geom_text(x = (12*horizontal.offset + table.left.margin),
y = y.current.level, label = served.12, size = 4.25,colour="black") + 
geom_text(x = (13*horizontal.offset + table.left.margin),
y = y.current.level, label = served.13, size = 4.25,colour="black") + 
geom_text(x = (14*horizontal.offset + table.left.margin),
y = y.current.level, label = served.14, size = 4.25,colour="black") + 
geom_text(x = (15*horizontal.offset + table.left.margin),
y = y.current.level, label = served.15, size = 4.25,colour="black") + 
geom_text(x = (16*horizontal.offset + table.left.margin),
y = y.current.level, label = served.16, size = 4.25,colour="black") + 
geom_text(x = (17*horizontal.offset + table.left.margin),
y = y.current.level, label = served.17, size = 4.25,colour="black") + 
geom_text(x = (18*horizontal.offset + table.left.margin),
y = y.current.level, label = served.18, size = 4.25,colour="black") + 
geom_text(x = (19*horizontal.offset + table.left.margin),
y = y.current.level, label = served.19, size = 4.25,colour="black") + 
geom_text(x = (20*horizontal.offset + table.left.margin),
y = y.current.level, label = served.20, size = 4.25,colour="black") + 
geom_text(x = (21*horizontal.offset + table.left.margin),
y = y.current.level, label = served.21, size = 4.25,colour="black") + 
geom_text(x = (22*horizontal.offset + table.left.margin),
y = y.current.level, label = served.22, size = 4.25,colour="black") + 
geom_text(x = (23*horizontal.offset + table.left.margin),
y = y.current.level, label = served.23, size = 4.25,colour="black")   

y.current.level <- y.current.level - y.medium.space

ggtextobject <- ggtextobject +  
geom_text(x = 0.025,y = y.current.level,
label = dropped.title, size = 4.25,colour="black") + 
geom_text(x = (00*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.00, size = 4.25,colour="black") + 
geom_text(x = (01*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.01, size = 4.25,colour="black") + 
geom_text(x = (02*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.02, size = 4.25,colour="black") + 
geom_text(x = (03*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.03, size = 4.25,colour="black") + 
geom_text(x = (04*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.04, size = 4.25,colour="black") + 
geom_text(x = (05*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.05, size = 4.25,colour="black") + 
geom_text(x = (06*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.06, size = 4.25,colour="black") + 
geom_text(x = (07*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.07, size = 4.25,colour="black") + 
geom_text(x = (08*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.08, size = 4.25,colour="black") + 
geom_text(x = (09*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.09, size = 4.25,colour="black") + 
geom_text(x = (10*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.10, size = 4.25,colour="black") + 
geom_text(x = (11*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.11, size = 4.25,colour="black") + 
geom_text(x = (12*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.12, size = 4.25,colour="black") + 
geom_text(x = (13*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.13, size = 4.25,colour="black") + 
geom_text(x = (14*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.14, size = 4.25,colour="black") + 
geom_text(x = (15*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.15, size = 4.25,colour="black") + 
geom_text(x = (16*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.16, size = 4.25,colour="black") + 
geom_text(x = (17*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.17, size = 4.25,colour="black") + 
geom_text(x = (18*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.18, size = 4.25,colour="black") + 
geom_text(x = (19*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.19, size = 4.25,colour="black") + 
geom_text(x = (20*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.20, size = 4.25,colour="black") + 
geom_text(x = (21*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.21, size = 4.25,colour="black") + 
geom_text(x = (22*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.22, size = 4.25,colour="black") + 
geom_text(x = (23*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.23, size = 4.25,colour="black")  

y.level.divider.line <- y.current.level - y.medium.space
# temporary data frame needed to input to geom_hline later
bottom.line.data <- data.frame(y.level.divider.line) 

y.current.level <- y.level.divider.line - y.medium.space

# add footnote centered at bottom of plot if requested
if (add_footnote_at_bottom_of_ribbon_plot)
  ggtextobject <- ggtextobject +  
  geom_text(x = 0.5,y = y.current.level,
  label = percentile.footnote, size = 4.25, colour="black") 

# finish up the plot with background definition and divider lines
ggtextobject <- ggtextobject + geom_hline(aes(yintercept=1)) + 
geom_hline(data=bottom.line.data, 
  mapping = aes(yintercept = y.level.divider.line)) + 
geom_hline(data=middle.line.data, 
  mapping = aes(yintercept = y.level.divider.line)) + 
theme(legend.position = "none")  + 
theme(panel.grid.minor = element_blank()) + 
theme(panel.grid.major = element_blank())  + 
theme(panel.background = element_blank()) + 
theme(axis.ticks = element_blank()) + 
scale_y_continuous(breaks=c(0,1),label=c("","")) + 
scale_x_continuous(breaks=c(0,1),label=c("",""))

# user-defined function plots with text annotation/tagging at the bottom
special.top.bottom.ggplot.print.with.margins(ggobject,ggtextobject,
plot.pct=55,text.tagging.pct=35) 
}

} # end of wait-time ribbon function

# save wait-time ribbon utility for future work
save(wait.time.ribbon,
  file="mtpa_wait_time_ribbon_utility.Rdata")

# Text Scoring Script for Sentiment Analysis (R)
# --------------------------------------
# Word/item analysis method 
# --------------------------------------
# return to the training corpus to develop simple counts
# for each of the words in the sentiment list
# these new variables will be given the names of the words
# to keep things simple.... there are 50 such variables/words
# and we work with a corpus called working.corpus

# and the number of words that match each word
amazing <- integer(length(names(working.corpus)))
beautiful <- integer(length(names(working.corpus)))
classic <- integer(length(names(working.corpus)))
enjoy <- integer(length(names(working.corpus)))       
enjoyed <- integer(length(names(working.corpus)))
entertaining <- integer(length(names(working.corpus)))
excellent <- integer(length(names(working.corpus)))
fans <- integer(length(names(working.corpus)))        
favorite <- integer(length(names(working.corpus)))
fine <- integer(length(names(working.corpus)))
fun <- integer(length(names(working.corpus)))
humor <- integer(length(names(working.corpus)))       
lead <- integer(length(names(working.corpus)))
liked <- integer(length(names(working.corpus)))
love <- integer(length(names(working.corpus)))
loved <- integer(length(names(working.corpus)))       
modern <- integer(length(names(working.corpus)))
nice <- integer(length(names(working.corpus)))
perfect <- integer(length(names(working.corpus)))
pretty <- integer(length(names(working.corpus)))      
recommend <- integer(length(names(working.corpus)))
strong <- integer(length(names(working.corpus)))
top <- integer(length(names(working.corpus)))
wonderful <- integer(length(names(working.corpus)))   
worth <- integer(length(names(working.corpus)))       

bad <- integer(length(names(working.corpus)))       
boring <- integer(length(names(working.corpus)))    
cheap <- integer(length(names(working.corpus)))     
creepy <- integer(length(names(working.corpus)))    
dark <- integer(length(names(working.corpus)))      
dead <- integer(length(names(working.corpus)))     
death <- integer(length(names(working.corpus)))     
evil <- integer(length(names(working.corpus)))      
hard <- integer(length(names(working.corpus)))      
kill <- integer(length(names(working.corpus)))      
killed <- integer(length(names(working.corpus)))    
lack <- integer(length(names(working.corpus)))     
lost <- integer(length(names(working.corpus)))      
miss <- integer(length(names(working.corpus)))      
murder <- integer(length(names(working.corpus)))    
mystery <- integer(length(names(working.corpus)))   
plot <- integer(length(names(working.corpus)))      
poor <- integer(length(names(working.corpus)))     
sad <- integer(length(names(working.corpus)))       
scary <- integer(length(names(working.corpus)))     
slow <- integer(length(names(working.corpus)))      
terrible <- integer(length(names(working.corpus))) 
waste <- integer(length(names(working.corpus)))     
worst <- integer(length(names(working.corpus)))    
wrong <- integer(length(names(working.corpus)))    

reviews.tdm <- TermDocumentMatrix(working.corpus)

for(index.for.document in seq(along=names(working.corpus))) {
  amazing[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "amazing")))
  beautiful[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "beautiful")))  
  classic[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "classic")))  
  enjoy[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "enjoy")))      
  enjoyed[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "enjoyed")))  
  entertaining[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "entertaining")))  
  excellent[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "excellent")))  
  fans[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "fans")))  
  favorite[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "favorite")))  
  fine[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "fine")))  
  fun[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "fun")))  
  humor[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "humor")))  
  lead[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "lead")))  
  liked[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "liked")))  
  love[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "love")))  
  loved[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "loved")))  
  modern[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "modern")))  
  nice[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "nice")))  
  perfect[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "perfect")))  
  pretty[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "pretty")))  
  recommend[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "recommend")))  
  strong[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "strong")))  
  top[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "top")))  
  wonderful[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "wonderful")))                                         
  worth[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "worth")))  
  bad[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "bad")))  
  boring[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "boring")))  
  cheap[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "cheap")))  
  creepy[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "creepy")))  
  dark[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "dark")))  
  dead[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "dead")))  
  death[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "death")))  
  evil[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "evil")))  
  hard[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "hard")))  
  kill[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "kill")))  
  killed[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "killed")))  
  lack[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "lack")))  
  lost[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "lost")))  
  miss[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "miss")))  
  murder[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "murder")))  
  mystery[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "mystery")))  
  plot[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "plot")))  
  poor[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "poor")))  
  sad[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "sad")))                        
  scary[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "scary")))  
  slow[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "slow")))  
  terrible[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "terrible")))  
  waste[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "waste")))  
  worst[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "worst")))  
  wrong[index.for.document] <- 
    sum(termFreq(working.corpus[[index.for.document]], 
    control = list(dictionary = "wrong")))  
  }

# Utilities for Spatial Data Analysis (R)

# user-defined function to convert degrees to radians
# needed for lat.long.distance function
degrees.to.radians <- function(x) { 
  (pi/180)*x
  } # end degrees.to.radians function 

# user-defined function to convert distance between two points in miles
# when the two points (a and b) are defined by longitude and latitude
lat.long.distance <- function(longitude.a,latitude.a,longitude.b,latitude.b) {
  radius.of.earth <- 24872/(2*pi)
  c <- sin((degrees.to.radians(latitude.a) - 
    degrees.to.radians(latitude.b))/2)^2 + 
    cos(degrees.to.radians(latitude.a)) * 
    cos(degrees.to.radians(latitude.b)) * 
    sin((degrees.to.radians(longitude.a) -
    degrees.to.radians(longitude.b))/2)^2
  2 * radius.of.earth * (asin(sqrt(c)))
  } # end lat.long.distance function
  
save(degrees.to.radians,
  lat.long.distance, 
  file = "mtpa_spatial_distance_utilities.R")  

# Movie Tagline Data Preparation Script for Text Analysis (R)

library(stringr)  # character manipulation with regular expressions

# convert to bytecodes to avoid "invalid multibyte string" messages
bytecode.convert <- function(x) {iconv(enc2utf8(x), sub = "byte")}

# NLINES <- 21  # for development and test runs
# input.data.file.name <- "taglines_list_sample.txt"
#  scan("taglines_list_sample.txt", what = "character")  # development runs
# nlines_to_read <- 21  # for development and test runs

# there are 345317 records in the full taglines data file
# the number of lines in the input data file
# or maximum number of lines to read
NLINES <- 345317   
input.data.file.name <- "taglines_copy_data.txt"  # production runs
# read the data in blocks of nlines_to_read at a time
nlines_to_read <- 10000  # size of block of lines to read

# debug print was used during the code development process
debub.print.mode <- FALSE
debug.print <- function(title,date,tagline,status) {
  cat("\n title =",title,"  date = ", date," tagline",
    tagline, " status = ",status,"\n")
  }

# this user-defined function shows how R can be used to parse text input
tagline.parser <- function(input.list) {
# where we start depends upon the status on entry
# tagline parser can only be in one status at a time
# begin
# indicator
# title (actually a title and date status)
# moretitle (another title and data status, but following a previous title)
# tagline
# comment

# data are not clean... if you get a new movie indicator "#" start a new movie
# we may lose a few movies this way... but that is better than editing a file
# with about 40 thousand movies..

# at this time all valid dates should look be six characters long
# four numbers surrounded by parentheses 
# lets use The Birth of a Nation (1915) as the earliest possible valid date
# and the current year as the latest possible valid date 
# obtained by as.numeric(format(Sys.time(), "%Y"))
valid.years <- 1915:as.numeric(format(Sys.time(), "%Y"))
valid.years.strings.four <- paste("(",as.character(valid.years),sep="")

   text <- input.list[[1]]
   status <- input.list[[2]]
   title <- input.list[[3]]
   date <- input.list[[4]]
   tagline <- input.list[[5]]
   
   nitems <- length(text)
   ncount <- 1  # initialize on entry
   tagline_data.store <- NULL
   
   while(ncount < nitems) {   
# debug printing was used in the development and testing of parsing logic   
     if (debub.print.mode) debug.print(title,date,tagline,status) 
     if (status == "indicator" | status == "begin") {
       if (ncount <= nitems) {
         ncount <- ncount + 1
         status <- "initialtitle"
         title <- " "  # blank title to start
         date <- " "   # blank date to start
         tagline <- " "  # blank tagline to start
         }
       }
       
     if (status == "initialtitle") {
       if (ncount <= nitems) {
         title <- text[ncount]
         ncount <- ncount + 1
         if (ncount <= nitems) {
           test_date <- text[ncount]
           if (substring(test_date,1,5) %in% valid.years.strings.four) {
             date <- test_date
             ncount <- ncount + 1
             status <- "tagline"
             }
           if (!(substring(test_date,1,5) %in% valid.years.strings.four)) {     
             if (test_date == "#") {
                 status <- "indicator"
                 }           
             if (test_date != "#") {
                 title <- paste(title, test_date)  
                 ncount <- ncount + 1 
                 status <- "moretitle"
                 }               
             } 
           }  
         }
       }
                                        
     if (status == "moretitle") {
       if (ncount <= nitems) {
         ncount <- ncount + 1
         if (ncount <= nitems) {
           test_date <- text[ncount]
           if (substring(test_date,1,5) %in% valid.years.strings.four) {
             date <- test_date
             ncount <- ncount + 1
             status <- "tagline"
             }
           if (!(substring(test_date,1,5) %in% valid.years.strings.four)) {     
             if (test_date == "#") {
                 status <- "indicator"
                 }           
             if (test_date != "#") {
                 title <- paste(title, test_date)  
                 ncount <- ncount + 1  
                 }               
             } 
           }  
         }
       }                    
                                       
       if (status == "tagline") {
         if (ncount <= nitems) {
           new_text <- text[ncount]
           if (new_text == "#") {
             tagline_data.store <- rbind(tagline_data.store,
                data.frame(title, date, tagline, stringsAsFactors = FALSE))
                status <- "indicator"
             }           
           if (new_text != "#") {
             if (substring(new_text,1,1) == "{") {
               ncount <- ncount + 1
               status <- "comment"
               }
             if (substring(new_text,1,1) != "{") {
               tagline <- paste(tagline, new_text)
               ncount <- ncount + 1
               }
             }             
           }          
         }
  
       if (status == "comment") {
         if (ncount <= nitems) {
           new_text <- text[ncount]         
           if (substring(new_text,nchar(new_text),nchar(new_text)) == "}") {
             ncount <- ncount + 1
             status <- "tagline"
             }
           if (substring(new_text,nchar(new_text),nchar(new_text)) != "}") {
             ncount <- ncount + 1
             }
           }
         }  
  } # end of primary while-loop
list(tagline_data.store, status, title, date, tagline)  # return list
}  # end of function

cat("\n\n","NUMBER OF LINES READ: ")

skip <- 0  # initialize the number of lines to skip
nlines_read_so_far <- 0  # intitialze number of lines read so far


status <- "begin"  # initial status
title <- " "  # blank title to start
date <- " "   # blank date to start
tagline <- " "  # blank tagline to start

data.store <- NULL  # initialize the data frame for storing text data

while(nlines_read_so_far < NLINES)  {

if ((NLINES - nlines_read_so_far) < nlines_to_read) 
  nlines_to_read <- (NLINES - nlines_read_so_far)
  
text <- scan(file = input.data.file.name, what = "character",
    skip = nlines_read_so_far, nlines = nlines_to_read)
 
# convert individual text items to bytecodes 
# to avoid to avoid "invalid multibyte string" error messages going forward
text <- bytecode.convert(text)

input.list <- list(text, status, title, date, tagline)  
 
# parse this block of text with the tagline parser function 
output.list <- tagline.parser(input.list) 
  
  new_data_for_store <- output.list[[1]]
  status <- output.list[[2]]
  title <- output.list[[3]]
  date <- output.list[[4]]
  tagline <- output.list[[5]]
      
  data.store <- rbind(data.store, new_data_for_store)
  
  nlines_read_so_far <- nlines_read_so_far + nlines_to_read
  
  cat(" ","nlines_read_so_far:",nlines_read_so_far)
  }
  
# if there is full movie info in output list 
# add this last movie to the end of the data.store

if ((!is.null(output.list[[3]])) & 
   (!is.null(output.list[[4]])) &
   (!is.null(output.list[[5]]))) {
       title <- output.list[[3]]
       date <- output.list[[4]]
       tagline <- output.list[[5]] 
    data.store <- rbind(data.store, 
      data.frame(title, date, tagline, stringsAsFactors = FALSE))
  }
    
# data cleaning... check the date field... 
# if it does not start with "(" or end with ")"
# strip any character other than numeric in the date field
# using regular expressions coding and the string replace function from stringr
data.store$replace.date <- str_replace_all(data.store$date, "[^.(0-9)]", "")

# at this time all valid dates should be six characters long
# four numbers surrounded by parentheses 
# lets use The Birth of a Nation (1915) as the earliest possible valid date
# and the current year as the latest possible valid date 
# obtained by as.numeric(format(Sys.time(), "%Y"))
valid.years <- 1915:as.numeric(format(Sys.time(), "%Y"))
valid.years.strings <- paste("(",as.character(valid.years),")",sep="")

# valid observations must have dates with valid.years.strings
data.store$valid <- 
  ifelse((data.store$replace.date %in% valid.years.strings),"YES","NO")

# use the subset of movies with valid data
valid.data.store <- subset(data.store, subset = (valid == "YES"))

# add date field to title field to create unique identifier for each movie
valid.data.store$movie <- paste(valid.data.store$title, valid.data.store$date)

# strip parenteses from replace.date and create an integer variable for year
valid.data.store$replace.date <- 
  str_replace(valid.data.store$replace.date,"[(]","")
valid.data.store$replace.date <- 
  str_replace(valid.data.store$replace.date,"[)]","")
valid.data.store$year <- as.integer(valid.data.store$replace.date)

# merge title and tagline text into new movie text variable for text analysis
valid.data.store$text <- 
  paste(valid.data.store$title, valid.data.store$tagline)

# drop replace.date and reorder variables for text analysis
# at this point we have one large data frame with text columns
movies <- valid.data.store[,c("movie","year","title","tagline","text")]

cat("\n writing movies data frame to comma-delimited text file\n",
    "         <movie_tagline_data_parsed.csv>","\n")
write.csv(movies, file = "movie_tagline_data_parsed.csv", row.names = FALSE)

    










  





















